{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datasets import flickr8k_parse\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, LSTM, add, Embedding, GRU, Dropout, Multiply, Dot, Lambda, BatchNormalization, \\\n",
    "    RepeatVector, concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from models import batch_generator, decoder\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import path_generation\n",
    "import tensorflow as tf\n",
    "import text_processing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2498995129509002572, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 1462032793\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10982203293942572571\n",
       " physical_device_desc: \"device: 0, name: GeForce 840M, pci bus id: 0000:07:00.0, compute capability: 5.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building decoder, it is necessary to encode captions into one-hot vectors which further would be used in embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_path = 'D:/coco/annotations/'\n",
    "# images_path = 'D:/coco/images/'\n",
    "\n",
    "# # parse JSON file with captions to get paths to images with captions\n",
    "# val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=False)\n",
    "# val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "# train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=True)\n",
    "# train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)\n",
    "\n",
    "# ### Extract captions\n",
    "# train_captions = coco_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "# val_captions = coco_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'D:/Flickr8k/images/'\n",
    "annotations_path = 'D:/Flickr8k/annotations/'\n",
    "captions_file = 'D:/Flickr8k/annotations/Flickr8k.token.txt'\n",
    "train_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.trainImages.txt'\n",
    "dev_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.devImages.txt'\n",
    "test_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.testImages.txt'\n",
    "\n",
    "filenames_with_all_captions = flickr8k_parse.generate_filenames_with_all_captions(captions_file, images_path)\n",
    "\n",
    "train_filenames_with_all_captions = flickr8k_parse.generate_set(train_txt_path, filenames_with_all_captions, images_path)\n",
    "val_filenames_with_all_captions = flickr8k_parse.generate_set(dev_txt_path, filenames_with_all_captions, images_path)\n",
    "test_filenames_with_all_captions = flickr8k_parse.generate_set(test_txt_path, filenames_with_all_captions, images_path)\n",
    "\n",
    "train_captions = flickr8k_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "val_captions = flickr8k_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess captions\n",
    "text_processing.preprocess_captions(val_captions)\n",
    "text_processing.preprocess_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add markers of captions' starts and ends\n",
    "text_processing.add_start_and_end_to_captions(train_captions)\n",
    "text_processing.add_start_and_end_to_captions(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vocabulary from the training captions\n",
    "train_vocab = text_processing.Vocabulary()\n",
    "for caption_list in train_captions:\n",
    "    for caption in caption_list:\n",
    "        tmp_caption_list = caption.split()\n",
    "        for word in tmp_caption_list:\n",
    "            train_vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./vocabulary'):\n",
    "    os.mkdir('./vocabulary')\n",
    "train_vocab.save_vocabulary('word_to_id.pickle', 'id_to_word.pickle', 'word_counter.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions_tokens = text_processing.tokenise_captions(train_captions, train_vocab)\n",
    "val_captions_tokens = text_processing.tokenise_captions(val_captions, train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 2, 8, 4, 9, 10, 11, 12],\n",
       " [1, 3, 4, 13, 14, 4, 15, 11, 12],\n",
       " [1, 16, 17, 18, 19, 20, 21, 10, 22, 23, 12],\n",
       " [1, 16, 17, 24, 25, 9, 10, 11, 12],\n",
       " [1, 16, 17, 6, 15, 2, 26, 27, 28, 29, 30, 12]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black dog is running after a white dog in the snow <eos>',\n",
       " '<sos> black dog chasing brown dog through snow <eos>',\n",
       " '<sos> two dogs chase each other across the snowy ground <eos>',\n",
       " '<sos> two dogs play together in the snow <eos>',\n",
       " '<sos> two dogs running through a low lying body of water <eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'flickr8k'\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "steps_per_epoch = int(len(train_captions) / batch_size)\n",
    "initial_state_size = 512\n",
    "embedding_out_size = 512\n",
    "number_of_layers = 2\n",
    "batch_norm = True\n",
    "dropout = True\n",
    "gru = False\n",
    "attn = True\n",
    "attn_type = 'bahdanau'\n",
    "max_len = 30\n",
    "path_gen = path_generation.PathGenerator(gru, dataset, number_of_layers, batch_size, batch_norm, dropout, attn, attn_type)\n",
    "path_checkpoint = path_gen.get_weights_path()\n",
    "model_path = path_gen.get_model_path()\n",
    "callbacks_path = path_gen.get_callbacks_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model_files/callbacks/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.csv\n"
     ]
    }
   ],
   "source": [
    "print(callbacks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attn:\n",
    "    transfer_values = np.load('./cnn_features/vgg16_flickr8k_train_attn.npy')\n",
    "    val_transfer_values = np.load('./cnn_features/vgg16_flickr8k_val_attn.npy')\n",
    "else:\n",
    "    transfer_values = np.load('./cnn_features/vgg16_flickr8k_train.npy')\n",
    "    val_transfer_values = np.load('./cnn_features/vgg16_flickr8k_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 14, 14, 512)\n",
      "(6000, 196, 512)\n"
     ]
    }
   ],
   "source": [
    "if attn:\n",
    "    print(transfer_values.shape)\n",
    "    transfer_values = transfer_values.reshape(6000, -1, 512)\n",
    "    val_transfer_values = val_transfer_values.reshape(1000, -1, 512)\n",
    "    print(transfer_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features shape (?, 196, 512)\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "word-embedding (?, 30, 512)\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Initial states\n",
      "s initial (?, 512)\n",
      "c initial (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 0\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 1\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 2\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 3\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 4\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 5\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 6\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 7\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 8\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 9\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 10\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 11\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 12\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 13\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 14\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 15\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 16\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 17\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 18\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 19\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 20\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 21\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 22\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 23\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 24\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 25\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 26\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 27\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 28\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 29\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "final lstm output shape (?, 30, 512)\n",
      "output (?, 30, 7373)\n"
     ]
    }
   ],
   "source": [
    "decoder_model = decoder.Decoder(initial_state_size,\n",
    "                               embedding_out_size,\n",
    "                               number_of_layers,\n",
    "                               gru,\n",
    "                               batch_norm,\n",
    "                               dropout,\n",
    "                               attn,\n",
    "                               attn_type,\n",
    "                               transfer_values,\n",
    "                               train_vocab)\n",
    "decoder_model = decoder_model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gru:\n",
    "    generator = batch_generator.generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size)\n",
    "    val_generator = batch_generator.generate_batch(val_transfer_values, val_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size)\n",
    "else:\n",
    "    generator = batch_generator.generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)\n",
    "    val_generator = batch_generator.generate_batch(val_transfer_values, val_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3, decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 196, 512)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512)          0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          262656      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 512)          2048        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 512)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 196, 512)     262656      encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 512)       262656      lambda_3[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 lambda_45[0][0]                  \n",
      "                                                                 lambda_48[0][0]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "                                                                 lambda_63[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "                                                                 lambda_69[0][0]                  \n",
      "                                                                 lambda_72[0][0]                  \n",
      "                                                                 lambda_75[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "                                                                 lambda_81[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "                                                                 lambda_87[0][0]                  \n",
      "                                                                 lambda_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 196, 512)     0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 196, 512)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 196, 1)       513         lambda_4[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "                                                                 lambda_40[0][0]                  \n",
      "                                                                 lambda_43[0][0]                  \n",
      "                                                                 lambda_46[0][0]                  \n",
      "                                                                 lambda_49[0][0]                  \n",
      "                                                                 lambda_52[0][0]                  \n",
      "                                                                 lambda_55[0][0]                  \n",
      "                                                                 lambda_58[0][0]                  \n",
      "                                                                 lambda_61[0][0]                  \n",
      "                                                                 lambda_64[0][0]                  \n",
      "                                                                 lambda_67[0][0]                  \n",
      "                                                                 lambda_70[0][0]                  \n",
      "                                                                 lambda_73[0][0]                  \n",
      "                                                                 lambda_76[0][0]                  \n",
      "                                                                 lambda_79[0][0]                  \n",
      "                                                                 lambda_82[0][0]                  \n",
      "                                                                 lambda_85[0][0]                  \n",
      "                                                                 lambda_88[0][0]                  \n",
      "                                                                 lambda_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "weights_0 (Lambda)              (None, 196, 1)       0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 512)      3774976     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 512)       0           weights_0[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 512)       262656      lambda_3[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 lambda_45[0][0]                  \n",
      "                                                                 lambda_48[0][0]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "                                                                 lambda_63[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "                                                                 lambda_69[0][0]                  \n",
      "                                                                 lambda_72[0][0]                  \n",
      "                                                                 lambda_75[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "                                                                 lambda_81[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "                                                                 lambda_87[0][0]                  \n",
      "                                                                 lambda_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 512)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 512)          0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1, 512)       0           dot_1[0][0]                      \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          262656      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1024)      0           multiply_1[0][0]                 \n",
      "                                                                 lambda_5[0][0]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 512)          2048        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 512), (None, 3147776     concatenate_1[0][0]              \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 concatenate_5[0][0]              \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 concatenate_7[0][0]              \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 concatenate_8[0][0]              \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 concatenate_11[0][0]             \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 concatenate_12[0][0]             \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 concatenate_13[0][0]             \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 concatenate_14[0][0]             \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 concatenate_15[0][0]             \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "                                                                 concatenate_16[0][0]             \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[14][2]                    \n",
      "                                                                 concatenate_17[0][0]             \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[15][2]                    \n",
      "                                                                 concatenate_18[0][0]             \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[16][2]                    \n",
      "                                                                 concatenate_19[0][0]             \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[17][2]                    \n",
      "                                                                 concatenate_20[0][0]             \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[18][2]                    \n",
      "                                                                 concatenate_21[0][0]             \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[19][2]                    \n",
      "                                                                 concatenate_22[0][0]             \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[20][2]                    \n",
      "                                                                 concatenate_23[0][0]             \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[21][2]                    \n",
      "                                                                 concatenate_24[0][0]             \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[22][2]                    \n",
      "                                                                 concatenate_25[0][0]             \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[23][2]                    \n",
      "                                                                 concatenate_26[0][0]             \n",
      "                                                                 lstm_1[24][0]                    \n",
      "                                                                 lstm_1[24][2]                    \n",
      "                                                                 concatenate_27[0][0]             \n",
      "                                                                 lstm_1[25][0]                    \n",
      "                                                                 lstm_1[25][2]                    \n",
      "                                                                 concatenate_28[0][0]             \n",
      "                                                                 lstm_1[26][0]                    \n",
      "                                                                 lstm_1[26][2]                    \n",
      "                                                                 concatenate_29[0][0]             \n",
      "                                                                 lstm_1[27][0]                    \n",
      "                                                                 lstm_1[27][2]                    \n",
      "                                                                 concatenate_30[0][0]             \n",
      "                                                                 lstm_1[28][0]                    \n",
      "                                                                 lstm_1[28][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1, 512)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 196, 512)     0           dense_2[1][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 196, 512)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_1 (Lambda)              (None, 196, 1)       0           dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 512)       0           weights_1[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1, 512)       0           dot_2[0][0]                      \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 1024)      0           multiply_2[0][0]                 \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1, 512)       0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 196, 512)     0           dense_2[2][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 196, 512)     0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_2 (Lambda)              (None, 196, 1)       0           dense_4[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 512)       0           weights_2[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 1, 512)       0           dot_3[0][0]                      \n",
      "                                                                 dense_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 1024)      0           multiply_3[0][0]                 \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 1, 512)       0           lstm_1[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 196, 512)     0           dense_2[3][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 196, 512)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_3 (Lambda)              (None, 196, 1)       0           dense_4[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1, 512)       0           weights_3[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 1, 512)       0           dot_4[0][0]                      \n",
      "                                                                 dense_3[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 1024)      0           multiply_4[0][0]                 \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 1, 512)       0           lstm_1[3][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 196, 512)     0           dense_2[4][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 196, 512)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_4 (Lambda)              (None, 196, 1)       0           dense_4[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1, 512)       0           weights_4[0][0]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 1, 512)       0           dot_5[0][0]                      \n",
      "                                                                 dense_3[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 1024)      0           multiply_5[0][0]                 \n",
      "                                                                 lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1, 512)       0           lstm_1[4][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 196, 512)     0           dense_2[5][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 196, 512)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_5 (Lambda)              (None, 196, 1)       0           dense_4[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1, 512)       0           weights_5[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 1, 512)       0           dot_6[0][0]                      \n",
      "                                                                 dense_3[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 1024)      0           multiply_6[0][0]                 \n",
      "                                                                 lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1, 512)       0           lstm_1[5][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 196, 512)     0           dense_2[6][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 196, 512)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_6 (Lambda)              (None, 196, 1)       0           dense_4[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_7 (Dot)                     (None, 1, 512)       0           weights_6[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 1, 512)       0           dot_7[0][0]                      \n",
      "                                                                 dense_3[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 1024)      0           multiply_7[0][0]                 \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1, 512)       0           lstm_1[6][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 196, 512)     0           dense_2[7][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 196, 512)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_7 (Lambda)              (None, 196, 1)       0           dense_4[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 1, 512)       0           weights_7[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 1, 512)       0           dot_8[0][0]                      \n",
      "                                                                 dense_3[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1, 1024)      0           multiply_8[0][0]                 \n",
      "                                                                 lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 1, 512)       0           lstm_1[7][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 196, 512)     0           dense_2[8][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 196, 512)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_8 (Lambda)              (None, 196, 1)       0           dense_4[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 1, 512)       0           weights_8[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 1, 512)       0           dot_9[0][0]                      \n",
      "                                                                 dense_3[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1, 1024)      0           multiply_9[0][0]                 \n",
      "                                                                 lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 1, 512)       0           lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 196, 512)     0           dense_2[9][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 196, 512)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_9 (Lambda)              (None, 196, 1)       0           dense_4[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_10 (Dot)                    (None, 1, 512)       0           weights_9[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 1, 512)       0           dot_10[0][0]                     \n",
      "                                                                 dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1, 1024)      0           multiply_10[0][0]                \n",
      "                                                                 lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 1, 512)       0           lstm_1[9][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 196, 512)     0           dense_2[10][0]                   \n",
      "                                                                 dense_1[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 196, 512)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_10 (Lambda)             (None, 196, 1)       0           dense_4[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_11 (Dot)                    (None, 1, 512)       0           weights_10[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 1, 512)       0           dot_11[0][0]                     \n",
      "                                                                 dense_3[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1, 1024)      0           multiply_11[0][0]                \n",
      "                                                                 lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 1, 512)       0           lstm_1[10][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 196, 512)     0           dense_2[11][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 196, 512)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_11 (Lambda)             (None, 196, 1)       0           dense_4[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_12 (Dot)                    (None, 1, 512)       0           weights_11[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 1, 512)       0           dot_12[0][0]                     \n",
      "                                                                 dense_3[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1, 1024)      0           multiply_12[0][0]                \n",
      "                                                                 lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 1, 512)       0           lstm_1[11][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 196, 512)     0           dense_2[12][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 196, 512)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_12 (Lambda)             (None, 196, 1)       0           dense_4[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_13 (Dot)                    (None, 1, 512)       0           weights_12[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 1, 512)       0           dot_13[0][0]                     \n",
      "                                                                 dense_3[12][0]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1, 1024)      0           multiply_13[0][0]                \n",
      "                                                                 lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 1, 512)       0           lstm_1[12][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 196, 512)     0           dense_2[13][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 196, 512)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_13 (Lambda)             (None, 196, 1)       0           dense_4[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_14 (Dot)                    (None, 1, 512)       0           weights_13[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 1, 512)       0           dot_14[0][0]                     \n",
      "                                                                 dense_3[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1, 1024)      0           multiply_14[0][0]                \n",
      "                                                                 lambda_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 1, 512)       0           lstm_1[13][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 196, 512)     0           dense_2[14][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 196, 512)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_14 (Lambda)             (None, 196, 1)       0           dense_4[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_15 (Dot)                    (None, 1, 512)       0           weights_14[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 1, 512)       0           dot_15[0][0]                     \n",
      "                                                                 dense_3[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1, 1024)      0           multiply_15[0][0]                \n",
      "                                                                 lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 1, 512)       0           lstm_1[14][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 196, 512)     0           dense_2[15][0]                   \n",
      "                                                                 dense_1[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 196, 512)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_15 (Lambda)             (None, 196, 1)       0           dense_4[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_16 (Dot)                    (None, 1, 512)       0           weights_15[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 1, 512)       0           dot_16[0][0]                     \n",
      "                                                                 dense_3[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1, 1024)      0           multiply_16[0][0]                \n",
      "                                                                 lambda_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 1, 512)       0           lstm_1[15][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 196, 512)     0           dense_2[16][0]                   \n",
      "                                                                 dense_1[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 196, 512)     0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_16 (Lambda)             (None, 196, 1)       0           dense_4[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_17 (Dot)                    (None, 1, 512)       0           weights_16[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 1, 512)       0           dot_17[0][0]                     \n",
      "                                                                 dense_3[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1, 1024)      0           multiply_17[0][0]                \n",
      "                                                                 lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 1, 512)       0           lstm_1[16][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 196, 512)     0           dense_2[17][0]                   \n",
      "                                                                 dense_1[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 196, 512)     0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_17 (Lambda)             (None, 196, 1)       0           dense_4[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_18 (Dot)                    (None, 1, 512)       0           weights_17[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 1, 512)       0           dot_18[0][0]                     \n",
      "                                                                 dense_3[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1, 1024)      0           multiply_18[0][0]                \n",
      "                                                                 lambda_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 1, 512)       0           lstm_1[17][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 196, 512)     0           dense_2[18][0]                   \n",
      "                                                                 dense_1[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 196, 512)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_18 (Lambda)             (None, 196, 1)       0           dense_4[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_19 (Dot)                    (None, 1, 512)       0           weights_18[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 1, 512)       0           dot_19[0][0]                     \n",
      "                                                                 dense_3[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 1, 1024)      0           multiply_19[0][0]                \n",
      "                                                                 lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 1, 512)       0           lstm_1[18][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 196, 512)     0           dense_2[19][0]                   \n",
      "                                                                 dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 196, 512)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_19 (Lambda)             (None, 196, 1)       0           dense_4[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_20 (Dot)                    (None, 1, 512)       0           weights_19[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 1, 512)       0           dot_20[0][0]                     \n",
      "                                                                 dense_3[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 1, 1024)      0           multiply_20[0][0]                \n",
      "                                                                 lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 1, 512)       0           lstm_1[19][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 196, 512)     0           dense_2[20][0]                   \n",
      "                                                                 dense_1[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 196, 512)     0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_20 (Lambda)             (None, 196, 1)       0           dense_4[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_21 (Dot)                    (None, 1, 512)       0           weights_20[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 1, 512)       0           dot_21[0][0]                     \n",
      "                                                                 dense_3[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 1, 1024)      0           multiply_21[0][0]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 1, 512)       0           lstm_1[20][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 196, 512)     0           dense_2[21][0]                   \n",
      "                                                                 dense_1[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 196, 512)     0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_21 (Lambda)             (None, 196, 1)       0           dense_4[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_22 (Dot)                    (None, 1, 512)       0           weights_21[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 1, 512)       0           dot_22[0][0]                     \n",
      "                                                                 dense_3[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 1, 1024)      0           multiply_22[0][0]                \n",
      "                                                                 lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1, 512)       0           lstm_1[21][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 196, 512)     0           dense_2[22][0]                   \n",
      "                                                                 dense_1[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 196, 512)     0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_22 (Lambda)             (None, 196, 1)       0           dense_4[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_23 (Dot)                    (None, 1, 512)       0           weights_22[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 1, 512)       0           dot_23[0][0]                     \n",
      "                                                                 dense_3[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 1, 1024)      0           multiply_23[0][0]                \n",
      "                                                                 lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 1, 512)       0           lstm_1[22][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 196, 512)     0           dense_2[23][0]                   \n",
      "                                                                 dense_1[23][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 196, 512)     0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_23 (Lambda)             (None, 196, 1)       0           dense_4[23][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_24 (Dot)                    (None, 1, 512)       0           weights_23[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 1, 512)       0           dot_24[0][0]                     \n",
      "                                                                 dense_3[23][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1, 1024)      0           multiply_24[0][0]                \n",
      "                                                                 lambda_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 1, 512)       0           lstm_1[23][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 196, 512)     0           dense_2[24][0]                   \n",
      "                                                                 dense_1[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 196, 512)     0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_24 (Lambda)             (None, 196, 1)       0           dense_4[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_25 (Dot)                    (None, 1, 512)       0           weights_24[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_25 (Multiply)          (None, 1, 512)       0           dot_25[0][0]                     \n",
      "                                                                 dense_3[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 1, 1024)      0           multiply_25[0][0]                \n",
      "                                                                 lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 1, 512)       0           lstm_1[24][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 196, 512)     0           dense_2[25][0]                   \n",
      "                                                                 dense_1[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 196, 512)     0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_25 (Lambda)             (None, 196, 1)       0           dense_4[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_26 (Dot)                    (None, 1, 512)       0           weights_25[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 1, 512)       0           dot_26[0][0]                     \n",
      "                                                                 dense_3[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 1, 1024)      0           multiply_26[0][0]                \n",
      "                                                                 lambda_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 1, 512)       0           lstm_1[25][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 196, 512)     0           dense_2[26][0]                   \n",
      "                                                                 dense_1[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 196, 512)     0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_26 (Lambda)             (None, 196, 1)       0           dense_4[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_27 (Dot)                    (None, 1, 512)       0           weights_26[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_27 (Multiply)          (None, 1, 512)       0           dot_27[0][0]                     \n",
      "                                                                 dense_3[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 1, 1024)      0           multiply_27[0][0]                \n",
      "                                                                 lambda_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 1, 512)       0           lstm_1[26][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 196, 512)     0           dense_2[27][0]                   \n",
      "                                                                 dense_1[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 196, 512)     0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_27 (Lambda)             (None, 196, 1)       0           dense_4[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_28 (Dot)                    (None, 1, 512)       0           weights_27[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_28 (Multiply)          (None, 1, 512)       0           dot_28[0][0]                     \n",
      "                                                                 dense_3[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 1, 1024)      0           multiply_28[0][0]                \n",
      "                                                                 lambda_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 1, 512)       0           lstm_1[27][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 196, 512)     0           dense_2[28][0]                   \n",
      "                                                                 dense_1[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 196, 512)     0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_28 (Lambda)             (None, 196, 1)       0           dense_4[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_29 (Dot)                    (None, 1, 512)       0           weights_28[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_29 (Multiply)          (None, 1, 512)       0           dot_29[0][0]                     \n",
      "                                                                 dense_3[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 1, 1024)      0           multiply_29[0][0]                \n",
      "                                                                 lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 1, 512)       0           lstm_1[28][0]                    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_30 (Add)                    (None, 196, 512)     0           dense_2[29][0]                   \n",
      "                                                                 dense_1[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 196, 512)     0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_29 (Lambda)             (None, 196, 1)       0           dense_4[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_30 (Dot)                    (None, 1, 512)       0           weights_29[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_30 (Multiply)          (None, 1, 512)       0           dot_30[0][0]                     \n",
      "                                                                 dense_3[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 1, 1024)      0           multiply_30[0][0]                \n",
      "                                                                 lambda_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 30, 512)      0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[24][0]                    \n",
      "                                                                 lstm_1[25][0]                    \n",
      "                                                                 lstm_1[26][0]                    \n",
      "                                                                 lstm_1[27][0]                    \n",
      "                                                                 lstm_1[28][0]                    \n",
      "                                                                 lstm_1[29][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30, 512)      2048        lambda_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 30, 512)      2099200     batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 30, 512)      2048        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, 30, 7373)     3782349     batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 14,126,286\n",
      "Trainable params: 14,122,190\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'batch_normalization_5/cond/Merge:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'batch_normalization_6/cond/Merge:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_1/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_1/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_2/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_2/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_3/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_3/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_4/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_4/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_5/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_5/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_6/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_6/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_7/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_7/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_8/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_8/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_9/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_9/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_10/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_10/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_11/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_11/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_12/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_12/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_13/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_13/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_14/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_14/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_15/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_15/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_16/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_16/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_17/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_17/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_18/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_18/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_19/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_19/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_20/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_20/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_21/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_21/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_22/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_22/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_23/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_23/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_24/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_24/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_25/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_25/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_26/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_26/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_27/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_27/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_28/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_28/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'batch_normalization_5/cond/Merge:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'batch_normalization_6/cond/Merge:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "model_json = decoder_model.to_json()\n",
    "try:\n",
    "    os.mkdir('./models')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json.dump(json.loads(model_json), json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "During the training process, it is a good idea to save the weights periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('./weights/')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "\n",
    "checkpoints = ModelCheckpoint(path_checkpoint, verbose=1, save_weights_only=True, save_best_only=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, verbose=1, min_lr=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/30\n",
      "187/187 [==============================] - ETA: 3:28:46 - loss: 8.96 - ETA: 1:46:19 - loss: 7.95 - ETA: 1:12:07 - loss: 7.12 - ETA: 55:00 - loss: 6.5388 - ETA: 44:44 - loss: 6.03 - ETA: 37:53 - loss: 5.79 - ETA: 32:59 - loss: 5.48 - ETA: 29:17 - loss: 5.19 - ETA: 26:25 - loss: 4.96 - ETA: 24:07 - loss: 4.74 - ETA: 22:14 - loss: 4.55 - ETA: 20:39 - loss: 4.39 - ETA: 19:20 - loss: 4.25 - ETA: 18:12 - loss: 4.11 - ETA: 17:13 - loss: 3.98 - ETA: 16:21 - loss: 3.88 - ETA: 15:35 - loss: 3.77 - ETA: 14:54 - loss: 3.68 - ETA: 14:17 - loss: 3.60 - ETA: 13:44 - loss: 3.53 - ETA: 13:14 - loss: 3.47 - ETA: 12:46 - loss: 3.41 - ETA: 12:20 - loss: 3.35 - ETA: 11:57 - loss: 3.31 - ETA: 11:35 - loss: 3.25 - ETA: 11:15 - loss: 3.20 - ETA: 10:56 - loss: 3.17 - ETA: 10:38 - loss: 3.13 - ETA: 10:22 - loss: 3.09 - ETA: 10:06 - loss: 3.06 - ETA: 9:51 - loss: 3.0299 - ETA: 9:38 - loss: 2.992 - ETA: 9:24 - loss: 2.962 - ETA: 9:12 - loss: 2.933 - ETA: 9:00 - loss: 2.908 - ETA: 8:49 - loss: 2.886 - ETA: 8:38 - loss: 2.862 - ETA: 8:28 - loss: 2.847 - ETA: 8:18 - loss: 2.826 - ETA: 8:09 - loss: 2.805 - ETA: 8:00 - loss: 2.788 - ETA: 7:51 - loss: 2.770 - ETA: 7:43 - loss: 2.753 - ETA: 7:35 - loss: 2.738 - ETA: 7:27 - loss: 2.720 - ETA: 7:19 - loss: 2.703 - ETA: 7:12 - loss: 2.686 - ETA: 7:05 - loss: 2.670 - ETA: 6:58 - loss: 2.654 - ETA: 6:52 - loss: 2.640 - ETA: 6:45 - loss: 2.627 - ETA: 6:39 - loss: 2.614 - ETA: 6:33 - loss: 2.601 - ETA: 6:27 - loss: 2.590 - ETA: 6:21 - loss: 2.574 - ETA: 6:16 - loss: 2.557 - ETA: 6:10 - loss: 2.543 - ETA: 6:05 - loss: 2.528 - ETA: 5:59 - loss: 2.514 - ETA: 5:54 - loss: 2.504 - ETA: 5:49 - loss: 2.493 - ETA: 5:44 - loss: 2.481 - ETA: 5:40 - loss: 2.473 - ETA: 5:35 - loss: 2.465 - ETA: 5:30 - loss: 2.454 - ETA: 5:26 - loss: 2.445 - ETA: 5:21 - loss: 2.436 - ETA: 5:17 - loss: 2.425 - ETA: 5:13 - loss: 2.414 - ETA: 5:09 - loss: 2.406 - ETA: 5:05 - loss: 2.398 - ETA: 5:00 - loss: 2.387 - ETA: 4:56 - loss: 2.380 - ETA: 4:53 - loss: 2.373 - ETA: 4:49 - loss: 2.364 - ETA: 4:45 - loss: 2.356 - ETA: 4:41 - loss: 2.350 - ETA: 4:38 - loss: 2.342 - ETA: 4:34 - loss: 2.333 - ETA: 4:30 - loss: 2.327 - ETA: 4:27 - loss: 2.319 - ETA: 4:23 - loss: 2.311 - ETA: 4:20 - loss: 2.302 - ETA: 4:17 - loss: 2.297 - ETA: 4:13 - loss: 2.291 - ETA: 4:10 - loss: 2.284 - ETA: 4:07 - loss: 2.276 - ETA: 4:03 - loss: 2.269 - ETA: 4:00 - loss: 2.264 - ETA: 3:57 - loss: 2.260 - ETA: 3:54 - loss: 2.255 - ETA: 3:51 - loss: 2.248 - ETA: 3:48 - loss: 2.242 - ETA: 3:45 - loss: 2.235 - ETA: 3:42 - loss: 2.228 - ETA: 3:39 - loss: 2.223 - ETA: 3:36 - loss: 2.215 - ETA: 3:33 - loss: 2.210 - ETA: 3:30 - loss: 2.208 - ETA: 3:27 - loss: 2.205 - ETA: 3:24 - loss: 2.199 - ETA: 3:21 - loss: 2.194 - ETA: 3:19 - loss: 2.192 - ETA: 3:16 - loss: 2.185 - ETA: 3:13 - loss: 2.180 - ETA: 3:10 - loss: 2.174 - ETA: 3:08 - loss: 2.169 - ETA: 3:05 - loss: 2.164 - ETA: 3:02 - loss: 2.161 - ETA: 3:00 - loss: 2.158 - ETA: 2:57 - loss: 2.154 - ETA: 2:55 - loss: 2.148 - ETA: 2:52 - loss: 2.146 - ETA: 2:50 - loss: 2.142 - ETA: 2:47 - loss: 2.139 - ETA: 2:44 - loss: 2.135 - ETA: 2:42 - loss: 2.134 - ETA: 2:39 - loss: 2.130 - ETA: 2:37 - loss: 2.126 - ETA: 2:34 - loss: 2.122 - ETA: 2:32 - loss: 2.121 - ETA: 2:29 - loss: 2.118 - ETA: 2:26 - loss: 2.112 - ETA: 2:24 - loss: 2.108 - ETA: 2:21 - loss: 2.104 - ETA: 2:19 - loss: 2.101 - ETA: 2:17 - loss: 2.097 - ETA: 2:14 - loss: 2.094 - ETA: 2:12 - loss: 2.091 - ETA: 2:09 - loss: 2.087 - ETA: 2:07 - loss: 2.084 - ETA: 2:04 - loss: 2.081 - ETA: 2:02 - loss: 2.078 - ETA: 1:59 - loss: 2.077 - ETA: 1:57 - loss: 2.072 - ETA: 1:54 - loss: 2.069 - ETA: 1:52 - loss: 2.066 - ETA: 1:50 - loss: 2.062 - ETA: 1:47 - loss: 2.060 - ETA: 1:45 - loss: 2.057 - ETA: 1:43 - loss: 2.054 - ETA: 1:40 - loss: 2.050 - ETA: 1:38 - loss: 2.046 - ETA: 1:35 - loss: 2.044 - ETA: 1:33 - loss: 2.040 - ETA: 1:31 - loss: 2.036 - ETA: 1:28 - loss: 2.033 - ETA: 1:26 - loss: 2.030 - ETA: 1:24 - loss: 2.027 - ETA: 1:21 - loss: 2.024 - ETA: 1:19 - loss: 2.022 - ETA: 1:17 - loss: 2.019 - ETA: 1:15 - loss: 2.016 - ETA: 1:12 - loss: 2.013 - ETA: 1:10 - loss: 2.010 - ETA: 1:08 - loss: 2.007 - ETA: 1:05 - loss: 2.005 - ETA: 1:03 - loss: 2.004 - ETA: 1:01 - loss: 2.003 - ETA: 59s - loss: 2.000 - ETA: 56s - loss: 1.99 - ETA: 54s - loss: 1.99 - ETA: 52s - loss: 1.99 - ETA: 50s - loss: 1.99 - ETA: 48s - loss: 1.99 - ETA: 45s - loss: 1.99 - ETA: 43s - loss: 1.98 - ETA: 41s - loss: 1.98 - ETA: 39s - loss: 1.98 - ETA: 36s - loss: 1.98 - ETA: 34s - loss: 1.97 - ETA: 32s - loss: 1.97 - ETA: 30s - loss: 1.97 - ETA: 28s - loss: 1.97 - ETA: 25s - loss: 1.96 - ETA: 23s - loss: 1.96 - ETA: 21s - loss: 1.96 - ETA: 19s - loss: 1.96 - ETA: 17s - loss: 1.95 - ETA: 15s - loss: 1.95 - ETA: 12s - loss: 1.95 - ETA: 10s - loss: 1.95 - ETA: 8s - loss: 1.9520 - ETA: 6s - loss: 1.949 - ETA: 4s - loss: 1.947 - ETA: 2s - loss: 1.945 - 410s 2s/step - loss: 1.9444 - val_loss: 11.0878\n",
      "\n",
      "Epoch 00001: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 2/30\n",
      "187/187 [==============================] - ETA: 5:32 - loss: 1.578 - ETA: 5:29 - loss: 1.518 - ETA: 5:27 - loss: 1.485 - ETA: 5:27 - loss: 1.505 - ETA: 5:25 - loss: 1.544 - ETA: 5:23 - loss: 1.523 - ETA: 5:20 - loss: 1.501 - ETA: 5:18 - loss: 1.526 - ETA: 5:16 - loss: 1.551 - ETA: 5:14 - loss: 1.553 - ETA: 5:12 - loss: 1.531 - ETA: 5:10 - loss: 1.508 - ETA: 5:08 - loss: 1.509 - ETA: 5:06 - loss: 1.504 - ETA: 5:02 - loss: 1.509 - ETA: 4:59 - loss: 1.522 - ETA: 4:56 - loss: 1.512 - ETA: 4:53 - loss: 1.502 - ETA: 4:50 - loss: 1.493 - ETA: 4:47 - loss: 1.492 - ETA: 4:44 - loss: 1.493 - ETA: 4:42 - loss: 1.491 - ETA: 4:39 - loss: 1.488 - ETA: 4:37 - loss: 1.486 - ETA: 4:34 - loss: 1.495 - ETA: 4:32 - loss: 1.497 - ETA: 4:30 - loss: 1.504 - ETA: 4:28 - loss: 1.507 - ETA: 4:25 - loss: 1.503 - ETA: 4:23 - loss: 1.500 - ETA: 4:21 - loss: 1.504 - ETA: 4:19 - loss: 1.503 - ETA: 4:17 - loss: 1.506 - ETA: 4:15 - loss: 1.499 - ETA: 4:13 - loss: 1.497 - ETA: 4:11 - loss: 1.500 - ETA: 4:09 - loss: 1.495 - ETA: 4:08 - loss: 1.494 - ETA: 4:06 - loss: 1.494 - ETA: 4:04 - loss: 1.493 - ETA: 4:02 - loss: 1.491 - ETA: 4:00 - loss: 1.490 - ETA: 3:58 - loss: 1.493 - ETA: 3:56 - loss: 1.494 - ETA: 3:55 - loss: 1.498 - ETA: 3:53 - loss: 1.494 - ETA: 3:51 - loss: 1.496 - ETA: 3:49 - loss: 1.493 - ETA: 3:47 - loss: 1.492 - ETA: 3:46 - loss: 1.490 - ETA: 3:44 - loss: 1.491 - ETA: 3:42 - loss: 1.487 - ETA: 3:40 - loss: 1.488 - ETA: 3:38 - loss: 1.492 - ETA: 3:37 - loss: 1.498 - ETA: 3:35 - loss: 1.498 - ETA: 3:33 - loss: 1.496 - ETA: 3:32 - loss: 1.497 - ETA: 3:30 - loss: 1.496 - ETA: 3:28 - loss: 1.500 - ETA: 3:26 - loss: 1.500 - ETA: 3:25 - loss: 1.503 - ETA: 3:23 - loss: 1.504 - ETA: 3:21 - loss: 1.503 - ETA: 3:19 - loss: 1.506 - ETA: 3:18 - loss: 1.505 - ETA: 3:16 - loss: 1.502 - ETA: 3:14 - loss: 1.502 - ETA: 3:13 - loss: 1.500 - ETA: 3:11 - loss: 1.499 - ETA: 3:09 - loss: 1.499 - ETA: 3:08 - loss: 1.501 - ETA: 3:06 - loss: 1.502 - ETA: 3:04 - loss: 1.502 - ETA: 3:02 - loss: 1.502 - ETA: 3:01 - loss: 1.504 - ETA: 2:59 - loss: 1.503 - ETA: 2:57 - loss: 1.502 - ETA: 2:56 - loss: 1.502 - ETA: 2:54 - loss: 1.501 - ETA: 2:52 - loss: 1.497 - ETA: 2:51 - loss: 1.496 - ETA: 2:49 - loss: 1.497 - ETA: 2:47 - loss: 1.497 - ETA: 2:46 - loss: 1.497 - ETA: 2:44 - loss: 1.496 - ETA: 2:42 - loss: 1.494 - ETA: 2:41 - loss: 1.493 - ETA: 2:39 - loss: 1.493 - ETA: 2:37 - loss: 1.491 - ETA: 2:36 - loss: 1.489 - ETA: 2:34 - loss: 1.490 - ETA: 2:32 - loss: 1.488 - ETA: 2:31 - loss: 1.489 - ETA: 2:29 - loss: 1.488 - ETA: 2:28 - loss: 1.487 - ETA: 2:26 - loss: 1.486 - ETA: 2:24 - loss: 1.485 - ETA: 2:23 - loss: 1.483 - ETA: 2:21 - loss: 1.483 - ETA: 2:19 - loss: 1.482 - ETA: 2:18 - loss: 1.482 - ETA: 2:16 - loss: 1.481 - ETA: 2:14 - loss: 1.481 - ETA: 2:13 - loss: 1.481 - ETA: 2:11 - loss: 1.480 - ETA: 2:09 - loss: 1.479 - ETA: 2:08 - loss: 1.478 - ETA: 2:06 - loss: 1.478 - ETA: 2:05 - loss: 1.479 - ETA: 2:03 - loss: 1.478 - ETA: 2:01 - loss: 1.476 - ETA: 2:00 - loss: 1.474 - ETA: 1:58 - loss: 1.473 - ETA: 1:56 - loss: 1.472 - ETA: 1:55 - loss: 1.473 - ETA: 1:53 - loss: 1.472 - ETA: 1:51 - loss: 1.472 - ETA: 1:50 - loss: 1.471 - ETA: 1:48 - loss: 1.470 - ETA: 1:46 - loss: 1.470 - ETA: 1:45 - loss: 1.471 - ETA: 1:43 - loss: 1.471 - ETA: 1:42 - loss: 1.472 - ETA: 1:40 - loss: 1.473 - ETA: 1:38 - loss: 1.472 - ETA: 1:37 - loss: 1.471 - ETA: 1:35 - loss: 1.471 - ETA: 1:33 - loss: 1.471 - ETA: 1:32 - loss: 1.471 - ETA: 1:30 - loss: 1.471 - ETA: 1:29 - loss: 1.471 - ETA: 1:27 - loss: 1.470 - ETA: 1:25 - loss: 1.470 - ETA: 1:24 - loss: 1.470 - ETA: 1:22 - loss: 1.471 - ETA: 1:20 - loss: 1.470 - ETA: 1:19 - loss: 1.471 - ETA: 1:17 - loss: 1.472 - ETA: 1:16 - loss: 1.472 - ETA: 1:14 - loss: 1.470 - ETA: 1:12 - loss: 1.468 - ETA: 1:11 - loss: 1.470 - ETA: 1:09 - loss: 1.471 - ETA: 1:07 - loss: 1.470 - ETA: 1:06 - loss: 1.469 - ETA: 1:04 - loss: 1.468 - ETA: 1:03 - loss: 1.466 - ETA: 1:01 - loss: 1.465 - ETA: 59s - loss: 1.465 - ETA: 58s - loss: 1.46 - ETA: 56s - loss: 1.46 - ETA: 54s - loss: 1.46 - ETA: 53s - loss: 1.46 - ETA: 51s - loss: 1.46 - ETA: 50s - loss: 1.46 - ETA: 48s - loss: 1.46 - ETA: 46s - loss: 1.46 - ETA: 45s - loss: 1.46 - ETA: 43s - loss: 1.46 - ETA: 41s - loss: 1.46 - ETA: 40s - loss: 1.45 - ETA: 38s - loss: 1.45 - ETA: 37s - loss: 1.45 - ETA: 35s - loss: 1.45 - ETA: 33s - loss: 1.45 - ETA: 32s - loss: 1.45 - ETA: 30s - loss: 1.45 - ETA: 29s - loss: 1.45 - ETA: 27s - loss: 1.45 - ETA: 25s - loss: 1.45 - ETA: 24s - loss: 1.45 - ETA: 22s - loss: 1.45 - ETA: 20s - loss: 1.45 - ETA: 19s - loss: 1.45 - ETA: 17s - loss: 1.45 - ETA: 16s - loss: 1.45 - ETA: 14s - loss: 1.45 - ETA: 12s - loss: 1.45 - ETA: 11s - loss: 1.45 - ETA: 9s - loss: 1.4554 - ETA: 8s - loss: 1.455 - ETA: 6s - loss: 1.455 - ETA: 4s - loss: 1.455 - ETA: 3s - loss: 1.455 - ETA: 1s - loss: 1.455 - 304s 2s/step - loss: 1.4551 - val_loss: 2.9322\n",
      "\n",
      "Epoch 00002: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:02 - loss: 1.334 - ETA: 4:57 - loss: 1.410 - ETA: 4:55 - loss: 1.431 - ETA: 4:54 - loss: 1.392 - ETA: 4:52 - loss: 1.357 - ETA: 4:50 - loss: 1.347 - ETA: 4:48 - loss: 1.337 - ETA: 4:47 - loss: 1.305 - ETA: 4:45 - loss: 1.324 - ETA: 4:43 - loss: 1.332 - ETA: 4:42 - loss: 1.330 - ETA: 4:40 - loss: 1.335 - ETA: 4:39 - loss: 1.336 - ETA: 4:37 - loss: 1.354 - ETA: 4:36 - loss: 1.356 - ETA: 4:34 - loss: 1.355 - ETA: 4:32 - loss: 1.355 - ETA: 4:31 - loss: 1.362 - ETA: 4:29 - loss: 1.363 - ETA: 4:27 - loss: 1.368 - ETA: 4:26 - loss: 1.371 - ETA: 4:24 - loss: 1.370 - ETA: 4:22 - loss: 1.370 - ETA: 4:21 - loss: 1.367 - ETA: 4:19 - loss: 1.366 - ETA: 4:17 - loss: 1.367 - ETA: 4:16 - loss: 1.370 - ETA: 4:14 - loss: 1.370 - ETA: 4:13 - loss: 1.374 - ETA: 4:11 - loss: 1.371 - ETA: 4:09 - loss: 1.372 - ETA: 4:08 - loss: 1.370 - ETA: 4:06 - loss: 1.371 - ETA: 4:05 - loss: 1.372 - ETA: 4:03 - loss: 1.372 - ETA: 4:01 - loss: 1.375 - ETA: 4:00 - loss: 1.373 - ETA: 3:58 - loss: 1.367 - ETA: 3:57 - loss: 1.373 - ETA: 3:55 - loss: 1.375 - ETA: 3:53 - loss: 1.374 - ETA: 3:52 - loss: 1.374 - ETA: 3:50 - loss: 1.380 - ETA: 3:49 - loss: 1.378 - ETA: 3:47 - loss: 1.379 - ETA: 3:45 - loss: 1.382 - ETA: 3:44 - loss: 1.384 - ETA: 3:42 - loss: 1.382 - ETA: 3:40 - loss: 1.384 - ETA: 3:39 - loss: 1.383 - ETA: 3:37 - loss: 1.386 - ETA: 3:36 - loss: 1.390 - ETA: 3:34 - loss: 1.390 - ETA: 3:32 - loss: 1.390 - ETA: 3:31 - loss: 1.390 - ETA: 3:29 - loss: 1.390 - ETA: 3:28 - loss: 1.389 - ETA: 3:26 - loss: 1.389 - ETA: 3:24 - loss: 1.389 - ETA: 3:23 - loss: 1.388 - ETA: 3:21 - loss: 1.388 - ETA: 3:20 - loss: 1.387 - ETA: 3:18 - loss: 1.389 - ETA: 3:16 - loss: 1.392 - ETA: 3:15 - loss: 1.395 - ETA: 3:13 - loss: 1.396 - ETA: 3:12 - loss: 1.395 - ETA: 3:10 - loss: 1.393 - ETA: 3:08 - loss: 1.393 - ETA: 3:07 - loss: 1.390 - ETA: 3:05 - loss: 1.390 - ETA: 3:04 - loss: 1.390 - ETA: 3:02 - loss: 1.392 - ETA: 3:00 - loss: 1.391 - ETA: 2:59 - loss: 1.391 - ETA: 2:57 - loss: 1.392 - ETA: 2:56 - loss: 1.394 - ETA: 2:54 - loss: 1.397 - ETA: 2:52 - loss: 1.397 - ETA: 2:51 - loss: 1.395 - ETA: 2:49 - loss: 1.394 - ETA: 2:48 - loss: 1.394 - ETA: 2:46 - loss: 1.393 - ETA: 2:44 - loss: 1.391 - ETA: 2:43 - loss: 1.391 - ETA: 2:41 - loss: 1.389 - ETA: 2:40 - loss: 1.389 - ETA: 2:38 - loss: 1.389 - ETA: 2:36 - loss: 1.385 - ETA: 2:35 - loss: 1.386 - ETA: 2:33 - loss: 1.387 - ETA: 2:32 - loss: 1.385 - ETA: 2:30 - loss: 1.386 - ETA: 2:28 - loss: 1.387 - ETA: 2:27 - loss: 1.387 - ETA: 2:25 - loss: 1.386 - ETA: 2:24 - loss: 1.385 - ETA: 2:22 - loss: 1.384 - ETA: 2:20 - loss: 1.384 - ETA: 2:19 - loss: 1.383 - ETA: 2:17 - loss: 1.383 - ETA: 2:16 - loss: 1.381 - ETA: 2:14 - loss: 1.380 - ETA: 2:12 - loss: 1.379 - ETA: 2:11 - loss: 1.378 - ETA: 2:09 - loss: 1.379 - ETA: 2:08 - loss: 1.380 - ETA: 2:06 - loss: 1.380 - ETA: 2:04 - loss: 1.380 - ETA: 2:03 - loss: 1.381 - ETA: 2:01 - loss: 1.380 - ETA: 2:00 - loss: 1.381 - ETA: 1:58 - loss: 1.381 - ETA: 1:56 - loss: 1.381 - ETA: 1:55 - loss: 1.379 - ETA: 1:53 - loss: 1.380 - ETA: 1:52 - loss: 1.379 - ETA: 1:50 - loss: 1.380 - ETA: 1:48 - loss: 1.381 - ETA: 1:47 - loss: 1.382 - ETA: 1:45 - loss: 1.382 - ETA: 1:44 - loss: 1.381 - ETA: 1:42 - loss: 1.380 - ETA: 1:40 - loss: 1.380 - ETA: 1:39 - loss: 1.380 - ETA: 1:37 - loss: 1.379 - ETA: 1:36 - loss: 1.378 - ETA: 1:34 - loss: 1.378 - ETA: 1:32 - loss: 1.378 - ETA: 1:31 - loss: 1.377 - ETA: 1:29 - loss: 1.377 - ETA: 1:28 - loss: 1.376 - ETA: 1:26 - loss: 1.376 - ETA: 1:24 - loss: 1.376 - ETA: 1:23 - loss: 1.375 - ETA: 1:21 - loss: 1.376 - ETA: 1:20 - loss: 1.377 - ETA: 1:18 - loss: 1.376 - ETA: 1:16 - loss: 1.375 - ETA: 1:15 - loss: 1.375 - ETA: 1:13 - loss: 1.376 - ETA: 1:12 - loss: 1.377 - ETA: 1:10 - loss: 1.376 - ETA: 1:08 - loss: 1.377 - ETA: 1:07 - loss: 1.377 - ETA: 1:05 - loss: 1.376 - ETA: 1:04 - loss: 1.376 - ETA: 1:02 - loss: 1.376 - ETA: 1:00 - loss: 1.376 - ETA: 59s - loss: 1.375 - ETA: 57s - loss: 1.37 - ETA: 56s - loss: 1.37 - ETA: 54s - loss: 1.37 - ETA: 52s - loss: 1.37 - ETA: 51s - loss: 1.37 - ETA: 49s - loss: 1.37 - ETA: 48s - loss: 1.37 - ETA: 46s - loss: 1.37 - ETA: 44s - loss: 1.37 - ETA: 43s - loss: 1.37 - ETA: 41s - loss: 1.37 - ETA: 40s - loss: 1.37 - ETA: 38s - loss: 1.37 - ETA: 36s - loss: 1.37 - ETA: 35s - loss: 1.37 - ETA: 33s - loss: 1.37 - ETA: 32s - loss: 1.37 - ETA: 30s - loss: 1.37 - ETA: 28s - loss: 1.37 - ETA: 27s - loss: 1.37 - ETA: 25s - loss: 1.37 - ETA: 24s - loss: 1.37 - ETA: 22s - loss: 1.37 - ETA: 20s - loss: 1.37 - ETA: 19s - loss: 1.37 - ETA: 17s - loss: 1.37 - ETA: 16s - loss: 1.37 - ETA: 14s - loss: 1.37 - ETA: 12s - loss: 1.37 - ETA: 11s - loss: 1.37 - ETA: 9s - loss: 1.3743 - ETA: 8s - loss: 1.373 - ETA: 6s - loss: 1.372 - ETA: 4s - loss: 1.372 - ETA: 3s - loss: 1.372 - ETA: 1s - loss: 1.372 - 301s 2s/step - loss: 1.3719 - val_loss: 3.5093\n",
      "\n",
      "Epoch 00003: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 4/30\n",
      "187/187 [==============================] - ETA: 4:57 - loss: 1.397 - ETA: 4:56 - loss: 1.384 - ETA: 4:54 - loss: 1.389 - ETA: 4:52 - loss: 1.316 - ETA: 4:51 - loss: 1.320 - ETA: 4:49 - loss: 1.337 - ETA: 4:47 - loss: 1.349 - ETA: 4:46 - loss: 1.337 - ETA: 4:44 - loss: 1.331 - ETA: 4:42 - loss: 1.335 - ETA: 4:41 - loss: 1.335 - ETA: 4:39 - loss: 1.333 - ETA: 4:38 - loss: 1.338 - ETA: 4:36 - loss: 1.337 - ETA: 4:35 - loss: 1.330 - ETA: 4:33 - loss: 1.320 - ETA: 4:31 - loss: 1.326 - ETA: 4:30 - loss: 1.328 - ETA: 4:28 - loss: 1.327 - ETA: 4:27 - loss: 1.331 - ETA: 4:25 - loss: 1.330 - ETA: 4:23 - loss: 1.332 - ETA: 4:22 - loss: 1.334 - ETA: 4:20 - loss: 1.335 - ETA: 4:18 - loss: 1.335 - ETA: 4:17 - loss: 1.334 - ETA: 4:15 - loss: 1.333 - ETA: 4:14 - loss: 1.327 - ETA: 4:12 - loss: 1.322 - ETA: 4:11 - loss: 1.325 - ETA: 4:09 - loss: 1.322 - ETA: 4:07 - loss: 1.323 - ETA: 4:06 - loss: 1.320 - ETA: 4:04 - loss: 1.320 - ETA: 4:03 - loss: 1.319 - ETA: 4:01 - loss: 1.318 - ETA: 3:59 - loss: 1.318 - ETA: 3:58 - loss: 1.316 - ETA: 3:56 - loss: 1.318 - ETA: 3:55 - loss: 1.321 - ETA: 3:53 - loss: 1.324 - ETA: 3:51 - loss: 1.324 - ETA: 3:50 - loss: 1.328 - ETA: 3:48 - loss: 1.330 - ETA: 3:47 - loss: 1.333 - ETA: 3:45 - loss: 1.331 - ETA: 3:43 - loss: 1.328 - ETA: 3:42 - loss: 1.337 - ETA: 3:40 - loss: 1.338 - ETA: 3:39 - loss: 1.335 - ETA: 3:37 - loss: 1.337 - ETA: 3:35 - loss: 1.337 - ETA: 3:34 - loss: 1.338 - ETA: 3:32 - loss: 1.336 - ETA: 3:31 - loss: 1.332 - ETA: 3:29 - loss: 1.327 - ETA: 3:27 - loss: 1.330 - ETA: 3:26 - loss: 1.330 - ETA: 3:24 - loss: 1.329 - ETA: 3:23 - loss: 1.328 - ETA: 3:21 - loss: 1.327 - ETA: 3:19 - loss: 1.328 - ETA: 3:18 - loss: 1.328 - ETA: 3:16 - loss: 1.327 - ETA: 3:15 - loss: 1.327 - ETA: 3:13 - loss: 1.328 - ETA: 3:11 - loss: 1.327 - ETA: 3:10 - loss: 1.329 - ETA: 3:08 - loss: 1.330 - ETA: 3:07 - loss: 1.328 - ETA: 3:05 - loss: 1.330 - ETA: 3:03 - loss: 1.330 - ETA: 3:02 - loss: 1.331 - ETA: 3:00 - loss: 1.332 - ETA: 2:59 - loss: 1.333 - ETA: 2:57 - loss: 1.334 - ETA: 2:55 - loss: 1.333 - ETA: 2:54 - loss: 1.332 - ETA: 2:52 - loss: 1.332 - ETA: 2:51 - loss: 1.333 - ETA: 2:49 - loss: 1.334 - ETA: 2:47 - loss: 1.333 - ETA: 2:46 - loss: 1.332 - ETA: 2:44 - loss: 1.334 - ETA: 2:43 - loss: 1.332 - ETA: 2:41 - loss: 1.334 - ETA: 2:39 - loss: 1.335 - ETA: 2:38 - loss: 1.333 - ETA: 2:36 - loss: 1.333 - ETA: 2:35 - loss: 1.335 - ETA: 2:33 - loss: 1.334 - ETA: 2:31 - loss: 1.334 - ETA: 2:30 - loss: 1.333 - ETA: 2:28 - loss: 1.333 - ETA: 2:27 - loss: 1.334 - ETA: 2:25 - loss: 1.333 - ETA: 2:23 - loss: 1.331 - ETA: 2:22 - loss: 1.334 - ETA: 2:20 - loss: 1.334 - ETA: 2:19 - loss: 1.334 - ETA: 2:17 - loss: 1.333 - ETA: 2:15 - loss: 1.332 - ETA: 2:14 - loss: 1.330 - ETA: 2:12 - loss: 1.330 - ETA: 2:11 - loss: 1.329 - ETA: 2:09 - loss: 1.328 - ETA: 2:07 - loss: 1.327 - ETA: 2:06 - loss: 1.327 - ETA: 2:04 - loss: 1.328 - ETA: 2:03 - loss: 1.330 - ETA: 2:01 - loss: 1.331 - ETA: 1:59 - loss: 1.330 - ETA: 1:58 - loss: 1.331 - ETA: 1:56 - loss: 1.330 - ETA: 1:55 - loss: 1.329 - ETA: 1:53 - loss: 1.329 - ETA: 1:51 - loss: 1.326 - ETA: 1:50 - loss: 1.326 - ETA: 1:48 - loss: 1.326 - ETA: 1:47 - loss: 1.326 - ETA: 1:45 - loss: 1.326 - ETA: 1:43 - loss: 1.325 - ETA: 1:42 - loss: 1.325 - ETA: 1:40 - loss: 1.326 - ETA: 1:39 - loss: 1.325 - ETA: 1:37 - loss: 1.323 - ETA: 1:35 - loss: 1.322 - ETA: 1:34 - loss: 1.323 - ETA: 1:32 - loss: 1.324 - ETA: 1:31 - loss: 1.324 - ETA: 1:29 - loss: 1.324 - ETA: 1:28 - loss: 1.324 - ETA: 1:26 - loss: 1.325 - ETA: 1:24 - loss: 1.327 - ETA: 1:23 - loss: 1.327 - ETA: 1:21 - loss: 1.328 - ETA: 1:20 - loss: 1.329 - ETA: 1:18 - loss: 1.328 - ETA: 1:16 - loss: 1.329 - ETA: 1:15 - loss: 1.329 - ETA: 1:13 - loss: 1.329 - ETA: 1:12 - loss: 1.329 - ETA: 1:10 - loss: 1.330 - ETA: 1:08 - loss: 1.330 - ETA: 1:07 - loss: 1.329 - ETA: 1:05 - loss: 1.329 - ETA: 1:04 - loss: 1.329 - ETA: 1:02 - loss: 1.328 - ETA: 1:00 - loss: 1.329 - ETA: 59s - loss: 1.329 - ETA: 57s - loss: 1.32 - ETA: 56s - loss: 1.32 - ETA: 54s - loss: 1.32 - ETA: 52s - loss: 1.32 - ETA: 51s - loss: 1.32 - ETA: 49s - loss: 1.32 - ETA: 48s - loss: 1.32 - ETA: 46s - loss: 1.32 - ETA: 44s - loss: 1.32 - ETA: 43s - loss: 1.32 - ETA: 41s - loss: 1.32 - ETA: 40s - loss: 1.32 - ETA: 38s - loss: 1.32 - ETA: 36s - loss: 1.32 - ETA: 35s - loss: 1.32 - ETA: 33s - loss: 1.32 - ETA: 32s - loss: 1.32 - ETA: 30s - loss: 1.32 - ETA: 28s - loss: 1.32 - ETA: 27s - loss: 1.32 - ETA: 25s - loss: 1.32 - ETA: 24s - loss: 1.32 - ETA: 22s - loss: 1.32 - ETA: 20s - loss: 1.32 - ETA: 19s - loss: 1.32 - ETA: 17s - loss: 1.32 - ETA: 16s - loss: 1.32 - ETA: 14s - loss: 1.32 - ETA: 12s - loss: 1.32 - ETA: 11s - loss: 1.32 - ETA: 9s - loss: 1.3251 - ETA: 8s - loss: 1.325 - ETA: 6s - loss: 1.325 - ETA: 4s - loss: 1.325 - ETA: 3s - loss: 1.325 - ETA: 1s - loss: 1.325 - 302s 2s/step - loss: 1.3256 - val_loss: 1.5402\n",
      "\n",
      "Epoch 00004: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:56 - loss: 1.400 - ETA: 4:54 - loss: 1.477 - ETA: 4:54 - loss: 1.458 - ETA: 4:52 - loss: 1.410 - ETA: 4:51 - loss: 1.379 - ETA: 4:49 - loss: 1.340 - ETA: 4:47 - loss: 1.327 - ETA: 4:46 - loss: 1.362 - ETA: 4:44 - loss: 1.361 - ETA: 4:42 - loss: 1.381 - ETA: 4:41 - loss: 1.385 - ETA: 4:39 - loss: 1.369 - ETA: 4:37 - loss: 1.365 - ETA: 4:36 - loss: 1.363 - ETA: 4:34 - loss: 1.353 - ETA: 4:33 - loss: 1.356 - ETA: 4:31 - loss: 1.352 - ETA: 4:29 - loss: 1.346 - ETA: 4:28 - loss: 1.340 - ETA: 4:26 - loss: 1.335 - ETA: 4:25 - loss: 1.329 - ETA: 4:23 - loss: 1.322 - ETA: 4:21 - loss: 1.315 - ETA: 4:20 - loss: 1.325 - ETA: 4:18 - loss: 1.320 - ETA: 4:17 - loss: 1.321 - ETA: 4:15 - loss: 1.321 - ETA: 4:14 - loss: 1.321 - ETA: 4:12 - loss: 1.314 - ETA: 4:10 - loss: 1.317 - ETA: 4:09 - loss: 1.315 - ETA: 4:07 - loss: 1.310 - ETA: 4:06 - loss: 1.308 - ETA: 4:04 - loss: 1.309 - ETA: 4:02 - loss: 1.313 - ETA: 4:01 - loss: 1.310 - ETA: 3:59 - loss: 1.314 - ETA: 3:58 - loss: 1.314 - ETA: 3:56 - loss: 1.313 - ETA: 3:54 - loss: 1.310 - ETA: 3:53 - loss: 1.312 - ETA: 3:51 - loss: 1.311 - ETA: 3:49 - loss: 1.308 - ETA: 3:48 - loss: 1.308 - ETA: 3:46 - loss: 1.311 - ETA: 3:45 - loss: 1.309 - ETA: 3:43 - loss: 1.315 - ETA: 3:41 - loss: 1.317 - ETA: 3:40 - loss: 1.319 - ETA: 3:38 - loss: 1.322 - ETA: 3:37 - loss: 1.317 - ETA: 3:35 - loss: 1.314 - ETA: 3:34 - loss: 1.314 - ETA: 3:32 - loss: 1.312 - ETA: 3:30 - loss: 1.311 - ETA: 3:29 - loss: 1.309 - ETA: 3:27 - loss: 1.306 - ETA: 3:26 - loss: 1.306 - ETA: 3:24 - loss: 1.305 - ETA: 3:22 - loss: 1.305 - ETA: 3:21 - loss: 1.310 - ETA: 3:19 - loss: 1.309 - ETA: 3:18 - loss: 1.313 - ETA: 3:16 - loss: 1.314 - ETA: 3:14 - loss: 1.313 - ETA: 3:13 - loss: 1.315 - ETA: 3:11 - loss: 1.315 - ETA: 3:10 - loss: 1.313 - ETA: 3:08 - loss: 1.311 - ETA: 3:06 - loss: 1.311 - ETA: 3:05 - loss: 1.308 - ETA: 3:03 - loss: 1.307 - ETA: 3:02 - loss: 1.307 - ETA: 3:00 - loss: 1.308 - ETA: 2:58 - loss: 1.308 - ETA: 2:57 - loss: 1.306 - ETA: 2:55 - loss: 1.306 - ETA: 2:54 - loss: 1.304 - ETA: 2:52 - loss: 1.303 - ETA: 2:50 - loss: 1.303 - ETA: 2:49 - loss: 1.303 - ETA: 2:47 - loss: 1.303 - ETA: 2:46 - loss: 1.300 - ETA: 2:44 - loss: 1.301 - ETA: 2:42 - loss: 1.299 - ETA: 2:41 - loss: 1.298 - ETA: 2:39 - loss: 1.298 - ETA: 2:38 - loss: 1.296 - ETA: 2:36 - loss: 1.295 - ETA: 2:34 - loss: 1.296 - ETA: 2:33 - loss: 1.294 - ETA: 2:31 - loss: 1.293 - ETA: 2:30 - loss: 1.292 - ETA: 2:28 - loss: 1.291 - ETA: 2:26 - loss: 1.290 - ETA: 2:25 - loss: 1.290 - ETA: 2:23 - loss: 1.290 - ETA: 2:22 - loss: 1.291 - ETA: 2:20 - loss: 1.290 - ETA: 2:18 - loss: 1.289 - ETA: 2:17 - loss: 1.289 - ETA: 2:15 - loss: 1.288 - ETA: 2:14 - loss: 1.287 - ETA: 2:12 - loss: 1.287 - ETA: 2:10 - loss: 1.288 - ETA: 2:09 - loss: 1.289 - ETA: 2:07 - loss: 1.290 - ETA: 2:06 - loss: 1.288 - ETA: 2:04 - loss: 1.289 - ETA: 2:02 - loss: 1.290 - ETA: 2:01 - loss: 1.290 - ETA: 1:59 - loss: 1.292 - ETA: 1:58 - loss: 1.292 - ETA: 1:56 - loss: 1.292 - ETA: 1:54 - loss: 1.291 - ETA: 1:53 - loss: 1.291 - ETA: 1:51 - loss: 1.291 - ETA: 1:50 - loss: 1.292 - ETA: 1:48 - loss: 1.291 - ETA: 1:46 - loss: 1.289 - ETA: 1:45 - loss: 1.289 - ETA: 1:43 - loss: 1.289 - ETA: 1:42 - loss: 1.289 - ETA: 1:40 - loss: 1.288 - ETA: 1:39 - loss: 1.289 - ETA: 1:37 - loss: 1.287 - ETA: 1:35 - loss: 1.288 - ETA: 1:34 - loss: 1.287 - ETA: 1:32 - loss: 1.287 - ETA: 1:31 - loss: 1.286 - ETA: 1:29 - loss: 1.285 - ETA: 1:27 - loss: 1.284 - ETA: 1:26 - loss: 1.284 - ETA: 1:24 - loss: 1.283 - ETA: 1:23 - loss: 1.284 - ETA: 1:21 - loss: 1.285 - ETA: 1:19 - loss: 1.285 - ETA: 1:18 - loss: 1.285 - ETA: 1:16 - loss: 1.286 - ETA: 1:15 - loss: 1.286 - ETA: 1:13 - loss: 1.286 - ETA: 1:11 - loss: 1.286 - ETA: 1:10 - loss: 1.287 - ETA: 1:08 - loss: 1.287 - ETA: 1:07 - loss: 1.286 - ETA: 1:05 - loss: 1.287 - ETA: 1:03 - loss: 1.288 - ETA: 1:02 - loss: 1.288 - ETA: 1:00 - loss: 1.288 - ETA: 59s - loss: 1.288 - ETA: 57s - loss: 1.28 - ETA: 55s - loss: 1.28 - ETA: 54s - loss: 1.28 - ETA: 52s - loss: 1.28 - ETA: 51s - loss: 1.28 - ETA: 49s - loss: 1.28 - ETA: 47s - loss: 1.28 - ETA: 46s - loss: 1.28 - ETA: 44s - loss: 1.28 - ETA: 43s - loss: 1.28 - ETA: 41s - loss: 1.28 - ETA: 39s - loss: 1.28 - ETA: 38s - loss: 1.28 - ETA: 36s - loss: 1.28 - ETA: 35s - loss: 1.28 - ETA: 33s - loss: 1.28 - ETA: 31s - loss: 1.29 - ETA: 30s - loss: 1.29 - ETA: 28s - loss: 1.29 - ETA: 27s - loss: 1.29 - ETA: 25s - loss: 1.29 - ETA: 23s - loss: 1.29 - ETA: 22s - loss: 1.29 - ETA: 20s - loss: 1.29 - ETA: 19s - loss: 1.29 - ETA: 17s - loss: 1.28 - ETA: 15s - loss: 1.28 - ETA: 14s - loss: 1.28 - ETA: 12s - loss: 1.28 - ETA: 11s - loss: 1.28 - ETA: 9s - loss: 1.2876 - ETA: 7s - loss: 1.287 - ETA: 6s - loss: 1.286 - ETA: 4s - loss: 1.286 - ETA: 3s - loss: 1.285 - ETA: 1s - loss: 1.286 - 301s 2s/step - loss: 1.2874 - val_loss: 1.3684\n",
      "\n",
      "Epoch 00005: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 6/30\n",
      "187/187 [==============================] - ETA: 4:54 - loss: 1.296 - ETA: 4:53 - loss: 1.133 - ETA: 4:52 - loss: 1.190 - ETA: 4:51 - loss: 1.194 - ETA: 4:49 - loss: 1.177 - ETA: 4:47 - loss: 1.205 - ETA: 4:46 - loss: 1.202 - ETA: 4:44 - loss: 1.212 - ETA: 4:43 - loss: 1.224 - ETA: 4:41 - loss: 1.237 - ETA: 4:40 - loss: 1.259 - ETA: 4:38 - loss: 1.264 - ETA: 4:37 - loss: 1.260 - ETA: 4:35 - loss: 1.256 - ETA: 4:33 - loss: 1.253 - ETA: 4:32 - loss: 1.247 - ETA: 4:30 - loss: 1.258 - ETA: 4:29 - loss: 1.263 - ETA: 4:27 - loss: 1.250 - ETA: 4:26 - loss: 1.253 - ETA: 4:24 - loss: 1.251 - ETA: 4:23 - loss: 1.255 - ETA: 4:21 - loss: 1.257 - ETA: 4:19 - loss: 1.259 - ETA: 4:18 - loss: 1.264 - ETA: 4:16 - loss: 1.257 - ETA: 4:14 - loss: 1.256 - ETA: 4:13 - loss: 1.257 - ETA: 4:11 - loss: 1.253 - ETA: 4:10 - loss: 1.250 - ETA: 4:08 - loss: 1.246 - ETA: 4:07 - loss: 1.248 - ETA: 4:05 - loss: 1.251 - ETA: 4:03 - loss: 1.254 - ETA: 4:02 - loss: 1.257 - ETA: 4:00 - loss: 1.258 - ETA: 3:59 - loss: 1.255 - ETA: 3:57 - loss: 1.254 - ETA: 3:55 - loss: 1.248 - ETA: 3:54 - loss: 1.246 - ETA: 3:52 - loss: 1.248 - ETA: 3:51 - loss: 1.248 - ETA: 3:49 - loss: 1.246 - ETA: 3:48 - loss: 1.241 - ETA: 3:46 - loss: 1.248 - ETA: 3:44 - loss: 1.248 - ETA: 3:43 - loss: 1.246 - ETA: 3:41 - loss: 1.247 - ETA: 3:40 - loss: 1.244 - ETA: 3:38 - loss: 1.245 - ETA: 3:36 - loss: 1.244 - ETA: 3:35 - loss: 1.243 - ETA: 3:33 - loss: 1.242 - ETA: 3:32 - loss: 1.241 - ETA: 3:30 - loss: 1.243 - ETA: 3:28 - loss: 1.243 - ETA: 3:27 - loss: 1.244 - ETA: 3:25 - loss: 1.243 - ETA: 3:24 - loss: 1.243 - ETA: 3:22 - loss: 1.239 - ETA: 3:20 - loss: 1.240 - ETA: 3:19 - loss: 1.236 - ETA: 3:17 - loss: 1.237 - ETA: 3:16 - loss: 1.237 - ETA: 3:14 - loss: 1.236 - ETA: 3:12 - loss: 1.238 - ETA: 3:11 - loss: 1.238 - ETA: 3:09 - loss: 1.236 - ETA: 3:08 - loss: 1.238 - ETA: 3:06 - loss: 1.238 - ETA: 3:05 - loss: 1.240 - ETA: 3:03 - loss: 1.242 - ETA: 3:01 - loss: 1.242 - ETA: 3:00 - loss: 1.241 - ETA: 2:58 - loss: 1.240 - ETA: 2:57 - loss: 1.240 - ETA: 2:55 - loss: 1.241 - ETA: 2:53 - loss: 1.241 - ETA: 2:52 - loss: 1.237 - ETA: 2:50 - loss: 1.238 - ETA: 2:49 - loss: 1.237 - ETA: 2:47 - loss: 1.238 - ETA: 2:45 - loss: 1.236 - ETA: 2:44 - loss: 1.236 - ETA: 2:42 - loss: 1.237 - ETA: 2:41 - loss: 1.239 - ETA: 2:39 - loss: 1.239 - ETA: 2:37 - loss: 1.239 - ETA: 2:36 - loss: 1.238 - ETA: 2:34 - loss: 1.238 - ETA: 2:33 - loss: 1.237 - ETA: 2:31 - loss: 1.235 - ETA: 2:29 - loss: 1.237 - ETA: 2:28 - loss: 1.237 - ETA: 2:26 - loss: 1.236 - ETA: 2:25 - loss: 1.235 - ETA: 2:23 - loss: 1.235 - ETA: 2:21 - loss: 1.235 - ETA: 2:20 - loss: 1.236 - ETA: 2:18 - loss: 1.236 - ETA: 2:17 - loss: 1.234 - ETA: 2:15 - loss: 1.235 - ETA: 2:13 - loss: 1.233 - ETA: 2:12 - loss: 1.232 - ETA: 2:10 - loss: 1.231 - ETA: 2:09 - loss: 1.232 - ETA: 2:07 - loss: 1.235 - ETA: 2:06 - loss: 1.234 - ETA: 2:04 - loss: 1.234 - ETA: 2:02 - loss: 1.233 - ETA: 2:01 - loss: 1.233 - ETA: 1:59 - loss: 1.234 - ETA: 1:58 - loss: 1.233 - ETA: 1:56 - loss: 1.234 - ETA: 1:54 - loss: 1.233 - ETA: 1:53 - loss: 1.232 - ETA: 1:51 - loss: 1.231 - ETA: 1:50 - loss: 1.230 - ETA: 1:48 - loss: 1.229 - ETA: 1:46 - loss: 1.230 - ETA: 1:45 - loss: 1.232 - ETA: 1:43 - loss: 1.233 - ETA: 1:42 - loss: 1.233 - ETA: 1:40 - loss: 1.233 - ETA: 1:38 - loss: 1.233 - ETA: 1:37 - loss: 1.235 - ETA: 1:35 - loss: 1.234 - ETA: 1:34 - loss: 1.234 - ETA: 1:32 - loss: 1.233 - ETA: 1:30 - loss: 1.233 - ETA: 1:29 - loss: 1.233 - ETA: 1:27 - loss: 1.233 - ETA: 1:26 - loss: 1.233 - ETA: 1:24 - loss: 1.231 - ETA: 1:22 - loss: 1.232 - ETA: 1:21 - loss: 1.232 - ETA: 1:19 - loss: 1.231 - ETA: 1:18 - loss: 1.232 - ETA: 1:16 - loss: 1.231 - ETA: 1:14 - loss: 1.231 - ETA: 1:13 - loss: 1.231 - ETA: 1:11 - loss: 1.231 - ETA: 1:10 - loss: 1.230 - ETA: 1:08 - loss: 1.229 - ETA: 1:07 - loss: 1.231 - ETA: 1:05 - loss: 1.230 - ETA: 1:03 - loss: 1.229 - ETA: 1:02 - loss: 1.229 - ETA: 1:00 - loss: 1.229 - ETA: 59s - loss: 1.230 - ETA: 57s - loss: 1.23 - ETA: 55s - loss: 1.23 - ETA: 54s - loss: 1.23 - ETA: 52s - loss: 1.23 - ETA: 51s - loss: 1.23 - ETA: 49s - loss: 1.23 - ETA: 47s - loss: 1.23 - ETA: 46s - loss: 1.23 - ETA: 44s - loss: 1.23 - ETA: 43s - loss: 1.23 - ETA: 41s - loss: 1.23 - ETA: 39s - loss: 1.22 - ETA: 38s - loss: 1.23 - ETA: 36s - loss: 1.22 - ETA: 35s - loss: 1.22 - ETA: 33s - loss: 1.22 - ETA: 31s - loss: 1.22 - ETA: 30s - loss: 1.22 - ETA: 28s - loss: 1.22 - ETA: 27s - loss: 1.22 - ETA: 25s - loss: 1.22 - ETA: 23s - loss: 1.22 - ETA: 22s - loss: 1.22 - ETA: 20s - loss: 1.22 - ETA: 19s - loss: 1.22 - ETA: 17s - loss: 1.22 - ETA: 15s - loss: 1.22 - ETA: 14s - loss: 1.22 - ETA: 12s - loss: 1.22 - ETA: 11s - loss: 1.22 - ETA: 9s - loss: 1.2273 - ETA: 7s - loss: 1.227 - ETA: 6s - loss: 1.227 - ETA: 4s - loss: 1.227 - ETA: 3s - loss: 1.227 - ETA: 1s - loss: 1.227 - 301s 2s/step - loss: 1.2287 - val_loss: 1.4167\n",
      "\n",
      "Epoch 00006: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:56 - loss: 1.308 - ETA: 4:55 - loss: 1.215 - ETA: 4:53 - loss: 1.187 - ETA: 4:52 - loss: 1.269 - ETA: 4:50 - loss: 1.296 - ETA: 4:48 - loss: 1.282 - ETA: 4:46 - loss: 1.248 - ETA: 4:45 - loss: 1.235 - ETA: 4:43 - loss: 1.231 - ETA: 4:42 - loss: 1.248 - ETA: 4:40 - loss: 1.247 - ETA: 4:39 - loss: 1.244 - ETA: 4:37 - loss: 1.236 - ETA: 4:36 - loss: 1.239 - ETA: 4:34 - loss: 1.235 - ETA: 4:32 - loss: 1.229 - ETA: 4:31 - loss: 1.239 - ETA: 4:29 - loss: 1.241 - ETA: 4:28 - loss: 1.237 - ETA: 4:26 - loss: 1.233 - ETA: 4:25 - loss: 1.235 - ETA: 4:23 - loss: 1.227 - ETA: 4:21 - loss: 1.223 - ETA: 4:20 - loss: 1.226 - ETA: 4:18 - loss: 1.233 - ETA: 4:17 - loss: 1.234 - ETA: 4:15 - loss: 1.234 - ETA: 4:13 - loss: 1.232 - ETA: 4:12 - loss: 1.240 - ETA: 4:10 - loss: 1.239 - ETA: 4:09 - loss: 1.240 - ETA: 4:07 - loss: 1.238 - ETA: 4:05 - loss: 1.234 - ETA: 4:04 - loss: 1.236 - ETA: 4:02 - loss: 1.236 - ETA: 4:01 - loss: 1.238 - ETA: 3:59 - loss: 1.237 - ETA: 3:57 - loss: 1.239 - ETA: 3:56 - loss: 1.241 - ETA: 3:54 - loss: 1.242 - ETA: 3:53 - loss: 1.238 - ETA: 3:51 - loss: 1.240 - ETA: 3:50 - loss: 1.239 - ETA: 3:48 - loss: 1.239 - ETA: 3:46 - loss: 1.242 - ETA: 3:45 - loss: 1.241 - ETA: 3:43 - loss: 1.238 - ETA: 3:41 - loss: 1.235 - ETA: 3:40 - loss: 1.236 - ETA: 3:38 - loss: 1.235 - ETA: 3:37 - loss: 1.238 - ETA: 3:35 - loss: 1.239 - ETA: 3:33 - loss: 1.236 - ETA: 3:32 - loss: 1.230 - ETA: 3:30 - loss: 1.230 - ETA: 3:29 - loss: 1.230 - ETA: 3:27 - loss: 1.227 - ETA: 3:25 - loss: 1.229 - ETA: 3:24 - loss: 1.227 - ETA: 3:22 - loss: 1.227 - ETA: 3:21 - loss: 1.226 - ETA: 3:19 - loss: 1.225 - ETA: 3:17 - loss: 1.224 - ETA: 3:16 - loss: 1.223 - ETA: 3:14 - loss: 1.221 - ETA: 3:13 - loss: 1.219 - ETA: 3:11 - loss: 1.220 - ETA: 3:09 - loss: 1.221 - ETA: 3:08 - loss: 1.220 - ETA: 3:06 - loss: 1.222 - ETA: 3:05 - loss: 1.223 - ETA: 3:03 - loss: 1.224 - ETA: 3:01 - loss: 1.224 - ETA: 3:00 - loss: 1.224 - ETA: 2:58 - loss: 1.224 - ETA: 2:57 - loss: 1.224 - ETA: 2:55 - loss: 1.224 - ETA: 2:53 - loss: 1.225 - ETA: 2:52 - loss: 1.222 - ETA: 2:50 - loss: 1.221 - ETA: 2:49 - loss: 1.221 - ETA: 2:47 - loss: 1.219 - ETA: 2:45 - loss: 1.218 - ETA: 2:44 - loss: 1.218 - ETA: 2:42 - loss: 1.218 - ETA: 2:41 - loss: 1.216 - ETA: 2:39 - loss: 1.217 - ETA: 2:37 - loss: 1.217 - ETA: 2:36 - loss: 1.215 - ETA: 2:34 - loss: 1.215 - ETA: 2:33 - loss: 1.215 - ETA: 2:31 - loss: 1.214 - ETA: 2:29 - loss: 1.214 - ETA: 2:28 - loss: 1.214 - ETA: 2:26 - loss: 1.215 - ETA: 2:25 - loss: 1.215 - ETA: 2:23 - loss: 1.215 - ETA: 2:22 - loss: 1.216 - ETA: 2:20 - loss: 1.215 - ETA: 2:18 - loss: 1.217 - ETA: 2:17 - loss: 1.217 - ETA: 2:15 - loss: 1.218 - ETA: 2:14 - loss: 1.219 - ETA: 2:12 - loss: 1.219 - ETA: 2:10 - loss: 1.218 - ETA: 2:09 - loss: 1.219 - ETA: 2:07 - loss: 1.220 - ETA: 2:06 - loss: 1.220 - ETA: 2:04 - loss: 1.218 - ETA: 2:02 - loss: 1.218 - ETA: 2:01 - loss: 1.218 - ETA: 1:59 - loss: 1.217 - ETA: 1:58 - loss: 1.217 - ETA: 1:56 - loss: 1.218 - ETA: 1:54 - loss: 1.218 - ETA: 1:53 - loss: 1.219 - ETA: 1:51 - loss: 1.219 - ETA: 1:50 - loss: 1.219 - ETA: 1:48 - loss: 1.218 - ETA: 1:46 - loss: 1.217 - ETA: 1:45 - loss: 1.218 - ETA: 1:43 - loss: 1.219 - ETA: 1:42 - loss: 1.219 - ETA: 1:40 - loss: 1.219 - ETA: 1:38 - loss: 1.219 - ETA: 1:37 - loss: 1.220 - ETA: 1:35 - loss: 1.220 - ETA: 1:34 - loss: 1.220 - ETA: 1:32 - loss: 1.220 - ETA: 1:30 - loss: 1.222 - ETA: 1:29 - loss: 1.221 - ETA: 1:27 - loss: 1.221 - ETA: 1:26 - loss: 1.222 - ETA: 1:24 - loss: 1.223 - ETA: 1:23 - loss: 1.222 - ETA: 1:21 - loss: 1.223 - ETA: 1:19 - loss: 1.224 - ETA: 1:18 - loss: 1.223 - ETA: 1:16 - loss: 1.223 - ETA: 1:15 - loss: 1.223 - ETA: 1:13 - loss: 1.222 - ETA: 1:11 - loss: 1.224 - ETA: 1:10 - loss: 1.224 - ETA: 1:08 - loss: 1.223 - ETA: 1:07 - loss: 1.222 - ETA: 1:05 - loss: 1.222 - ETA: 1:03 - loss: 1.222 - ETA: 1:02 - loss: 1.222 - ETA: 1:00 - loss: 1.222 - ETA: 59s - loss: 1.223 - ETA: 57s - loss: 1.22 - ETA: 55s - loss: 1.22 - ETA: 54s - loss: 1.22 - ETA: 52s - loss: 1.22 - ETA: 51s - loss: 1.22 - ETA: 49s - loss: 1.22 - ETA: 47s - loss: 1.22 - ETA: 46s - loss: 1.22 - ETA: 44s - loss: 1.22 - ETA: 43s - loss: 1.22 - ETA: 41s - loss: 1.21 - ETA: 39s - loss: 1.21 - ETA: 38s - loss: 1.22 - ETA: 36s - loss: 1.22 - ETA: 35s - loss: 1.21 - ETA: 33s - loss: 1.21 - ETA: 31s - loss: 1.21 - ETA: 30s - loss: 1.21 - ETA: 28s - loss: 1.21 - ETA: 27s - loss: 1.21 - ETA: 25s - loss: 1.21 - ETA: 23s - loss: 1.21 - ETA: 22s - loss: 1.21 - ETA: 20s - loss: 1.21 - ETA: 19s - loss: 1.21 - ETA: 17s - loss: 1.21 - ETA: 15s - loss: 1.21 - ETA: 14s - loss: 1.21 - ETA: 12s - loss: 1.21 - ETA: 11s - loss: 1.21 - ETA: 9s - loss: 1.2163 - ETA: 7s - loss: 1.216 - ETA: 6s - loss: 1.216 - ETA: 4s - loss: 1.216 - ETA: 3s - loss: 1.216 - ETA: 1s - loss: 1.216 - 301s 2s/step - loss: 1.2154 - val_loss: 1.4637\n",
      "\n",
      "Epoch 00007: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/30\n",
      "187/187 [==============================] - ETA: 4:54 - loss: 1.241 - ETA: 4:54 - loss: 1.179 - ETA: 4:53 - loss: 1.220 - ETA: 4:51 - loss: 1.237 - ETA: 4:50 - loss: 1.211 - ETA: 4:48 - loss: 1.174 - ETA: 4:47 - loss: 1.149 - ETA: 4:45 - loss: 1.156 - ETA: 4:44 - loss: 1.159 - ETA: 4:42 - loss: 1.149 - ETA: 4:41 - loss: 1.150 - ETA: 4:39 - loss: 1.148 - ETA: 4:38 - loss: 1.141 - ETA: 4:36 - loss: 1.146 - ETA: 4:34 - loss: 1.161 - ETA: 4:33 - loss: 1.165 - ETA: 4:31 - loss: 1.175 - ETA: 4:29 - loss: 1.170 - ETA: 4:28 - loss: 1.177 - ETA: 4:26 - loss: 1.173 - ETA: 4:25 - loss: 1.175 - ETA: 4:23 - loss: 1.174 - ETA: 4:21 - loss: 1.170 - ETA: 4:20 - loss: 1.167 - ETA: 4:18 - loss: 1.167 - ETA: 4:17 - loss: 1.169 - ETA: 4:15 - loss: 1.162 - ETA: 4:13 - loss: 1.156 - ETA: 4:12 - loss: 1.161 - ETA: 4:10 - loss: 1.165 - ETA: 4:08 - loss: 1.167 - ETA: 4:07 - loss: 1.163 - ETA: 4:05 - loss: 1.161 - ETA: 4:04 - loss: 1.165 - ETA: 4:02 - loss: 1.164 - ETA: 4:00 - loss: 1.168 - ETA: 3:59 - loss: 1.167 - ETA: 3:57 - loss: 1.166 - ETA: 3:56 - loss: 1.166 - ETA: 3:54 - loss: 1.162 - ETA: 3:52 - loss: 1.164 - ETA: 3:51 - loss: 1.165 - ETA: 3:49 - loss: 1.161 - ETA: 3:48 - loss: 1.158 - ETA: 3:46 - loss: 1.159 - ETA: 3:44 - loss: 1.155 - ETA: 3:43 - loss: 1.158 - ETA: 3:41 - loss: 1.159 - ETA: 3:40 - loss: 1.161 - ETA: 3:38 - loss: 1.161 - ETA: 3:37 - loss: 1.160 - ETA: 3:35 - loss: 1.159 - ETA: 3:33 - loss: 1.163 - ETA: 3:32 - loss: 1.162 - ETA: 3:30 - loss: 1.163 - ETA: 3:28 - loss: 1.164 - ETA: 3:27 - loss: 1.167 - ETA: 3:25 - loss: 1.166 - ETA: 3:24 - loss: 1.165 - ETA: 3:22 - loss: 1.166 - ETA: 3:20 - loss: 1.166 - ETA: 3:19 - loss: 1.167 - ETA: 3:17 - loss: 1.168 - ETA: 3:16 - loss: 1.167 - ETA: 3:14 - loss: 1.166 - ETA: 3:13 - loss: 1.167 - ETA: 3:11 - loss: 1.166 - ETA: 3:09 - loss: 1.165 - ETA: 3:08 - loss: 1.163 - ETA: 3:06 - loss: 1.166 - ETA: 3:05 - loss: 1.162 - ETA: 3:03 - loss: 1.161 - ETA: 3:01 - loss: 1.160 - ETA: 3:00 - loss: 1.163 - ETA: 2:58 - loss: 1.164 - ETA: 2:57 - loss: 1.161 - ETA: 2:55 - loss: 1.160 - ETA: 2:53 - loss: 1.161 - ETA: 2:52 - loss: 1.160 - ETA: 2:50 - loss: 1.158 - ETA: 2:49 - loss: 1.157 - ETA: 2:47 - loss: 1.158 - ETA: 2:45 - loss: 1.160 - ETA: 2:44 - loss: 1.160 - ETA: 2:42 - loss: 1.160 - ETA: 2:41 - loss: 1.160 - ETA: 2:39 - loss: 1.158 - ETA: 2:37 - loss: 1.157 - ETA: 2:36 - loss: 1.157 - ETA: 2:34 - loss: 1.158 - ETA: 2:33 - loss: 1.157 - ETA: 2:31 - loss: 1.157 - ETA: 2:29 - loss: 1.156 - ETA: 2:28 - loss: 1.157 - ETA: 2:26 - loss: 1.158 - ETA: 2:25 - loss: 1.159 - ETA: 2:23 - loss: 1.162 - ETA: 2:21 - loss: 1.160 - ETA: 2:20 - loss: 1.159 - ETA: 2:18 - loss: 1.160 - ETA: 2:17 - loss: 1.159 - ETA: 2:15 - loss: 1.158 - ETA: 2:13 - loss: 1.158 - ETA: 2:12 - loss: 1.157 - ETA: 2:10 - loss: 1.157 - ETA: 2:09 - loss: 1.157 - ETA: 2:07 - loss: 1.158 - ETA: 2:05 - loss: 1.158 - ETA: 2:04 - loss: 1.157 - ETA: 2:02 - loss: 1.159 - ETA: 2:01 - loss: 1.159 - ETA: 1:59 - loss: 1.158 - ETA: 1:58 - loss: 1.159 - ETA: 1:56 - loss: 1.161 - ETA: 1:54 - loss: 1.159 - ETA: 1:53 - loss: 1.160 - ETA: 1:51 - loss: 1.163 - ETA: 1:50 - loss: 1.162 - ETA: 1:48 - loss: 1.163 - ETA: 1:46 - loss: 1.163 - ETA: 1:45 - loss: 1.164 - ETA: 1:43 - loss: 1.164 - ETA: 1:42 - loss: 1.162 - ETA: 1:40 - loss: 1.161 - ETA: 1:38 - loss: 1.160 - ETA: 1:37 - loss: 1.160 - ETA: 1:35 - loss: 1.160 - ETA: 1:34 - loss: 1.160 - ETA: 1:32 - loss: 1.161 - ETA: 1:30 - loss: 1.160 - ETA: 1:29 - loss: 1.158 - ETA: 1:27 - loss: 1.159 - ETA: 1:26 - loss: 1.160 - ETA: 1:24 - loss: 1.161 - ETA: 1:22 - loss: 1.162 - ETA: 1:21 - loss: 1.163 - ETA: 1:19 - loss: 1.163 - ETA: 1:18 - loss: 1.163 - ETA: 1:16 - loss: 1.161 - ETA: 1:14 - loss: 1.162 - ETA: 1:13 - loss: 1.160 - ETA: 1:11 - loss: 1.159 - ETA: 1:10 - loss: 1.159 - ETA: 1:08 - loss: 1.159 - ETA: 1:06 - loss: 1.158 - ETA: 1:05 - loss: 1.157 - ETA: 1:03 - loss: 1.158 - ETA: 1:02 - loss: 1.157 - ETA: 1:00 - loss: 1.157 - ETA: 58s - loss: 1.156 - ETA: 57s - loss: 1.15 - ETA: 55s - loss: 1.15 - ETA: 54s - loss: 1.15 - ETA: 52s - loss: 1.15 - ETA: 51s - loss: 1.15 - ETA: 49s - loss: 1.16 - ETA: 47s - loss: 1.15 - ETA: 46s - loss: 1.15 - ETA: 44s - loss: 1.15 - ETA: 43s - loss: 1.15 - ETA: 41s - loss: 1.15 - ETA: 39s - loss: 1.15 - ETA: 38s - loss: 1.15 - ETA: 36s - loss: 1.15 - ETA: 35s - loss: 1.15 - ETA: 33s - loss: 1.15 - ETA: 31s - loss: 1.15 - ETA: 30s - loss: 1.15 - ETA: 28s - loss: 1.15 - ETA: 27s - loss: 1.15 - ETA: 25s - loss: 1.15 - ETA: 23s - loss: 1.15 - ETA: 22s - loss: 1.15 - ETA: 20s - loss: 1.15 - ETA: 19s - loss: 1.15 - ETA: 17s - loss: 1.15 - ETA: 15s - loss: 1.15 - ETA: 14s - loss: 1.15 - ETA: 12s - loss: 1.15 - ETA: 11s - loss: 1.15 - ETA: 9s - loss: 1.1576 - ETA: 7s - loss: 1.157 - ETA: 6s - loss: 1.158 - ETA: 4s - loss: 1.158 - ETA: 3s - loss: 1.157 - ETA: 1s - loss: 1.157 - 300s 2s/step - loss: 1.1562 - val_loss: 1.4612\n",
      "\n",
      "Epoch 00008: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:56 - loss: 1.355 - ETA: 4:54 - loss: 1.250 - ETA: 4:53 - loss: 1.269 - ETA: 4:51 - loss: 1.233 - ETA: 4:49 - loss: 1.219 - ETA: 4:48 - loss: 1.211 - ETA: 4:46 - loss: 1.207 - ETA: 4:45 - loss: 1.191 - ETA: 4:43 - loss: 1.191 - ETA: 4:42 - loss: 1.191 - ETA: 4:40 - loss: 1.196 - ETA: 4:39 - loss: 1.193 - ETA: 4:37 - loss: 1.182 - ETA: 4:36 - loss: 1.185 - ETA: 4:34 - loss: 1.195 - ETA: 4:32 - loss: 1.208 - ETA: 4:31 - loss: 1.202 - ETA: 4:29 - loss: 1.195 - ETA: 4:28 - loss: 1.197 - ETA: 4:26 - loss: 1.197 - ETA: 4:24 - loss: 1.194 - ETA: 4:23 - loss: 1.185 - ETA: 4:21 - loss: 1.186 - ETA: 4:20 - loss: 1.184 - ETA: 4:18 - loss: 1.194 - ETA: 4:16 - loss: 1.194 - ETA: 4:15 - loss: 1.196 - ETA: 4:13 - loss: 1.198 - ETA: 4:12 - loss: 1.197 - ETA: 4:10 - loss: 1.194 - ETA: 4:08 - loss: 1.199 - ETA: 4:07 - loss: 1.198 - ETA: 4:05 - loss: 1.198 - ETA: 4:04 - loss: 1.192 - ETA: 4:02 - loss: 1.193 - ETA: 4:00 - loss: 1.188 - ETA: 3:59 - loss: 1.188 - ETA: 3:57 - loss: 1.188 - ETA: 3:56 - loss: 1.187 - ETA: 3:54 - loss: 1.185 - ETA: 3:53 - loss: 1.185 - ETA: 3:51 - loss: 1.187 - ETA: 3:50 - loss: 1.186 - ETA: 3:48 - loss: 1.183 - ETA: 3:46 - loss: 1.182 - ETA: 3:45 - loss: 1.178 - ETA: 3:43 - loss: 1.177 - ETA: 3:42 - loss: 1.177 - ETA: 3:40 - loss: 1.178 - ETA: 3:38 - loss: 1.178 - ETA: 3:37 - loss: 1.177 - ETA: 3:35 - loss: 1.179 - ETA: 3:34 - loss: 1.175 - ETA: 3:32 - loss: 1.175 - ETA: 3:30 - loss: 1.177 - ETA: 3:29 - loss: 1.175 - ETA: 3:27 - loss: 1.177 - ETA: 3:26 - loss: 1.177 - ETA: 3:24 - loss: 1.180 - ETA: 3:22 - loss: 1.180 - ETA: 3:21 - loss: 1.182 - ETA: 3:19 - loss: 1.184 - ETA: 3:18 - loss: 1.185 - ETA: 3:16 - loss: 1.183 - ETA: 3:15 - loss: 1.181 - ETA: 3:13 - loss: 1.182 - ETA: 3:11 - loss: 1.181 - ETA: 3:10 - loss: 1.180 - ETA: 3:08 - loss: 1.179 - ETA: 3:07 - loss: 1.182 - ETA: 3:05 - loss: 1.182 - ETA: 3:03 - loss: 1.182 - ETA: 3:02 - loss: 1.184 - ETA: 3:00 - loss: 1.182 - ETA: 2:59 - loss: 1.181 - ETA: 2:57 - loss: 1.181 - ETA: 2:55 - loss: 1.181 - ETA: 2:54 - loss: 1.182 - ETA: 2:52 - loss: 1.183 - ETA: 2:51 - loss: 1.181 - ETA: 2:49 - loss: 1.181 - ETA: 2:47 - loss: 1.181 - ETA: 2:46 - loss: 1.183 - ETA: 2:44 - loss: 1.182 - ETA: 2:43 - loss: 1.183 - ETA: 2:41 - loss: 1.182 - ETA: 2:39 - loss: 1.182 - ETA: 2:38 - loss: 1.182 - ETA: 2:36 - loss: 1.183 - ETA: 2:35 - loss: 1.182 - ETA: 2:33 - loss: 1.181 - ETA: 2:31 - loss: 1.179 - ETA: 2:30 - loss: 1.178 - ETA: 2:28 - loss: 1.178 - ETA: 2:27 - loss: 1.178 - ETA: 2:25 - loss: 1.178 - ETA: 2:23 - loss: 1.177 - ETA: 2:22 - loss: 1.176 - ETA: 2:20 - loss: 1.175 - ETA: 2:19 - loss: 1.174 - ETA: 2:17 - loss: 1.173 - ETA: 2:15 - loss: 1.172 - ETA: 2:14 - loss: 1.174 - ETA: 2:12 - loss: 1.175 - ETA: 2:11 - loss: 1.174 - ETA: 2:09 - loss: 1.173 - ETA: 2:07 - loss: 1.171 - ETA: 2:06 - loss: 1.172 - ETA: 2:04 - loss: 1.173 - ETA: 2:03 - loss: 1.172 - ETA: 2:01 - loss: 1.173 - ETA: 1:59 - loss: 1.174 - ETA: 1:58 - loss: 1.173 - ETA: 1:56 - loss: 1.173 - ETA: 1:55 - loss: 1.173 - ETA: 1:53 - loss: 1.173 - ETA: 1:51 - loss: 1.172 - ETA: 1:50 - loss: 1.171 - ETA: 1:48 - loss: 1.172 - ETA: 1:47 - loss: 1.172 - ETA: 1:45 - loss: 1.171 - ETA: 1:43 - loss: 1.170 - ETA: 1:42 - loss: 1.169 - ETA: 1:40 - loss: 1.170 - ETA: 1:39 - loss: 1.171 - ETA: 1:37 - loss: 1.171 - ETA: 1:35 - loss: 1.171 - ETA: 1:34 - loss: 1.171 - ETA: 1:32 - loss: 1.173 - ETA: 1:31 - loss: 1.172 - ETA: 1:29 - loss: 1.171 - ETA: 1:27 - loss: 1.171 - ETA: 1:26 - loss: 1.170 - ETA: 1:24 - loss: 1.169 - ETA: 1:23 - loss: 1.167 - ETA: 1:21 - loss: 1.167 - ETA: 1:19 - loss: 1.167 - ETA: 1:18 - loss: 1.167 - ETA: 1:16 - loss: 1.167 - ETA: 1:15 - loss: 1.165 - ETA: 1:13 - loss: 1.166 - ETA: 1:11 - loss: 1.166 - ETA: 1:10 - loss: 1.165 - ETA: 1:08 - loss: 1.165 - ETA: 1:07 - loss: 1.164 - ETA: 1:05 - loss: 1.164 - ETA: 1:03 - loss: 1.164 - ETA: 1:02 - loss: 1.163 - ETA: 1:00 - loss: 1.162 - ETA: 59s - loss: 1.164 - ETA: 57s - loss: 1.16 - ETA: 55s - loss: 1.16 - ETA: 54s - loss: 1.16 - ETA: 52s - loss: 1.16 - ETA: 51s - loss: 1.16 - ETA: 49s - loss: 1.16 - ETA: 47s - loss: 1.16 - ETA: 46s - loss: 1.16 - ETA: 44s - loss: 1.16 - ETA: 43s - loss: 1.16 - ETA: 41s - loss: 1.16 - ETA: 39s - loss: 1.16 - ETA: 38s - loss: 1.16 - ETA: 36s - loss: 1.16 - ETA: 35s - loss: 1.16 - ETA: 33s - loss: 1.16 - ETA: 31s - loss: 1.16 - ETA: 30s - loss: 1.16 - ETA: 28s - loss: 1.16 - ETA: 27s - loss: 1.16 - ETA: 25s - loss: 1.16 - ETA: 23s - loss: 1.16 - ETA: 22s - loss: 1.16 - ETA: 20s - loss: 1.16 - ETA: 19s - loss: 1.16 - ETA: 17s - loss: 1.16 - ETA: 15s - loss: 1.16 - ETA: 14s - loss: 1.16 - ETA: 12s - loss: 1.16 - ETA: 11s - loss: 1.16 - ETA: 9s - loss: 1.1620 - ETA: 7s - loss: 1.161 - ETA: 6s - loss: 1.160 - ETA: 4s - loss: 1.160 - ETA: 3s - loss: 1.161 - ETA: 1s - loss: 1.160 - 301s 2s/step - loss: 1.1614 - val_loss: 1.2303\n",
      "\n",
      "Epoch 00009: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 10/30\n",
      "187/187 [==============================] - ETA: 4:57 - loss: 1.100 - ETA: 4:55 - loss: 1.146 - ETA: 4:53 - loss: 1.117 - ETA: 4:51 - loss: 1.137 - ETA: 4:50 - loss: 1.157 - ETA: 4:49 - loss: 1.145 - ETA: 4:47 - loss: 1.161 - ETA: 4:46 - loss: 1.165 - ETA: 4:44 - loss: 1.158 - ETA: 4:43 - loss: 1.148 - ETA: 4:41 - loss: 1.128 - ETA: 4:39 - loss: 1.130 - ETA: 4:38 - loss: 1.122 - ETA: 4:36 - loss: 1.118 - ETA: 4:35 - loss: 1.123 - ETA: 4:34 - loss: 1.121 - ETA: 4:33 - loss: 1.116 - ETA: 4:31 - loss: 1.120 - ETA: 4:30 - loss: 1.119 - ETA: 4:28 - loss: 1.118 - ETA: 4:26 - loss: 1.116 - ETA: 4:25 - loss: 1.117 - ETA: 4:23 - loss: 1.113 - ETA: 4:21 - loss: 1.117 - ETA: 4:20 - loss: 1.119 - ETA: 4:18 - loss: 1.118 - ETA: 4:16 - loss: 1.117 - ETA: 4:15 - loss: 1.113 - ETA: 4:13 - loss: 1.113 - ETA: 4:11 - loss: 1.114 - ETA: 4:10 - loss: 1.121 - ETA: 4:08 - loss: 1.123 - ETA: 4:07 - loss: 1.125 - ETA: 4:05 - loss: 1.123 - ETA: 4:03 - loss: 1.124 - ETA: 4:02 - loss: 1.124 - ETA: 4:00 - loss: 1.124 - ETA: 3:58 - loss: 1.122 - ETA: 3:57 - loss: 1.122 - ETA: 3:55 - loss: 1.121 - ETA: 3:54 - loss: 1.120 - ETA: 3:52 - loss: 1.119 - ETA: 3:50 - loss: 1.118 - ETA: 3:49 - loss: 1.118 - ETA: 3:47 - loss: 1.118 - ETA: 3:45 - loss: 1.120 - ETA: 3:44 - loss: 1.121 - ETA: 3:42 - loss: 1.120 - ETA: 3:41 - loss: 1.118 - ETA: 3:39 - loss: 1.118 - ETA: 3:37 - loss: 1.120 - ETA: 3:36 - loss: 1.119 - ETA: 3:34 - loss: 1.120 - ETA: 3:33 - loss: 1.117 - ETA: 3:31 - loss: 1.116 - ETA: 3:29 - loss: 1.117 - ETA: 3:28 - loss: 1.117 - ETA: 3:26 - loss: 1.118 - ETA: 3:25 - loss: 1.117 - ETA: 3:23 - loss: 1.115 - ETA: 3:21 - loss: 1.118 - ETA: 3:20 - loss: 1.120 - ETA: 3:18 - loss: 1.118 - ETA: 3:17 - loss: 1.115 - ETA: 3:15 - loss: 1.115 - ETA: 3:13 - loss: 1.117 - ETA: 3:12 - loss: 1.119 - ETA: 3:10 - loss: 1.117 - ETA: 3:09 - loss: 1.117 - ETA: 3:07 - loss: 1.118 - ETA: 3:05 - loss: 1.117 - ETA: 3:04 - loss: 1.114 - ETA: 3:02 - loss: 1.114 - ETA: 3:00 - loss: 1.116 - ETA: 2:59 - loss: 1.117 - ETA: 2:57 - loss: 1.116 - ETA: 2:56 - loss: 1.118 - ETA: 2:54 - loss: 1.118 - ETA: 2:52 - loss: 1.117 - ETA: 2:51 - loss: 1.120 - ETA: 2:49 - loss: 1.120 - ETA: 2:48 - loss: 1.118 - ETA: 2:46 - loss: 1.119 - ETA: 2:44 - loss: 1.120 - ETA: 2:43 - loss: 1.122 - ETA: 2:41 - loss: 1.122 - ETA: 2:40 - loss: 1.124 - ETA: 2:38 - loss: 1.125 - ETA: 2:36 - loss: 1.124 - ETA: 2:35 - loss: 1.125 - ETA: 2:33 - loss: 1.125 - ETA: 2:32 - loss: 1.125 - ETA: 2:30 - loss: 1.123 - ETA: 2:28 - loss: 1.121 - ETA: 2:27 - loss: 1.123 - ETA: 2:25 - loss: 1.122 - ETA: 2:24 - loss: 1.123 - ETA: 2:22 - loss: 1.124 - ETA: 2:20 - loss: 1.122 - ETA: 2:19 - loss: 1.121 - ETA: 2:17 - loss: 1.122 - ETA: 2:16 - loss: 1.120 - ETA: 2:14 - loss: 1.120 - ETA: 2:12 - loss: 1.119 - ETA: 2:11 - loss: 1.117 - ETA: 2:09 - loss: 1.117 - ETA: 2:08 - loss: 1.117 - ETA: 2:06 - loss: 1.118 - ETA: 2:04 - loss: 1.119 - ETA: 2:03 - loss: 1.119 - ETA: 2:01 - loss: 1.118 - ETA: 2:00 - loss: 1.119 - ETA: 1:58 - loss: 1.119 - ETA: 1:56 - loss: 1.120 - ETA: 1:55 - loss: 1.120 - ETA: 1:53 - loss: 1.120 - ETA: 1:52 - loss: 1.119 - ETA: 1:50 - loss: 1.119 - ETA: 1:48 - loss: 1.120 - ETA: 1:47 - loss: 1.120 - ETA: 1:45 - loss: 1.118 - ETA: 1:44 - loss: 1.119 - ETA: 1:42 - loss: 1.120 - ETA: 1:40 - loss: 1.119 - ETA: 1:39 - loss: 1.119 - ETA: 1:37 - loss: 1.119 - ETA: 1:36 - loss: 1.118 - ETA: 1:34 - loss: 1.118 - ETA: 1:32 - loss: 1.117 - ETA: 1:31 - loss: 1.115 - ETA: 1:29 - loss: 1.115 - ETA: 1:28 - loss: 1.114 - ETA: 1:26 - loss: 1.114 - ETA: 1:24 - loss: 1.115 - ETA: 1:23 - loss: 1.114 - ETA: 1:21 - loss: 1.114 - ETA: 1:20 - loss: 1.113 - ETA: 1:18 - loss: 1.112 - ETA: 1:16 - loss: 1.112 - ETA: 1:15 - loss: 1.113 - ETA: 1:13 - loss: 1.112 - ETA: 1:12 - loss: 1.113 - ETA: 1:10 - loss: 1.112 - ETA: 1:08 - loss: 1.112 - ETA: 1:07 - loss: 1.112 - ETA: 1:05 - loss: 1.112 - ETA: 1:04 - loss: 1.114 - ETA: 1:02 - loss: 1.114 - ETA: 1:00 - loss: 1.115 - ETA: 59s - loss: 1.114 - ETA: 57s - loss: 1.11 - ETA: 56s - loss: 1.11 - ETA: 54s - loss: 1.11 - ETA: 52s - loss: 1.11 - ETA: 51s - loss: 1.11 - ETA: 49s - loss: 1.11 - ETA: 48s - loss: 1.11 - ETA: 46s - loss: 1.11 - ETA: 44s - loss: 1.11 - ETA: 43s - loss: 1.11 - ETA: 41s - loss: 1.11 - ETA: 40s - loss: 1.11 - ETA: 38s - loss: 1.11 - ETA: 36s - loss: 1.11 - ETA: 35s - loss: 1.11 - ETA: 33s - loss: 1.11 - ETA: 32s - loss: 1.10 - ETA: 30s - loss: 1.11 - ETA: 28s - loss: 1.11 - ETA: 27s - loss: 1.11 - ETA: 25s - loss: 1.11 - ETA: 24s - loss: 1.11 - ETA: 22s - loss: 1.11 - ETA: 20s - loss: 1.11 - ETA: 19s - loss: 1.11 - ETA: 17s - loss: 1.11 - ETA: 16s - loss: 1.11 - ETA: 14s - loss: 1.11 - ETA: 12s - loss: 1.11 - ETA: 11s - loss: 1.11 - ETA: 9s - loss: 1.1117 - ETA: 8s - loss: 1.112 - ETA: 6s - loss: 1.111 - ETA: 4s - loss: 1.112 - ETA: 3s - loss: 1.112 - ETA: 1s - loss: 1.113 - 301s 2s/step - loss: 1.1130 - val_loss: 1.3963\n",
      "\n",
      "Epoch 00010: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:57 - loss: 1.302 - ETA: 4:55 - loss: 1.205 - ETA: 4:54 - loss: 1.196 - ETA: 4:52 - loss: 1.226 - ETA: 4:50 - loss: 1.224 - ETA: 4:49 - loss: 1.193 - ETA: 4:47 - loss: 1.172 - ETA: 4:46 - loss: 1.153 - ETA: 4:44 - loss: 1.140 - ETA: 4:43 - loss: 1.158 - ETA: 4:41 - loss: 1.175 - ETA: 4:40 - loss: 1.180 - ETA: 4:38 - loss: 1.186 - ETA: 4:36 - loss: 1.178 - ETA: 4:35 - loss: 1.175 - ETA: 4:33 - loss: 1.168 - ETA: 4:32 - loss: 1.161 - ETA: 4:30 - loss: 1.158 - ETA: 4:28 - loss: 1.164 - ETA: 4:27 - loss: 1.164 - ETA: 4:25 - loss: 1.165 - ETA: 4:24 - loss: 1.163 - ETA: 4:22 - loss: 1.164 - ETA: 4:20 - loss: 1.164 - ETA: 4:19 - loss: 1.163 - ETA: 4:17 - loss: 1.162 - ETA: 4:16 - loss: 1.156 - ETA: 4:14 - loss: 1.157 - ETA: 4:12 - loss: 1.155 - ETA: 4:11 - loss: 1.151 - ETA: 4:09 - loss: 1.153 - ETA: 4:07 - loss: 1.154 - ETA: 4:06 - loss: 1.155 - ETA: 4:04 - loss: 1.153 - ETA: 4:03 - loss: 1.146 - ETA: 4:01 - loss: 1.147 - ETA: 3:59 - loss: 1.142 - ETA: 3:58 - loss: 1.141 - ETA: 3:56 - loss: 1.140 - ETA: 3:55 - loss: 1.142 - ETA: 3:53 - loss: 1.145 - ETA: 3:51 - loss: 1.147 - ETA: 3:50 - loss: 1.150 - ETA: 3:48 - loss: 1.153 - ETA: 3:47 - loss: 1.156 - ETA: 3:45 - loss: 1.152 - ETA: 3:43 - loss: 1.152 - ETA: 3:42 - loss: 1.147 - ETA: 3:40 - loss: 1.148 - ETA: 3:39 - loss: 1.146 - ETA: 3:37 - loss: 1.145 - ETA: 3:35 - loss: 1.141 - ETA: 3:34 - loss: 1.140 - ETA: 3:32 - loss: 1.143 - ETA: 3:30 - loss: 1.143 - ETA: 3:29 - loss: 1.137 - ETA: 3:27 - loss: 1.136 - ETA: 3:26 - loss: 1.135 - ETA: 3:24 - loss: 1.136 - ETA: 3:22 - loss: 1.136 - ETA: 3:21 - loss: 1.137 - ETA: 3:19 - loss: 1.134 - ETA: 3:18 - loss: 1.131 - ETA: 3:16 - loss: 1.129 - ETA: 3:14 - loss: 1.130 - ETA: 3:13 - loss: 1.129 - ETA: 3:11 - loss: 1.126 - ETA: 3:10 - loss: 1.125 - ETA: 3:08 - loss: 1.126 - ETA: 3:06 - loss: 1.126 - ETA: 3:05 - loss: 1.125 - ETA: 3:03 - loss: 1.122 - ETA: 3:02 - loss: 1.121 - ETA: 3:00 - loss: 1.121 - ETA: 2:58 - loss: 1.119 - ETA: 2:57 - loss: 1.120 - ETA: 2:55 - loss: 1.119 - ETA: 2:54 - loss: 1.120 - ETA: 2:52 - loss: 1.117 - ETA: 2:50 - loss: 1.119 - ETA: 2:49 - loss: 1.116 - ETA: 2:47 - loss: 1.118 - ETA: 2:46 - loss: 1.118 - ETA: 2:44 - loss: 1.116 - ETA: 2:42 - loss: 1.115 - ETA: 2:41 - loss: 1.114 - ETA: 2:39 - loss: 1.117 - ETA: 2:38 - loss: 1.118 - ETA: 2:36 - loss: 1.117 - ETA: 2:34 - loss: 1.118 - ETA: 2:33 - loss: 1.118 - ETA: 2:31 - loss: 1.118 - ETA: 2:30 - loss: 1.120 - ETA: 2:28 - loss: 1.120 - ETA: 2:26 - loss: 1.122 - ETA: 2:25 - loss: 1.121 - ETA: 2:23 - loss: 1.121 - ETA: 2:22 - loss: 1.121 - ETA: 2:20 - loss: 1.121 - ETA: 2:18 - loss: 1.120 - ETA: 2:17 - loss: 1.120 - ETA: 2:15 - loss: 1.118 - ETA: 2:14 - loss: 1.116 - ETA: 2:12 - loss: 1.116 - ETA: 2:10 - loss: 1.117 - ETA: 2:09 - loss: 1.115 - ETA: 2:07 - loss: 1.117 - ETA: 2:06 - loss: 1.116 - ETA: 2:04 - loss: 1.116 - ETA: 2:02 - loss: 1.115 - ETA: 2:01 - loss: 1.114 - ETA: 1:59 - loss: 1.114 - ETA: 1:58 - loss: 1.115 - ETA: 1:56 - loss: 1.115 - ETA: 1:54 - loss: 1.115 - ETA: 1:53 - loss: 1.114 - ETA: 1:51 - loss: 1.116 - ETA: 1:50 - loss: 1.116 - ETA: 1:48 - loss: 1.116 - ETA: 1:46 - loss: 1.116 - ETA: 1:45 - loss: 1.115 - ETA: 1:43 - loss: 1.115 - ETA: 1:42 - loss: 1.116 - ETA: 1:40 - loss: 1.116 - ETA: 1:39 - loss: 1.116 - ETA: 1:37 - loss: 1.116 - ETA: 1:35 - loss: 1.117 - ETA: 1:34 - loss: 1.116 - ETA: 1:32 - loss: 1.116 - ETA: 1:31 - loss: 1.116 - ETA: 1:29 - loss: 1.117 - ETA: 1:27 - loss: 1.117 - ETA: 1:26 - loss: 1.118 - ETA: 1:24 - loss: 1.120 - ETA: 1:23 - loss: 1.118 - ETA: 1:21 - loss: 1.118 - ETA: 1:19 - loss: 1.118 - ETA: 1:18 - loss: 1.117 - ETA: 1:16 - loss: 1.117 - ETA: 1:15 - loss: 1.116 - ETA: 1:13 - loss: 1.117 - ETA: 1:11 - loss: 1.117 - ETA: 1:10 - loss: 1.118 - ETA: 1:08 - loss: 1.118 - ETA: 1:07 - loss: 1.117 - ETA: 1:05 - loss: 1.116 - ETA: 1:03 - loss: 1.115 - ETA: 1:02 - loss: 1.115 - ETA: 1:00 - loss: 1.114 - ETA: 59s - loss: 1.114 - ETA: 57s - loss: 1.11 - ETA: 55s - loss: 1.11 - ETA: 54s - loss: 1.11 - ETA: 52s - loss: 1.11 - ETA: 51s - loss: 1.11 - ETA: 49s - loss: 1.11 - ETA: 47s - loss: 1.11 - ETA: 46s - loss: 1.11 - ETA: 44s - loss: 1.11 - ETA: 43s - loss: 1.11 - ETA: 41s - loss: 1.11 - ETA: 39s - loss: 1.11 - ETA: 38s - loss: 1.11 - ETA: 36s - loss: 1.11 - ETA: 35s - loss: 1.11 - ETA: 33s - loss: 1.11 - ETA: 31s - loss: 1.11 - ETA: 30s - loss: 1.11 - ETA: 28s - loss: 1.11 - ETA: 27s - loss: 1.11 - ETA: 25s - loss: 1.11 - ETA: 23s - loss: 1.11 - ETA: 22s - loss: 1.11 - ETA: 20s - loss: 1.11 - ETA: 19s - loss: 1.11 - ETA: 17s - loss: 1.11 - ETA: 15s - loss: 1.11 - ETA: 14s - loss: 1.11 - ETA: 12s - loss: 1.11 - ETA: 11s - loss: 1.11 - ETA: 9s - loss: 1.1137 - ETA: 7s - loss: 1.113 - ETA: 6s - loss: 1.113 - ETA: 4s - loss: 1.113 - ETA: 3s - loss: 1.113 - ETA: 1s - loss: 1.112 - 301s 2s/step - loss: 1.1113 - val_loss: 1.4050\n",
      "\n",
      "Epoch 00011: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/30\n",
      "187/187 [==============================] - ETA: 4:57 - loss: 1.084 - ETA: 4:54 - loss: 1.030 - ETA: 4:53 - loss: 1.044 - ETA: 4:51 - loss: 1.057 - ETA: 4:50 - loss: 1.076 - ETA: 4:48 - loss: 1.050 - ETA: 4:47 - loss: 1.073 - ETA: 5:09 - loss: 1.072 - ETA: 5:10 - loss: 1.071 - ETA: 5:08 - loss: 1.064 - ETA: 5:06 - loss: 1.069 - ETA: 5:12 - loss: 1.063 - ETA: 5:15 - loss: 1.061 - ETA: 5:11 - loss: 1.074 - ETA: 5:08 - loss: 1.068 - ETA: 5:04 - loss: 1.071 - ETA: 5:01 - loss: 1.058 - ETA: 4:59 - loss: 1.052 - ETA: 4:56 - loss: 1.055 - ETA: 4:54 - loss: 1.064 - ETA: 4:52 - loss: 1.057 - ETA: 4:49 - loss: 1.050 - ETA: 4:47 - loss: 1.052 - ETA: 4:45 - loss: 1.050 - ETA: 4:42 - loss: 1.050 - ETA: 4:40 - loss: 1.053 - ETA: 4:38 - loss: 1.049 - ETA: 4:36 - loss: 1.053 - ETA: 4:34 - loss: 1.057 - ETA: 4:31 - loss: 1.062 - ETA: 4:29 - loss: 1.063 - ETA: 4:27 - loss: 1.069 - ETA: 4:25 - loss: 1.073 - ETA: 4:23 - loss: 1.071 - ETA: 4:21 - loss: 1.074 - ETA: 4:19 - loss: 1.075 - ETA: 4:18 - loss: 1.070 - ETA: 4:16 - loss: 1.074 - ETA: 4:14 - loss: 1.075 - ETA: 4:12 - loss: 1.073 - ETA: 4:10 - loss: 1.073 - ETA: 4:08 - loss: 1.074 - ETA: 4:06 - loss: 1.076 - ETA: 4:04 - loss: 1.078 - ETA: 4:03 - loss: 1.075 - ETA: 4:01 - loss: 1.073 - ETA: 3:59 - loss: 1.079 - ETA: 3:57 - loss: 1.079 - ETA: 3:55 - loss: 1.077 - ETA: 3:53 - loss: 1.074 - ETA: 3:52 - loss: 1.075 - ETA: 3:50 - loss: 1.077 - ETA: 3:48 - loss: 1.075 - ETA: 3:46 - loss: 1.077 - ETA: 3:44 - loss: 1.078 - ETA: 3:43 - loss: 1.081 - ETA: 3:41 - loss: 1.079 - ETA: 3:39 - loss: 1.076 - ETA: 3:37 - loss: 1.076 - ETA: 3:35 - loss: 1.075 - ETA: 3:34 - loss: 1.075 - ETA: 3:32 - loss: 1.076 - ETA: 3:30 - loss: 1.077 - ETA: 3:28 - loss: 1.077 - ETA: 3:27 - loss: 1.078 - ETA: 3:25 - loss: 1.077 - ETA: 3:23 - loss: 1.077 - ETA: 3:21 - loss: 1.078 - ETA: 3:20 - loss: 1.078 - ETA: 3:18 - loss: 1.077 - ETA: 3:16 - loss: 1.077 - ETA: 3:15 - loss: 1.075 - ETA: 3:13 - loss: 1.072 - ETA: 3:11 - loss: 1.071 - ETA: 3:09 - loss: 1.072 - ETA: 3:08 - loss: 1.070 - ETA: 3:06 - loss: 1.069 - ETA: 3:04 - loss: 1.068 - ETA: 3:02 - loss: 1.070 - ETA: 3:01 - loss: 1.070 - ETA: 2:59 - loss: 1.073 - ETA: 2:57 - loss: 1.071 - ETA: 2:56 - loss: 1.071 - ETA: 2:54 - loss: 1.072 - ETA: 2:52 - loss: 1.074 - ETA: 2:50 - loss: 1.073 - ETA: 2:49 - loss: 1.073 - ETA: 2:47 - loss: 1.075 - ETA: 2:45 - loss: 1.075 - ETA: 2:44 - loss: 1.074 - ETA: 2:42 - loss: 1.072 - ETA: 2:40 - loss: 1.072 - ETA: 2:38 - loss: 1.072 - ETA: 2:37 - loss: 1.072 - ETA: 2:35 - loss: 1.071 - ETA: 2:33 - loss: 1.072 - ETA: 2:32 - loss: 1.071 - ETA: 2:30 - loss: 1.072 - ETA: 2:28 - loss: 1.072 - ETA: 2:27 - loss: 1.073 - ETA: 2:25 - loss: 1.074 - ETA: 2:23 - loss: 1.074 - ETA: 2:21 - loss: 1.074 - ETA: 2:20 - loss: 1.074 - ETA: 2:18 - loss: 1.075 - ETA: 2:16 - loss: 1.076 - ETA: 2:15 - loss: 1.075 - ETA: 2:13 - loss: 1.075 - ETA: 2:11 - loss: 1.074 - ETA: 2:09 - loss: 1.073 - ETA: 2:08 - loss: 1.072 - ETA: 2:06 - loss: 1.073 - ETA: 2:04 - loss: 1.073 - ETA: 2:03 - loss: 1.072 - ETA: 2:01 - loss: 1.072 - ETA: 1:59 - loss: 1.072 - ETA: 1:58 - loss: 1.072 - ETA: 1:56 - loss: 1.073 - ETA: 1:54 - loss: 1.074 - ETA: 1:52 - loss: 1.073 - ETA: 1:51 - loss: 1.073 - ETA: 1:49 - loss: 1.073 - ETA: 1:47 - loss: 1.074 - ETA: 1:46 - loss: 1.075 - ETA: 1:44 - loss: 1.074 - ETA: 1:42 - loss: 1.074 - ETA: 1:41 - loss: 1.073 - ETA: 1:39 - loss: 1.074 - ETA: 1:37 - loss: 1.073 - ETA: 1:36 - loss: 1.073 - ETA: 1:34 - loss: 1.073 - ETA: 1:32 - loss: 1.073 - ETA: 1:30 - loss: 1.073 - ETA: 1:29 - loss: 1.073 - ETA: 1:27 - loss: 1.074 - ETA: 1:25 - loss: 1.074 - ETA: 1:24 - loss: 1.073 - ETA: 1:22 - loss: 1.073 - ETA: 1:20 - loss: 1.073 - ETA: 1:19 - loss: 1.072 - ETA: 1:17 - loss: 1.072 - ETA: 1:15 - loss: 1.074 - ETA: 1:14 - loss: 1.074 - ETA: 1:12 - loss: 1.075 - ETA: 1:10 - loss: 1.076 - ETA: 1:09 - loss: 1.075 - ETA: 1:07 - loss: 1.074 - ETA: 1:05 - loss: 1.075 - ETA: 1:03 - loss: 1.074 - ETA: 1:02 - loss: 1.075 - ETA: 1:00 - loss: 1.074 - ETA: 58s - loss: 1.075 - ETA: 57s - loss: 1.07 - ETA: 55s - loss: 1.07 - ETA: 53s - loss: 1.07 - ETA: 52s - loss: 1.07 - ETA: 50s - loss: 1.07 - ETA: 48s - loss: 1.07 - ETA: 47s - loss: 1.07 - ETA: 45s - loss: 1.07 - ETA: 43s - loss: 1.07 - ETA: 42s - loss: 1.07 - ETA: 40s - loss: 1.07 - ETA: 38s - loss: 1.07 - ETA: 36s - loss: 1.07 - ETA: 35s - loss: 1.07 - ETA: 33s - loss: 1.07 - ETA: 31s - loss: 1.07 - ETA: 30s - loss: 1.07 - ETA: 28s - loss: 1.07 - ETA: 26s - loss: 1.07 - ETA: 25s - loss: 1.07 - ETA: 23s - loss: 1.07 - ETA: 21s - loss: 1.07 - ETA: 20s - loss: 1.07 - ETA: 18s - loss: 1.07 - ETA: 16s - loss: 1.07 - ETA: 15s - loss: 1.07 - ETA: 13s - loss: 1.07 - ETA: 11s - loss: 1.07 - ETA: 10s - loss: 1.07 - ETA: 8s - loss: 1.0711 - ETA: 6s - loss: 1.070 - ETA: 5s - loss: 1.070 - ETA: 3s - loss: 1.070 - ETA: 1s - loss: 1.069 - 317s 2s/step - loss: 1.0692 - val_loss: 1.2968\n",
      "\n",
      "Epoch 00012: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:09 - loss: 1.083 - ETA: 5:09 - loss: 1.141 - ETA: 5:07 - loss: 1.053 - ETA: 5:05 - loss: 1.085 - ETA: 5:04 - loss: 1.083 - ETA: 5:02 - loss: 1.079 - ETA: 5:00 - loss: 1.060 - ETA: 4:59 - loss: 1.051 - ETA: 4:57 - loss: 1.054 - ETA: 4:55 - loss: 1.055 - ETA: 4:53 - loss: 1.053 - ETA: 4:52 - loss: 1.063 - ETA: 4:50 - loss: 1.076 - ETA: 4:48 - loss: 1.073 - ETA: 4:47 - loss: 1.068 - ETA: 4:45 - loss: 1.075 - ETA: 4:44 - loss: 1.067 - ETA: 4:42 - loss: 1.065 - ETA: 4:40 - loss: 1.064 - ETA: 4:39 - loss: 1.062 - ETA: 4:37 - loss: 1.064 - ETA: 4:35 - loss: 1.069 - ETA: 4:33 - loss: 1.071 - ETA: 4:32 - loss: 1.071 - ETA: 4:30 - loss: 1.062 - ETA: 4:28 - loss: 1.056 - ETA: 4:27 - loss: 1.054 - ETA: 4:25 - loss: 1.053 - ETA: 4:23 - loss: 1.056 - ETA: 4:22 - loss: 1.051 - ETA: 4:20 - loss: 1.045 - ETA: 4:18 - loss: 1.049 - ETA: 4:16 - loss: 1.047 - ETA: 4:15 - loss: 1.047 - ETA: 4:13 - loss: 1.046 - ETA: 4:11 - loss: 1.045 - ETA: 4:10 - loss: 1.048 - ETA: 4:08 - loss: 1.050 - ETA: 4:06 - loss: 1.048 - ETA: 4:05 - loss: 1.049 - ETA: 4:03 - loss: 1.047 - ETA: 4:01 - loss: 1.044 - ETA: 4:00 - loss: 1.041 - ETA: 3:58 - loss: 1.042 - ETA: 3:56 - loss: 1.039 - ETA: 3:55 - loss: 1.040 - ETA: 3:53 - loss: 1.041 - ETA: 3:51 - loss: 1.038 - ETA: 3:50 - loss: 1.038 - ETA: 3:48 - loss: 1.039 - ETA: 3:46 - loss: 1.036 - ETA: 3:45 - loss: 1.038 - ETA: 3:43 - loss: 1.040 - ETA: 3:41 - loss: 1.041 - ETA: 3:40 - loss: 1.040 - ETA: 3:38 - loss: 1.039 - ETA: 3:36 - loss: 1.037 - ETA: 3:35 - loss: 1.036 - ETA: 3:33 - loss: 1.040 - ETA: 3:31 - loss: 1.043 - ETA: 3:30 - loss: 1.044 - ETA: 3:28 - loss: 1.042 - ETA: 3:26 - loss: 1.041 - ETA: 3:25 - loss: 1.043 - ETA: 3:23 - loss: 1.044 - ETA: 3:21 - loss: 1.045 - ETA: 3:20 - loss: 1.047 - ETA: 3:18 - loss: 1.048 - ETA: 3:16 - loss: 1.049 - ETA: 3:15 - loss: 1.049 - ETA: 3:13 - loss: 1.049 - ETA: 3:11 - loss: 1.049 - ETA: 3:10 - loss: 1.051 - ETA: 3:08 - loss: 1.052 - ETA: 3:06 - loss: 1.053 - ETA: 3:05 - loss: 1.055 - ETA: 3:03 - loss: 1.053 - ETA: 3:01 - loss: 1.054 - ETA: 3:00 - loss: 1.055 - ETA: 2:58 - loss: 1.056 - ETA: 2:56 - loss: 1.057 - ETA: 2:55 - loss: 1.058 - ETA: 2:53 - loss: 1.060 - ETA: 2:51 - loss: 1.059 - ETA: 2:50 - loss: 1.060 - ETA: 2:48 - loss: 1.061 - ETA: 2:46 - loss: 1.060 - ETA: 2:45 - loss: 1.059 - ETA: 2:43 - loss: 1.061 - ETA: 2:41 - loss: 1.061 - ETA: 2:40 - loss: 1.060 - ETA: 2:38 - loss: 1.060 - ETA: 2:36 - loss: 1.060 - ETA: 2:35 - loss: 1.062 - ETA: 2:33 - loss: 1.062 - ETA: 2:31 - loss: 1.063 - ETA: 2:30 - loss: 1.063 - ETA: 2:28 - loss: 1.062 - ETA: 2:26 - loss: 1.064 - ETA: 2:25 - loss: 1.063 - ETA: 2:23 - loss: 1.063 - ETA: 2:21 - loss: 1.062 - ETA: 2:20 - loss: 1.061 - ETA: 2:18 - loss: 1.060 - ETA: 2:16 - loss: 1.060 - ETA: 2:15 - loss: 1.059 - ETA: 2:13 - loss: 1.058 - ETA: 2:11 - loss: 1.057 - ETA: 2:10 - loss: 1.056 - ETA: 2:08 - loss: 1.056 - ETA: 2:06 - loss: 1.057 - ETA: 2:05 - loss: 1.057 - ETA: 2:03 - loss: 1.055 - ETA: 2:01 - loss: 1.055 - ETA: 2:00 - loss: 1.054 - ETA: 1:58 - loss: 1.055 - ETA: 1:56 - loss: 1.056 - ETA: 1:55 - loss: 1.056 - ETA: 1:53 - loss: 1.055 - ETA: 1:51 - loss: 1.055 - ETA: 1:50 - loss: 1.055 - ETA: 1:48 - loss: 1.056 - ETA: 1:46 - loss: 1.057 - ETA: 1:45 - loss: 1.056 - ETA: 1:43 - loss: 1.057 - ETA: 1:41 - loss: 1.057 - ETA: 1:40 - loss: 1.056 - ETA: 1:38 - loss: 1.056 - ETA: 1:36 - loss: 1.055 - ETA: 1:35 - loss: 1.055 - ETA: 1:33 - loss: 1.055 - ETA: 1:31 - loss: 1.055 - ETA: 1:30 - loss: 1.054 - ETA: 1:28 - loss: 1.053 - ETA: 1:26 - loss: 1.053 - ETA: 1:25 - loss: 1.053 - ETA: 1:23 - loss: 1.053 - ETA: 1:21 - loss: 1.054 - ETA: 1:20 - loss: 1.055 - ETA: 1:18 - loss: 1.055 - ETA: 1:16 - loss: 1.055 - ETA: 1:15 - loss: 1.054 - ETA: 1:13 - loss: 1.054 - ETA: 1:11 - loss: 1.053 - ETA: 1:10 - loss: 1.052 - ETA: 1:08 - loss: 1.052 - ETA: 1:06 - loss: 1.052 - ETA: 1:05 - loss: 1.052 - ETA: 1:03 - loss: 1.052 - ETA: 1:01 - loss: 1.052 - ETA: 1:00 - loss: 1.051 - ETA: 58s - loss: 1.050 - ETA: 56s - loss: 1.05 - ETA: 55s - loss: 1.05 - ETA: 53s - loss: 1.05 - ETA: 51s - loss: 1.04 - ETA: 50s - loss: 1.05 - ETA: 48s - loss: 1.04 - ETA: 46s - loss: 1.04 - ETA: 45s - loss: 1.04 - ETA: 43s - loss: 1.04 - ETA: 41s - loss: 1.05 - ETA: 40s - loss: 1.05 - ETA: 38s - loss: 1.05 - ETA: 36s - loss: 1.05 - ETA: 35s - loss: 1.05 - ETA: 33s - loss: 1.05 - ETA: 31s - loss: 1.05 - ETA: 30s - loss: 1.05 - ETA: 28s - loss: 1.05 - ETA: 26s - loss: 1.05 - ETA: 25s - loss: 1.05 - ETA: 23s - loss: 1.05 - ETA: 21s - loss: 1.05 - ETA: 20s - loss: 1.05 - ETA: 18s - loss: 1.05 - ETA: 16s - loss: 1.05 - ETA: 15s - loss: 1.05 - ETA: 13s - loss: 1.05 - ETA: 11s - loss: 1.05 - ETA: 10s - loss: 1.05 - ETA: 8s - loss: 1.0539 - ETA: 6s - loss: 1.054 - ETA: 5s - loss: 1.054 - ETA: 3s - loss: 1.054 - ETA: 1s - loss: 1.054 - 314s 2s/step - loss: 1.0541 - val_loss: 1.3290\n",
      "\n",
      "Epoch 00013: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/30\n",
      "187/187 [==============================] - ETA: 5:08 - loss: 0.957 - ETA: 5:05 - loss: 1.024 - ETA: 5:05 - loss: 1.098 - ETA: 5:03 - loss: 1.133 - ETA: 5:02 - loss: 1.106 - ETA: 5:01 - loss: 1.079 - ETA: 4:59 - loss: 1.058 - ETA: 4:57 - loss: 1.064 - ETA: 4:55 - loss: 1.057 - ETA: 4:53 - loss: 1.050 - ETA: 4:52 - loss: 1.043 - ETA: 4:50 - loss: 1.056 - ETA: 4:48 - loss: 1.051 - ETA: 4:47 - loss: 1.053 - ETA: 4:45 - loss: 1.045 - ETA: 4:43 - loss: 1.054 - ETA: 4:42 - loss: 1.044 - ETA: 4:40 - loss: 1.045 - ETA: 4:39 - loss: 1.042 - ETA: 4:37 - loss: 1.041 - ETA: 4:35 - loss: 1.049 - ETA: 4:33 - loss: 1.053 - ETA: 4:32 - loss: 1.055 - ETA: 4:30 - loss: 1.049 - ETA: 4:28 - loss: 1.041 - ETA: 4:27 - loss: 1.041 - ETA: 4:25 - loss: 1.037 - ETA: 4:23 - loss: 1.038 - ETA: 4:21 - loss: 1.040 - ETA: 4:20 - loss: 1.040 - ETA: 4:18 - loss: 1.040 - ETA: 4:16 - loss: 1.044 - ETA: 4:15 - loss: 1.047 - ETA: 4:13 - loss: 1.045 - ETA: 4:12 - loss: 1.044 - ETA: 4:10 - loss: 1.039 - ETA: 4:08 - loss: 1.039 - ETA: 4:07 - loss: 1.040 - ETA: 4:05 - loss: 1.040 - ETA: 4:03 - loss: 1.041 - ETA: 4:02 - loss: 1.045 - ETA: 4:00 - loss: 1.047 - ETA: 3:58 - loss: 1.047 - ETA: 3:57 - loss: 1.047 - ETA: 3:55 - loss: 1.050 - ETA: 3:53 - loss: 1.051 - ETA: 3:52 - loss: 1.054 - ETA: 3:50 - loss: 1.054 - ETA: 3:48 - loss: 1.055 - ETA: 3:47 - loss: 1.054 - ETA: 3:45 - loss: 1.053 - ETA: 3:43 - loss: 1.053 - ETA: 3:42 - loss: 1.051 - ETA: 3:40 - loss: 1.050 - ETA: 3:38 - loss: 1.051 - ETA: 3:37 - loss: 1.052 - ETA: 3:35 - loss: 1.052 - ETA: 3:33 - loss: 1.052 - ETA: 3:32 - loss: 1.053 - ETA: 3:30 - loss: 1.057 - ETA: 3:28 - loss: 1.057 - ETA: 3:27 - loss: 1.055 - ETA: 3:25 - loss: 1.057 - ETA: 3:23 - loss: 1.059 - ETA: 3:22 - loss: 1.060 - ETA: 3:20 - loss: 1.058 - ETA: 3:18 - loss: 1.058 - ETA: 3:17 - loss: 1.056 - ETA: 3:15 - loss: 1.055 - ETA: 3:14 - loss: 1.057 - ETA: 3:12 - loss: 1.059 - ETA: 3:10 - loss: 1.061 - ETA: 3:09 - loss: 1.061 - ETA: 3:07 - loss: 1.062 - ETA: 3:05 - loss: 1.063 - ETA: 3:04 - loss: 1.061 - ETA: 3:02 - loss: 1.061 - ETA: 3:00 - loss: 1.062 - ETA: 2:59 - loss: 1.062 - ETA: 2:57 - loss: 1.062 - ETA: 2:55 - loss: 1.061 - ETA: 2:54 - loss: 1.060 - ETA: 2:52 - loss: 1.059 - ETA: 2:50 - loss: 1.060 - ETA: 2:49 - loss: 1.060 - ETA: 2:47 - loss: 1.059 - ETA: 2:45 - loss: 1.058 - ETA: 2:44 - loss: 1.059 - ETA: 2:42 - loss: 1.060 - ETA: 2:40 - loss: 1.062 - ETA: 2:39 - loss: 1.060 - ETA: 2:37 - loss: 1.060 - ETA: 2:36 - loss: 1.061 - ETA: 2:34 - loss: 1.060 - ETA: 2:32 - loss: 1.060 - ETA: 2:31 - loss: 1.061 - ETA: 2:29 - loss: 1.061 - ETA: 2:27 - loss: 1.061 - ETA: 2:26 - loss: 1.061 - ETA: 2:24 - loss: 1.060 - ETA: 2:22 - loss: 1.060 - ETA: 2:21 - loss: 1.059 - ETA: 2:19 - loss: 1.060 - ETA: 2:17 - loss: 1.060 - ETA: 2:16 - loss: 1.058 - ETA: 2:14 - loss: 1.057 - ETA: 2:12 - loss: 1.058 - ETA: 2:11 - loss: 1.056 - ETA: 2:09 - loss: 1.057 - ETA: 2:07 - loss: 1.056 - ETA: 2:06 - loss: 1.056 - ETA: 2:04 - loss: 1.057 - ETA: 2:02 - loss: 1.056 - ETA: 2:01 - loss: 1.054 - ETA: 1:59 - loss: 1.054 - ETA: 1:57 - loss: 1.054 - ETA: 1:56 - loss: 1.054 - ETA: 1:54 - loss: 1.055 - ETA: 1:52 - loss: 1.055 - ETA: 1:51 - loss: 1.055 - ETA: 1:49 - loss: 1.055 - ETA: 1:47 - loss: 1.053 - ETA: 1:46 - loss: 1.053 - ETA: 1:44 - loss: 1.053 - ETA: 1:42 - loss: 1.054 - ETA: 1:41 - loss: 1.054 - ETA: 1:39 - loss: 1.053 - ETA: 1:37 - loss: 1.052 - ETA: 1:36 - loss: 1.051 - ETA: 1:34 - loss: 1.051 - ETA: 1:32 - loss: 1.050 - ETA: 1:31 - loss: 1.051 - ETA: 1:29 - loss: 1.052 - ETA: 1:27 - loss: 1.051 - ETA: 1:26 - loss: 1.051 - ETA: 1:24 - loss: 1.051 - ETA: 1:22 - loss: 1.051 - ETA: 1:21 - loss: 1.051 - ETA: 1:19 - loss: 1.051 - ETA: 1:18 - loss: 1.051 - ETA: 1:16 - loss: 1.051 - ETA: 1:14 - loss: 1.052 - ETA: 1:13 - loss: 1.051 - ETA: 1:11 - loss: 1.051 - ETA: 1:09 - loss: 1.050 - ETA: 1:08 - loss: 1.051 - ETA: 1:06 - loss: 1.051 - ETA: 1:04 - loss: 1.050 - ETA: 1:03 - loss: 1.050 - ETA: 1:01 - loss: 1.049 - ETA: 59s - loss: 1.049 - ETA: 58s - loss: 1.04 - ETA: 56s - loss: 1.04 - ETA: 54s - loss: 1.04 - ETA: 53s - loss: 1.04 - ETA: 51s - loss: 1.04 - ETA: 49s - loss: 1.05 - ETA: 48s - loss: 1.05 - ETA: 46s - loss: 1.05 - ETA: 44s - loss: 1.05 - ETA: 43s - loss: 1.05 - ETA: 41s - loss: 1.05 - ETA: 39s - loss: 1.05 - ETA: 38s - loss: 1.05 - ETA: 36s - loss: 1.05 - ETA: 34s - loss: 1.05 - ETA: 33s - loss: 1.05 - ETA: 31s - loss: 1.05 - ETA: 29s - loss: 1.05 - ETA: 28s - loss: 1.05 - ETA: 26s - loss: 1.05 - ETA: 24s - loss: 1.05 - ETA: 23s - loss: 1.05 - ETA: 21s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 18s - loss: 1.05 - ETA: 16s - loss: 1.05 - ETA: 14s - loss: 1.05 - ETA: 13s - loss: 1.05 - ETA: 11s - loss: 1.05 - ETA: 9s - loss: 1.0521 - ETA: 8s - loss: 1.052 - ETA: 6s - loss: 1.052 - ETA: 4s - loss: 1.052 - ETA: 3s - loss: 1.052 - ETA: 1s - loss: 1.051 - 313s 2s/step - loss: 1.0512 - val_loss: 1.3826\n",
      "\n",
      "Epoch 00014: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:06 - loss: 1.176 - ETA: 5:05 - loss: 1.127 - ETA: 5:04 - loss: 1.079 - ETA: 5:02 - loss: 1.048 - ETA: 5:01 - loss: 1.047 - ETA: 4:59 - loss: 1.093 - ETA: 4:58 - loss: 1.075 - ETA: 4:57 - loss: 1.086 - ETA: 4:55 - loss: 1.075 - ETA: 4:53 - loss: 1.065 - ETA: 4:52 - loss: 1.062 - ETA: 4:50 - loss: 1.060 - ETA: 4:48 - loss: 1.067 - ETA: 4:47 - loss: 1.062 - ETA: 4:45 - loss: 1.069 - ETA: 4:43 - loss: 1.071 - ETA: 4:41 - loss: 1.061 - ETA: 4:40 - loss: 1.063 - ETA: 4:38 - loss: 1.066 - ETA: 4:36 - loss: 1.065 - ETA: 4:35 - loss: 1.064 - ETA: 4:33 - loss: 1.059 - ETA: 4:31 - loss: 1.055 - ETA: 4:30 - loss: 1.059 - ETA: 4:28 - loss: 1.058 - ETA: 4:26 - loss: 1.055 - ETA: 4:25 - loss: 1.056 - ETA: 4:23 - loss: 1.048 - ETA: 4:21 - loss: 1.044 - ETA: 4:20 - loss: 1.046 - ETA: 4:18 - loss: 1.050 - ETA: 4:16 - loss: 1.054 - ETA: 4:15 - loss: 1.055 - ETA: 4:13 - loss: 1.047 - ETA: 4:11 - loss: 1.049 - ETA: 4:10 - loss: 1.052 - ETA: 4:08 - loss: 1.050 - ETA: 4:07 - loss: 1.050 - ETA: 4:05 - loss: 1.048 - ETA: 4:03 - loss: 1.045 - ETA: 4:02 - loss: 1.045 - ETA: 4:00 - loss: 1.046 - ETA: 3:58 - loss: 1.044 - ETA: 3:57 - loss: 1.040 - ETA: 3:55 - loss: 1.037 - ETA: 3:53 - loss: 1.036 - ETA: 3:52 - loss: 1.038 - ETA: 3:50 - loss: 1.038 - ETA: 3:48 - loss: 1.036 - ETA: 3:47 - loss: 1.037 - ETA: 3:45 - loss: 1.043 - ETA: 3:43 - loss: 1.041 - ETA: 3:42 - loss: 1.041 - ETA: 3:40 - loss: 1.045 - ETA: 3:38 - loss: 1.044 - ETA: 3:37 - loss: 1.043 - ETA: 3:35 - loss: 1.043 - ETA: 3:33 - loss: 1.043 - ETA: 3:32 - loss: 1.040 - ETA: 3:30 - loss: 1.042 - ETA: 3:28 - loss: 1.041 - ETA: 3:27 - loss: 1.039 - ETA: 3:25 - loss: 1.041 - ETA: 3:23 - loss: 1.040 - ETA: 3:22 - loss: 1.040 - ETA: 3:20 - loss: 1.039 - ETA: 3:18 - loss: 1.038 - ETA: 3:17 - loss: 1.038 - ETA: 3:15 - loss: 1.036 - ETA: 3:13 - loss: 1.037 - ETA: 3:12 - loss: 1.036 - ETA: 3:10 - loss: 1.037 - ETA: 3:08 - loss: 1.040 - ETA: 3:07 - loss: 1.042 - ETA: 3:05 - loss: 1.041 - ETA: 3:03 - loss: 1.041 - ETA: 3:02 - loss: 1.040 - ETA: 3:00 - loss: 1.042 - ETA: 2:58 - loss: 1.042 - ETA: 2:57 - loss: 1.043 - ETA: 2:55 - loss: 1.041 - ETA: 2:54 - loss: 1.041 - ETA: 2:52 - loss: 1.040 - ETA: 2:50 - loss: 1.040 - ETA: 2:49 - loss: 1.040 - ETA: 2:47 - loss: 1.040 - ETA: 2:45 - loss: 1.040 - ETA: 2:44 - loss: 1.039 - ETA: 2:42 - loss: 1.038 - ETA: 2:40 - loss: 1.039 - ETA: 2:39 - loss: 1.038 - ETA: 2:37 - loss: 1.039 - ETA: 2:35 - loss: 1.039 - ETA: 2:34 - loss: 1.037 - ETA: 2:32 - loss: 1.037 - ETA: 2:30 - loss: 1.037 - ETA: 2:29 - loss: 1.035 - ETA: 2:27 - loss: 1.034 - ETA: 2:25 - loss: 1.033 - ETA: 2:24 - loss: 1.034 - ETA: 2:22 - loss: 1.033 - ETA: 2:20 - loss: 1.034 - ETA: 2:19 - loss: 1.035 - ETA: 2:17 - loss: 1.037 - ETA: 2:15 - loss: 1.038 - ETA: 2:14 - loss: 1.037 - ETA: 2:12 - loss: 1.036 - ETA: 2:10 - loss: 1.036 - ETA: 2:09 - loss: 1.037 - ETA: 2:07 - loss: 1.036 - ETA: 2:05 - loss: 1.036 - ETA: 2:04 - loss: 1.036 - ETA: 2:02 - loss: 1.038 - ETA: 2:00 - loss: 1.038 - ETA: 1:59 - loss: 1.038 - ETA: 1:57 - loss: 1.037 - ETA: 1:56 - loss: 1.037 - ETA: 1:54 - loss: 1.037 - ETA: 1:52 - loss: 1.037 - ETA: 1:51 - loss: 1.036 - ETA: 1:49 - loss: 1.036 - ETA: 1:47 - loss: 1.036 - ETA: 1:46 - loss: 1.036 - ETA: 1:44 - loss: 1.037 - ETA: 1:42 - loss: 1.036 - ETA: 1:41 - loss: 1.036 - ETA: 1:39 - loss: 1.036 - ETA: 1:37 - loss: 1.035 - ETA: 1:36 - loss: 1.036 - ETA: 1:34 - loss: 1.036 - ETA: 1:32 - loss: 1.036 - ETA: 1:31 - loss: 1.036 - ETA: 1:29 - loss: 1.036 - ETA: 1:27 - loss: 1.036 - ETA: 1:26 - loss: 1.036 - ETA: 1:24 - loss: 1.035 - ETA: 1:22 - loss: 1.035 - ETA: 1:21 - loss: 1.036 - ETA: 1:19 - loss: 1.037 - ETA: 1:17 - loss: 1.038 - ETA: 1:16 - loss: 1.038 - ETA: 1:14 - loss: 1.039 - ETA: 1:12 - loss: 1.037 - ETA: 1:11 - loss: 1.038 - ETA: 1:09 - loss: 1.038 - ETA: 1:07 - loss: 1.038 - ETA: 1:06 - loss: 1.038 - ETA: 1:04 - loss: 1.038 - ETA: 1:03 - loss: 1.037 - ETA: 1:01 - loss: 1.037 - ETA: 59s - loss: 1.036 - ETA: 58s - loss: 1.03 - ETA: 56s - loss: 1.03 - ETA: 54s - loss: 1.03 - ETA: 53s - loss: 1.03 - ETA: 51s - loss: 1.03 - ETA: 49s - loss: 1.03 - ETA: 48s - loss: 1.03 - ETA: 46s - loss: 1.03 - ETA: 44s - loss: 1.03 - ETA: 43s - loss: 1.03 - ETA: 41s - loss: 1.03 - ETA: 39s - loss: 1.03 - ETA: 38s - loss: 1.03 - ETA: 36s - loss: 1.03 - ETA: 34s - loss: 1.03 - ETA: 33s - loss: 1.03 - ETA: 31s - loss: 1.03 - ETA: 29s - loss: 1.03 - ETA: 28s - loss: 1.03 - ETA: 26s - loss: 1.03 - ETA: 24s - loss: 1.03 - ETA: 23s - loss: 1.03 - ETA: 21s - loss: 1.03 - ETA: 19s - loss: 1.03 - ETA: 18s - loss: 1.03 - ETA: 16s - loss: 1.03 - ETA: 14s - loss: 1.03 - ETA: 13s - loss: 1.03 - ETA: 11s - loss: 1.03 - ETA: 9s - loss: 1.0359 - ETA: 8s - loss: 1.035 - ETA: 6s - loss: 1.036 - ETA: 4s - loss: 1.034 - ETA: 3s - loss: 1.034 - ETA: 1s - loss: 1.035 - 313s 2s/step - loss: 1.0354 - val_loss: 1.3318\n",
      "\n",
      "Epoch 00015: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 16/30\n",
      "187/187 [==============================] - ETA: 5:05 - loss: 0.956 - ETA: 5:07 - loss: 0.978 - ETA: 5:05 - loss: 1.012 - ETA: 5:03 - loss: 1.019 - ETA: 5:01 - loss: 0.986 - ETA: 4:59 - loss: 1.020 - ETA: 4:57 - loss: 1.030 - ETA: 4:56 - loss: 1.051 - ETA: 4:54 - loss: 1.039 - ETA: 4:53 - loss: 1.033 - ETA: 4:51 - loss: 1.028 - ETA: 4:50 - loss: 1.026 - ETA: 4:48 - loss: 1.024 - ETA: 4:46 - loss: 1.028 - ETA: 4:45 - loss: 1.015 - ETA: 4:43 - loss: 1.015 - ETA: 4:42 - loss: 1.023 - ETA: 4:40 - loss: 1.022 - ETA: 4:38 - loss: 1.024 - ETA: 4:37 - loss: 1.023 - ETA: 4:35 - loss: 1.027 - ETA: 4:33 - loss: 1.033 - ETA: 4:32 - loss: 1.030 - ETA: 4:30 - loss: 1.029 - ETA: 4:29 - loss: 1.032 - ETA: 4:27 - loss: 1.033 - ETA: 4:25 - loss: 1.034 - ETA: 4:23 - loss: 1.040 - ETA: 4:22 - loss: 1.046 - ETA: 4:20 - loss: 1.048 - ETA: 4:18 - loss: 1.050 - ETA: 4:17 - loss: 1.054 - ETA: 4:15 - loss: 1.048 - ETA: 4:14 - loss: 1.051 - ETA: 4:12 - loss: 1.049 - ETA: 4:10 - loss: 1.051 - ETA: 4:09 - loss: 1.049 - ETA: 4:07 - loss: 1.045 - ETA: 4:05 - loss: 1.044 - ETA: 4:04 - loss: 1.044 - ETA: 4:02 - loss: 1.044 - ETA: 4:00 - loss: 1.045 - ETA: 3:58 - loss: 1.047 - ETA: 3:57 - loss: 1.045 - ETA: 3:55 - loss: 1.045 - ETA: 3:53 - loss: 1.047 - ETA: 3:52 - loss: 1.049 - ETA: 3:50 - loss: 1.049 - ETA: 3:49 - loss: 1.047 - ETA: 3:47 - loss: 1.046 - ETA: 3:45 - loss: 1.046 - ETA: 3:43 - loss: 1.043 - ETA: 3:42 - loss: 1.045 - ETA: 3:40 - loss: 1.045 - ETA: 3:39 - loss: 1.046 - ETA: 3:37 - loss: 1.043 - ETA: 3:35 - loss: 1.043 - ETA: 3:34 - loss: 1.042 - ETA: 3:32 - loss: 1.040 - ETA: 3:30 - loss: 1.039 - ETA: 3:29 - loss: 1.038 - ETA: 3:27 - loss: 1.038 - ETA: 3:25 - loss: 1.041 - ETA: 3:24 - loss: 1.041 - ETA: 3:22 - loss: 1.041 - ETA: 3:20 - loss: 1.040 - ETA: 3:19 - loss: 1.039 - ETA: 3:17 - loss: 1.038 - ETA: 3:15 - loss: 1.037 - ETA: 3:14 - loss: 1.036 - ETA: 3:12 - loss: 1.038 - ETA: 3:10 - loss: 1.039 - ETA: 3:09 - loss: 1.039 - ETA: 3:07 - loss: 1.038 - ETA: 3:05 - loss: 1.039 - ETA: 3:04 - loss: 1.040 - ETA: 3:02 - loss: 1.039 - ETA: 3:00 - loss: 1.040 - ETA: 2:59 - loss: 1.041 - ETA: 2:57 - loss: 1.041 - ETA: 2:55 - loss: 1.043 - ETA: 2:54 - loss: 1.045 - ETA: 2:52 - loss: 1.043 - ETA: 2:50 - loss: 1.043 - ETA: 2:49 - loss: 1.043 - ETA: 2:47 - loss: 1.044 - ETA: 2:45 - loss: 1.045 - ETA: 2:44 - loss: 1.048 - ETA: 2:42 - loss: 1.049 - ETA: 2:40 - loss: 1.048 - ETA: 2:39 - loss: 1.047 - ETA: 2:37 - loss: 1.047 - ETA: 2:36 - loss: 1.048 - ETA: 2:34 - loss: 1.047 - ETA: 2:32 - loss: 1.046 - ETA: 2:31 - loss: 1.044 - ETA: 2:29 - loss: 1.044 - ETA: 2:27 - loss: 1.045 - ETA: 2:26 - loss: 1.044 - ETA: 2:24 - loss: 1.045 - ETA: 2:22 - loss: 1.044 - ETA: 2:21 - loss: 1.044 - ETA: 2:19 - loss: 1.042 - ETA: 2:17 - loss: 1.042 - ETA: 2:16 - loss: 1.043 - ETA: 2:14 - loss: 1.043 - ETA: 2:12 - loss: 1.044 - ETA: 2:11 - loss: 1.042 - ETA: 2:09 - loss: 1.043 - ETA: 2:07 - loss: 1.044 - ETA: 2:06 - loss: 1.043 - ETA: 2:04 - loss: 1.042 - ETA: 2:02 - loss: 1.042 - ETA: 2:01 - loss: 1.042 - ETA: 1:59 - loss: 1.043 - ETA: 1:57 - loss: 1.042 - ETA: 1:56 - loss: 1.041 - ETA: 1:54 - loss: 1.041 - ETA: 1:53 - loss: 1.041 - ETA: 1:51 - loss: 1.041 - ETA: 1:49 - loss: 1.042 - ETA: 1:48 - loss: 1.042 - ETA: 1:46 - loss: 1.041 - ETA: 1:44 - loss: 1.040 - ETA: 1:43 - loss: 1.039 - ETA: 1:41 - loss: 1.040 - ETA: 1:39 - loss: 1.040 - ETA: 1:38 - loss: 1.040 - ETA: 1:36 - loss: 1.040 - ETA: 1:34 - loss: 1.040 - ETA: 1:33 - loss: 1.041 - ETA: 1:31 - loss: 1.040 - ETA: 1:29 - loss: 1.040 - ETA: 1:28 - loss: 1.039 - ETA: 1:26 - loss: 1.038 - ETA: 1:24 - loss: 1.039 - ETA: 1:23 - loss: 1.038 - ETA: 1:21 - loss: 1.038 - ETA: 1:19 - loss: 1.038 - ETA: 1:18 - loss: 1.038 - ETA: 1:16 - loss: 1.039 - ETA: 1:14 - loss: 1.038 - ETA: 1:13 - loss: 1.037 - ETA: 1:11 - loss: 1.038 - ETA: 1:09 - loss: 1.039 - ETA: 1:08 - loss: 1.039 - ETA: 1:06 - loss: 1.039 - ETA: 1:04 - loss: 1.039 - ETA: 1:03 - loss: 1.039 - ETA: 1:01 - loss: 1.039 - ETA: 59s - loss: 1.038 - ETA: 58s - loss: 1.03 - ETA: 56s - loss: 1.03 - ETA: 54s - loss: 1.03 - ETA: 53s - loss: 1.03 - ETA: 51s - loss: 1.03 - ETA: 49s - loss: 1.03 - ETA: 48s - loss: 1.03 - ETA: 46s - loss: 1.03 - ETA: 44s - loss: 1.03 - ETA: 43s - loss: 1.03 - ETA: 41s - loss: 1.03 - ETA: 39s - loss: 1.04 - ETA: 38s - loss: 1.04 - ETA: 36s - loss: 1.04 - ETA: 34s - loss: 1.03 - ETA: 33s - loss: 1.03 - ETA: 31s - loss: 1.03 - ETA: 29s - loss: 1.03 - ETA: 28s - loss: 1.03 - ETA: 26s - loss: 1.03 - ETA: 24s - loss: 1.03 - ETA: 23s - loss: 1.04 - ETA: 21s - loss: 1.03 - ETA: 19s - loss: 1.04 - ETA: 18s - loss: 1.03 - ETA: 16s - loss: 1.04 - ETA: 14s - loss: 1.04 - ETA: 13s - loss: 1.03 - ETA: 11s - loss: 1.03 - ETA: 9s - loss: 1.0389 - ETA: 8s - loss: 1.038 - ETA: 6s - loss: 1.038 - ETA: 4s - loss: 1.039 - ETA: 3s - loss: 1.038 - ETA: 1s - loss: 1.037 - 313s 2s/step - loss: 1.0373 - val_loss: 1.3150\n",
      "\n",
      "Epoch 00016: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:06 - loss: 0.884 - ETA: 5:04 - loss: 0.948 - ETA: 5:03 - loss: 0.933 - ETA: 5:01 - loss: 0.949 - ETA: 5:00 - loss: 0.954 - ETA: 4:59 - loss: 0.989 - ETA: 4:57 - loss: 0.988 - ETA: 4:55 - loss: 0.983 - ETA: 4:53 - loss: 0.998 - ETA: 4:52 - loss: 1.009 - ETA: 4:50 - loss: 1.021 - ETA: 4:49 - loss: 1.017 - ETA: 4:47 - loss: 1.008 - ETA: 4:46 - loss: 1.009 - ETA: 4:44 - loss: 1.007 - ETA: 4:43 - loss: 1.002 - ETA: 4:41 - loss: 1.005 - ETA: 4:40 - loss: 1.008 - ETA: 4:38 - loss: 1.014 - ETA: 4:37 - loss: 1.004 - ETA: 4:35 - loss: 1.004 - ETA: 4:34 - loss: 1.005 - ETA: 4:32 - loss: 1.003 - ETA: 4:31 - loss: 1.001 - ETA: 4:29 - loss: 1.006 - ETA: 4:28 - loss: 1.010 - ETA: 4:26 - loss: 1.014 - ETA: 4:24 - loss: 1.020 - ETA: 4:23 - loss: 1.022 - ETA: 4:21 - loss: 1.026 - ETA: 4:19 - loss: 1.022 - ETA: 4:18 - loss: 1.026 - ETA: 4:16 - loss: 1.022 - ETA: 4:14 - loss: 1.028 - ETA: 4:12 - loss: 1.028 - ETA: 4:11 - loss: 1.033 - ETA: 4:09 - loss: 1.028 - ETA: 4:08 - loss: 1.031 - ETA: 4:06 - loss: 1.032 - ETA: 4:04 - loss: 1.034 - ETA: 4:03 - loss: 1.034 - ETA: 4:01 - loss: 1.032 - ETA: 3:59 - loss: 1.032 - ETA: 3:58 - loss: 1.030 - ETA: 3:56 - loss: 1.035 - ETA: 3:55 - loss: 1.040 - ETA: 3:53 - loss: 1.038 - ETA: 3:51 - loss: 1.035 - ETA: 3:50 - loss: 1.035 - ETA: 3:48 - loss: 1.034 - ETA: 3:46 - loss: 1.030 - ETA: 3:45 - loss: 1.031 - ETA: 3:43 - loss: 1.031 - ETA: 3:41 - loss: 1.031 - ETA: 3:39 - loss: 1.027 - ETA: 3:38 - loss: 1.026 - ETA: 3:36 - loss: 1.024 - ETA: 3:34 - loss: 1.026 - ETA: 3:33 - loss: 1.025 - ETA: 3:31 - loss: 1.025 - ETA: 3:29 - loss: 1.026 - ETA: 3:28 - loss: 1.026 - ETA: 3:26 - loss: 1.024 - ETA: 3:24 - loss: 1.026 - ETA: 3:23 - loss: 1.025 - ETA: 3:21 - loss: 1.026 - ETA: 3:19 - loss: 1.026 - ETA: 3:18 - loss: 1.027 - ETA: 3:16 - loss: 1.028 - ETA: 3:14 - loss: 1.029 - ETA: 3:12 - loss: 1.031 - ETA: 3:11 - loss: 1.033 - ETA: 3:09 - loss: 1.034 - ETA: 3:07 - loss: 1.035 - ETA: 3:06 - loss: 1.034 - ETA: 3:04 - loss: 1.032 - ETA: 3:02 - loss: 1.030 - ETA: 3:01 - loss: 1.030 - ETA: 2:59 - loss: 1.028 - ETA: 2:57 - loss: 1.031 - ETA: 2:56 - loss: 1.030 - ETA: 2:54 - loss: 1.032 - ETA: 2:52 - loss: 1.031 - ETA: 2:51 - loss: 1.030 - ETA: 2:49 - loss: 1.030 - ETA: 2:47 - loss: 1.031 - ETA: 2:46 - loss: 1.032 - ETA: 2:44 - loss: 1.032 - ETA: 2:42 - loss: 1.030 - ETA: 2:41 - loss: 1.030 - ETA: 2:39 - loss: 1.029 - ETA: 2:37 - loss: 1.029 - ETA: 2:36 - loss: 1.030 - ETA: 2:34 - loss: 1.031 - ETA: 2:32 - loss: 1.030 - ETA: 2:31 - loss: 1.031 - ETA: 2:29 - loss: 1.031 - ETA: 2:28 - loss: 1.032 - ETA: 2:26 - loss: 1.034 - ETA: 2:24 - loss: 1.034 - ETA: 2:23 - loss: 1.034 - ETA: 2:21 - loss: 1.033 - ETA: 2:19 - loss: 1.032 - ETA: 2:18 - loss: 1.034 - ETA: 2:16 - loss: 1.035 - ETA: 2:14 - loss: 1.037 - ETA: 2:13 - loss: 1.038 - ETA: 2:11 - loss: 1.037 - ETA: 2:09 - loss: 1.035 - ETA: 2:07 - loss: 1.035 - ETA: 2:06 - loss: 1.035 - ETA: 2:04 - loss: 1.033 - ETA: 2:02 - loss: 1.035 - ETA: 2:01 - loss: 1.036 - ETA: 1:59 - loss: 1.036 - ETA: 1:57 - loss: 1.035 - ETA: 1:56 - loss: 1.035 - ETA: 1:54 - loss: 1.036 - ETA: 1:52 - loss: 1.037 - ETA: 1:51 - loss: 1.037 - ETA: 1:49 - loss: 1.036 - ETA: 1:47 - loss: 1.034 - ETA: 1:46 - loss: 1.036 - ETA: 1:44 - loss: 1.035 - ETA: 1:42 - loss: 1.036 - ETA: 1:41 - loss: 1.036 - ETA: 1:39 - loss: 1.036 - ETA: 1:37 - loss: 1.036 - ETA: 1:36 - loss: 1.037 - ETA: 1:34 - loss: 1.037 - ETA: 1:32 - loss: 1.037 - ETA: 1:31 - loss: 1.038 - ETA: 1:29 - loss: 1.038 - ETA: 1:28 - loss: 1.038 - ETA: 1:26 - loss: 1.037 - ETA: 1:24 - loss: 1.036 - ETA: 1:23 - loss: 1.037 - ETA: 1:21 - loss: 1.037 - ETA: 1:19 - loss: 1.036 - ETA: 1:18 - loss: 1.037 - ETA: 1:16 - loss: 1.037 - ETA: 1:14 - loss: 1.037 - ETA: 1:13 - loss: 1.038 - ETA: 1:11 - loss: 1.037 - ETA: 1:09 - loss: 1.037 - ETA: 1:08 - loss: 1.038 - ETA: 1:06 - loss: 1.038 - ETA: 1:04 - loss: 1.039 - ETA: 1:03 - loss: 1.038 - ETA: 1:01 - loss: 1.039 - ETA: 59s - loss: 1.039 - ETA: 58s - loss: 1.03 - ETA: 56s - loss: 1.03 - ETA: 54s - loss: 1.03 - ETA: 53s - loss: 1.03 - ETA: 51s - loss: 1.03 - ETA: 49s - loss: 1.03 - ETA: 48s - loss: 1.03 - ETA: 46s - loss: 1.03 - ETA: 44s - loss: 1.03 - ETA: 43s - loss: 1.03 - ETA: 41s - loss: 1.03 - ETA: 39s - loss: 1.03 - ETA: 38s - loss: 1.03 - ETA: 36s - loss: 1.03 - ETA: 34s - loss: 1.03 - ETA: 33s - loss: 1.03 - ETA: 31s - loss: 1.03 - ETA: 29s - loss: 1.03 - ETA: 28s - loss: 1.03 - ETA: 26s - loss: 1.03 - ETA: 24s - loss: 1.03 - ETA: 23s - loss: 1.03 - ETA: 21s - loss: 1.03 - ETA: 19s - loss: 1.03 - ETA: 18s - loss: 1.03 - ETA: 16s - loss: 1.03 - ETA: 14s - loss: 1.03 - ETA: 13s - loss: 1.03 - ETA: 11s - loss: 1.03 - ETA: 9s - loss: 1.0319 - ETA: 8s - loss: 1.032 - ETA: 6s - loss: 1.031 - ETA: 4s - loss: 1.032 - ETA: 3s - loss: 1.031 - ETA: 1s - loss: 1.031 - 313s 2s/step - loss: 1.0310 - val_loss: 1.3314\n",
      "\n",
      "Epoch 00017: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/30\n",
      "187/187 [==============================] - ETA: 5:07 - loss: 1.073 - ETA: 5:06 - loss: 1.083 - ETA: 5:03 - loss: 1.081 - ETA: 5:01 - loss: 1.080 - ETA: 5:01 - loss: 1.052 - ETA: 5:00 - loss: 1.065 - ETA: 4:59 - loss: 1.048 - ETA: 4:57 - loss: 1.036 - ETA: 4:55 - loss: 1.046 - ETA: 4:54 - loss: 1.036 - ETA: 4:52 - loss: 1.045 - ETA: 4:50 - loss: 1.054 - ETA: 4:49 - loss: 1.045 - ETA: 4:47 - loss: 1.044 - ETA: 4:45 - loss: 1.042 - ETA: 4:43 - loss: 1.031 - ETA: 4:42 - loss: 1.029 - ETA: 4:40 - loss: 1.031 - ETA: 4:38 - loss: 1.028 - ETA: 4:36 - loss: 1.030 - ETA: 4:35 - loss: 1.039 - ETA: 4:33 - loss: 1.043 - ETA: 4:31 - loss: 1.038 - ETA: 4:29 - loss: 1.037 - ETA: 4:28 - loss: 1.033 - ETA: 4:26 - loss: 1.034 - ETA: 4:24 - loss: 1.032 - ETA: 4:23 - loss: 1.028 - ETA: 4:21 - loss: 1.028 - ETA: 4:19 - loss: 1.030 - ETA: 4:18 - loss: 1.035 - ETA: 4:16 - loss: 1.036 - ETA: 4:15 - loss: 1.037 - ETA: 4:13 - loss: 1.040 - ETA: 4:11 - loss: 1.040 - ETA: 4:10 - loss: 1.034 - ETA: 4:08 - loss: 1.033 - ETA: 4:06 - loss: 1.034 - ETA: 4:05 - loss: 1.040 - ETA: 4:03 - loss: 1.039 - ETA: 4:01 - loss: 1.037 - ETA: 4:00 - loss: 1.037 - ETA: 3:58 - loss: 1.037 - ETA: 3:56 - loss: 1.033 - ETA: 3:55 - loss: 1.027 - ETA: 3:53 - loss: 1.027 - ETA: 3:51 - loss: 1.028 - ETA: 3:49 - loss: 1.030 - ETA: 3:48 - loss: 1.033 - ETA: 3:46 - loss: 1.033 - ETA: 3:44 - loss: 1.032 - ETA: 3:43 - loss: 1.032 - ETA: 3:41 - loss: 1.033 - ETA: 3:40 - loss: 1.037 - ETA: 3:38 - loss: 1.037 - ETA: 3:36 - loss: 1.038 - ETA: 3:35 - loss: 1.040 - ETA: 3:33 - loss: 1.038 - ETA: 3:31 - loss: 1.038 - ETA: 3:30 - loss: 1.036 - ETA: 3:28 - loss: 1.035 - ETA: 3:27 - loss: 1.034 - ETA: 3:25 - loss: 1.034 - ETA: 3:23 - loss: 1.035 - ETA: 3:22 - loss: 1.035 - ETA: 3:20 - loss: 1.035 - ETA: 3:18 - loss: 1.036 - ETA: 3:17 - loss: 1.038 - ETA: 3:15 - loss: 1.038 - ETA: 3:13 - loss: 1.040 - ETA: 3:12 - loss: 1.042 - ETA: 3:10 - loss: 1.040 - ETA: 3:08 - loss: 1.038 - ETA: 3:07 - loss: 1.040 - ETA: 3:05 - loss: 1.039 - ETA: 3:03 - loss: 1.038 - ETA: 3:02 - loss: 1.034 - ETA: 3:00 - loss: 1.035 - ETA: 2:58 - loss: 1.036 - ETA: 2:57 - loss: 1.037 - ETA: 2:55 - loss: 1.036 - ETA: 2:53 - loss: 1.034 - ETA: 2:52 - loss: 1.035 - ETA: 2:50 - loss: 1.034 - ETA: 2:48 - loss: 1.034 - ETA: 2:47 - loss: 1.032 - ETA: 2:45 - loss: 1.034 - ETA: 2:43 - loss: 1.035 - ETA: 2:42 - loss: 1.038 - ETA: 2:40 - loss: 1.039 - ETA: 2:38 - loss: 1.039 - ETA: 2:37 - loss: 1.037 - ETA: 2:35 - loss: 1.036 - ETA: 2:34 - loss: 1.035 - ETA: 2:32 - loss: 1.033 - ETA: 2:30 - loss: 1.033 - ETA: 2:29 - loss: 1.032 - ETA: 2:27 - loss: 1.031 - ETA: 2:25 - loss: 1.032 - ETA: 2:24 - loss: 1.031 - ETA: 2:22 - loss: 1.030 - ETA: 2:20 - loss: 1.029 - ETA: 2:19 - loss: 1.030 - ETA: 2:17 - loss: 1.030 - ETA: 2:15 - loss: 1.030 - ETA: 2:14 - loss: 1.028 - ETA: 2:12 - loss: 1.028 - ETA: 2:10 - loss: 1.028 - ETA: 2:09 - loss: 1.027 - ETA: 2:07 - loss: 1.029 - ETA: 2:05 - loss: 1.029 - ETA: 2:04 - loss: 1.028 - ETA: 2:02 - loss: 1.027 - ETA: 2:00 - loss: 1.030 - ETA: 1:59 - loss: 1.029 - ETA: 1:57 - loss: 1.029 - ETA: 1:55 - loss: 1.030 - ETA: 1:54 - loss: 1.029 - ETA: 1:52 - loss: 1.030 - ETA: 1:50 - loss: 1.029 - ETA: 1:49 - loss: 1.028 - ETA: 1:47 - loss: 1.028 - ETA: 1:45 - loss: 1.027 - ETA: 1:44 - loss: 1.027 - ETA: 1:42 - loss: 1.026 - ETA: 1:41 - loss: 1.026 - ETA: 1:39 - loss: 1.026 - ETA: 1:37 - loss: 1.027 - ETA: 1:36 - loss: 1.028 - ETA: 1:34 - loss: 1.028 - ETA: 1:32 - loss: 1.027 - ETA: 1:31 - loss: 1.027 - ETA: 1:29 - loss: 1.027 - ETA: 1:27 - loss: 1.025 - ETA: 1:26 - loss: 1.024 - ETA: 1:24 - loss: 1.024 - ETA: 1:22 - loss: 1.024 - ETA: 1:21 - loss: 1.023 - ETA: 1:19 - loss: 1.023 - ETA: 1:17 - loss: 1.023 - ETA: 1:16 - loss: 1.023 - ETA: 1:14 - loss: 1.023 - ETA: 1:12 - loss: 1.024 - ETA: 1:11 - loss: 1.024 - ETA: 1:09 - loss: 1.023 - ETA: 1:07 - loss: 1.022 - ETA: 1:06 - loss: 1.022 - ETA: 1:04 - loss: 1.022 - ETA: 1:02 - loss: 1.022 - ETA: 1:01 - loss: 1.022 - ETA: 59s - loss: 1.023 - ETA: 57s - loss: 1.02 - ETA: 56s - loss: 1.02 - ETA: 54s - loss: 1.02 - ETA: 53s - loss: 1.02 - ETA: 51s - loss: 1.02 - ETA: 49s - loss: 1.02 - ETA: 48s - loss: 1.02 - ETA: 46s - loss: 1.02 - ETA: 44s - loss: 1.02 - ETA: 43s - loss: 1.02 - ETA: 41s - loss: 1.02 - ETA: 39s - loss: 1.02 - ETA: 38s - loss: 1.02 - ETA: 36s - loss: 1.02 - ETA: 34s - loss: 1.02 - ETA: 33s - loss: 1.02 - ETA: 31s - loss: 1.02 - ETA: 29s - loss: 1.02 - ETA: 28s - loss: 1.02 - ETA: 26s - loss: 1.02 - ETA: 24s - loss: 1.02 - ETA: 23s - loss: 1.02 - ETA: 21s - loss: 1.02 - ETA: 19s - loss: 1.02 - ETA: 18s - loss: 1.02 - ETA: 16s - loss: 1.02 - ETA: 14s - loss: 1.02 - ETA: 13s - loss: 1.02 - ETA: 11s - loss: 1.02 - ETA: 9s - loss: 1.0211 - ETA: 8s - loss: 1.020 - ETA: 6s - loss: 1.020 - ETA: 4s - loss: 1.020 - ETA: 3s - loss: 1.020 - ETA: 1s - loss: 1.019 - 312s 2s/step - loss: 1.0207 - val_loss: 1.2838\n",
      "\n",
      "Epoch 00018: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:05 - loss: 0.912 - ETA: 5:05 - loss: 0.973 - ETA: 5:03 - loss: 0.991 - ETA: 5:02 - loss: 0.959 - ETA: 5:01 - loss: 0.973 - ETA: 4:59 - loss: 0.979 - ETA: 4:57 - loss: 0.951 - ETA: 4:56 - loss: 0.947 - ETA: 4:54 - loss: 0.922 - ETA: 4:53 - loss: 0.922 - ETA: 4:51 - loss: 0.936 - ETA: 4:49 - loss: 0.941 - ETA: 4:47 - loss: 0.951 - ETA: 4:46 - loss: 0.963 - ETA: 4:44 - loss: 0.981 - ETA: 4:42 - loss: 0.985 - ETA: 4:41 - loss: 0.987 - ETA: 4:39 - loss: 0.979 - ETA: 4:37 - loss: 0.981 - ETA: 4:36 - loss: 0.979 - ETA: 4:34 - loss: 0.978 - ETA: 4:32 - loss: 0.982 - ETA: 4:30 - loss: 0.984 - ETA: 4:29 - loss: 0.983 - ETA: 4:27 - loss: 0.986 - ETA: 4:25 - loss: 0.991 - ETA: 4:24 - loss: 0.993 - ETA: 4:22 - loss: 0.991 - ETA: 4:20 - loss: 0.997 - ETA: 4:19 - loss: 0.996 - ETA: 4:17 - loss: 1.008 - ETA: 4:15 - loss: 1.009 - ETA: 4:14 - loss: 1.010 - ETA: 4:12 - loss: 1.014 - ETA: 4:11 - loss: 1.014 - ETA: 4:10 - loss: 1.016 - ETA: 4:08 - loss: 1.015 - ETA: 4:07 - loss: 1.014 - ETA: 4:05 - loss: 1.012 - ETA: 4:03 - loss: 1.016 - ETA: 4:02 - loss: 1.020 - ETA: 4:00 - loss: 1.019 - ETA: 3:58 - loss: 1.016 - ETA: 3:57 - loss: 1.017 - ETA: 3:55 - loss: 1.017 - ETA: 3:53 - loss: 1.015 - ETA: 3:52 - loss: 1.014 - ETA: 3:50 - loss: 1.017 - ETA: 3:48 - loss: 1.019 - ETA: 3:47 - loss: 1.022 - ETA: 3:45 - loss: 1.022 - ETA: 3:44 - loss: 1.022 - ETA: 3:42 - loss: 1.018 - ETA: 3:40 - loss: 1.018 - ETA: 3:39 - loss: 1.017 - ETA: 3:37 - loss: 1.018 - ETA: 3:35 - loss: 1.018 - ETA: 3:34 - loss: 1.017 - ETA: 3:32 - loss: 1.017 - ETA: 3:30 - loss: 1.017 - ETA: 3:29 - loss: 1.018 - ETA: 3:27 - loss: 1.018 - ETA: 3:25 - loss: 1.018 - ETA: 3:24 - loss: 1.018 - ETA: 3:22 - loss: 1.018 - ETA: 3:20 - loss: 1.017 - ETA: 3:19 - loss: 1.015 - ETA: 3:17 - loss: 1.014 - ETA: 3:15 - loss: 1.016 - ETA: 3:14 - loss: 1.017 - ETA: 3:12 - loss: 1.017 - ETA: 3:10 - loss: 1.016 - ETA: 3:09 - loss: 1.014 - ETA: 3:07 - loss: 1.015 - ETA: 3:05 - loss: 1.015 - ETA: 3:04 - loss: 1.016 - ETA: 3:02 - loss: 1.016 - ETA: 3:00 - loss: 1.016 - ETA: 2:59 - loss: 1.017 - ETA: 2:57 - loss: 1.018 - ETA: 2:55 - loss: 1.016 - ETA: 2:54 - loss: 1.015 - ETA: 2:52 - loss: 1.016 - ETA: 2:50 - loss: 1.018 - ETA: 2:49 - loss: 1.018 - ETA: 2:47 - loss: 1.018 - ETA: 2:45 - loss: 1.016 - ETA: 2:44 - loss: 1.017 - ETA: 2:42 - loss: 1.017 - ETA: 2:40 - loss: 1.017 - ETA: 2:39 - loss: 1.019 - ETA: 2:37 - loss: 1.019 - ETA: 2:35 - loss: 1.020 - ETA: 2:34 - loss: 1.021 - ETA: 2:32 - loss: 1.022 - ETA: 2:30 - loss: 1.021 - ETA: 2:29 - loss: 1.019 - ETA: 2:27 - loss: 1.019 - ETA: 2:25 - loss: 1.020 - ETA: 2:24 - loss: 1.020 - ETA: 2:22 - loss: 1.020 - ETA: 2:21 - loss: 1.018 - ETA: 2:19 - loss: 1.017 - ETA: 2:17 - loss: 1.017 - ETA: 2:16 - loss: 1.017 - ETA: 2:14 - loss: 1.017 - ETA: 2:12 - loss: 1.017 - ETA: 2:11 - loss: 1.017 - ETA: 2:09 - loss: 1.017 - ETA: 2:07 - loss: 1.017 - ETA: 2:06 - loss: 1.017 - ETA: 2:04 - loss: 1.018 - ETA: 2:02 - loss: 1.017 - ETA: 2:01 - loss: 1.017 - ETA: 1:59 - loss: 1.017 - ETA: 1:57 - loss: 1.019 - ETA: 1:56 - loss: 1.018 - ETA: 1:54 - loss: 1.020 - ETA: 1:52 - loss: 1.020 - ETA: 1:51 - loss: 1.019 - ETA: 1:49 - loss: 1.019 - ETA: 1:47 - loss: 1.019 - ETA: 1:46 - loss: 1.021 - ETA: 1:44 - loss: 1.022 - ETA: 1:42 - loss: 1.021 - ETA: 1:41 - loss: 1.021 - ETA: 1:39 - loss: 1.021 - ETA: 1:37 - loss: 1.021 - ETA: 1:36 - loss: 1.023 - ETA: 1:34 - loss: 1.023 - ETA: 1:32 - loss: 1.022 - ETA: 1:31 - loss: 1.020 - ETA: 1:29 - loss: 1.020 - ETA: 1:27 - loss: 1.019 - ETA: 1:26 - loss: 1.021 - ETA: 1:24 - loss: 1.021 - ETA: 1:22 - loss: 1.020 - ETA: 1:21 - loss: 1.019 - ETA: 1:19 - loss: 1.019 - ETA: 1:17 - loss: 1.019 - ETA: 1:16 - loss: 1.019 - ETA: 1:14 - loss: 1.020 - ETA: 1:12 - loss: 1.019 - ETA: 1:11 - loss: 1.019 - ETA: 1:09 - loss: 1.020 - ETA: 1:07 - loss: 1.020 - ETA: 1:06 - loss: 1.020 - ETA: 1:04 - loss: 1.019 - ETA: 1:02 - loss: 1.019 - ETA: 1:01 - loss: 1.020 - ETA: 59s - loss: 1.020 - ETA: 58s - loss: 1.02 - ETA: 56s - loss: 1.02 - ETA: 54s - loss: 1.02 - ETA: 53s - loss: 1.02 - ETA: 51s - loss: 1.02 - ETA: 49s - loss: 1.02 - ETA: 48s - loss: 1.02 - ETA: 46s - loss: 1.02 - ETA: 44s - loss: 1.02 - ETA: 43s - loss: 1.02 - ETA: 41s - loss: 1.02 - ETA: 39s - loss: 1.01 - ETA: 38s - loss: 1.02 - ETA: 36s - loss: 1.02 - ETA: 34s - loss: 1.02 - ETA: 33s - loss: 1.01 - ETA: 31s - loss: 1.01 - ETA: 29s - loss: 1.01 - ETA: 28s - loss: 1.01 - ETA: 26s - loss: 1.01 - ETA: 24s - loss: 1.01 - ETA: 23s - loss: 1.01 - ETA: 21s - loss: 1.01 - ETA: 19s - loss: 1.01 - ETA: 18s - loss: 1.01 - ETA: 16s - loss: 1.01 - ETA: 14s - loss: 1.01 - ETA: 13s - loss: 1.01 - ETA: 11s - loss: 1.01 - ETA: 9s - loss: 1.0191 - ETA: 8s - loss: 1.019 - ETA: 6s - loss: 1.019 - ETA: 4s - loss: 1.018 - ETA: 3s - loss: 1.019 - ETA: 1s - loss: 1.019 - 312s 2s/step - loss: 1.0199 - val_loss: 1.3149\n",
      "\n",
      "Epoch 00019: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 20/30\n",
      "187/187 [==============================] - ETA: 5:08 - loss: 1.136 - ETA: 5:05 - loss: 1.059 - ETA: 5:03 - loss: 1.068 - ETA: 5:05 - loss: 1.107 - ETA: 5:05 - loss: 1.081 - ETA: 5:03 - loss: 1.073 - ETA: 5:00 - loss: 1.040 - ETA: 4:58 - loss: 1.054 - ETA: 4:56 - loss: 1.054 - ETA: 4:54 - loss: 1.043 - ETA: 4:52 - loss: 1.048 - ETA: 4:51 - loss: 1.047 - ETA: 4:49 - loss: 1.049 - ETA: 4:48 - loss: 1.034 - ETA: 4:46 - loss: 1.028 - ETA: 4:45 - loss: 1.022 - ETA: 4:43 - loss: 1.017 - ETA: 4:42 - loss: 1.022 - ETA: 4:40 - loss: 1.020 - ETA: 4:38 - loss: 1.016 - ETA: 4:37 - loss: 1.013 - ETA: 4:35 - loss: 1.015 - ETA: 4:33 - loss: 1.014 - ETA: 4:31 - loss: 1.017 - ETA: 4:30 - loss: 1.014 - ETA: 4:28 - loss: 1.012 - ETA: 4:26 - loss: 1.009 - ETA: 4:25 - loss: 1.007 - ETA: 4:23 - loss: 1.008 - ETA: 4:21 - loss: 1.014 - ETA: 4:20 - loss: 1.012 - ETA: 4:18 - loss: 1.011 - ETA: 4:16 - loss: 1.016 - ETA: 4:14 - loss: 1.015 - ETA: 4:13 - loss: 1.014 - ETA: 4:11 - loss: 1.014 - ETA: 4:09 - loss: 1.015 - ETA: 4:08 - loss: 1.014 - ETA: 4:06 - loss: 1.009 - ETA: 4:04 - loss: 1.013 - ETA: 4:03 - loss: 1.011 - ETA: 4:01 - loss: 1.010 - ETA: 3:59 - loss: 1.008 - ETA: 3:58 - loss: 1.007 - ETA: 3:56 - loss: 1.006 - ETA: 3:54 - loss: 1.006 - ETA: 3:53 - loss: 1.004 - ETA: 3:51 - loss: 1.004 - ETA: 3:49 - loss: 1.004 - ETA: 3:48 - loss: 1.004 - ETA: 3:46 - loss: 1.003 - ETA: 3:44 - loss: 1.000 - ETA: 3:43 - loss: 1.003 - ETA: 3:41 - loss: 1.005 - ETA: 3:39 - loss: 1.005 - ETA: 3:38 - loss: 1.004 - ETA: 3:36 - loss: 1.003 - ETA: 3:35 - loss: 1.003 - ETA: 3:33 - loss: 1.002 - ETA: 3:31 - loss: 1.003 - ETA: 3:29 - loss: 1.003 - ETA: 3:28 - loss: 1.003 - ETA: 3:26 - loss: 0.999 - ETA: 3:24 - loss: 1.002 - ETA: 3:23 - loss: 1.002 - ETA: 3:21 - loss: 0.999 - ETA: 3:19 - loss: 0.996 - ETA: 3:18 - loss: 0.997 - ETA: 3:16 - loss: 0.996 - ETA: 3:15 - loss: 0.996 - ETA: 3:13 - loss: 0.999 - ETA: 3:11 - loss: 0.997 - ETA: 3:10 - loss: 0.996 - ETA: 3:08 - loss: 0.995 - ETA: 3:06 - loss: 0.996 - ETA: 3:05 - loss: 0.995 - ETA: 3:03 - loss: 0.993 - ETA: 3:01 - loss: 0.993 - ETA: 3:00 - loss: 0.995 - ETA: 2:58 - loss: 0.994 - ETA: 2:56 - loss: 0.992 - ETA: 2:55 - loss: 0.991 - ETA: 2:53 - loss: 0.992 - ETA: 2:51 - loss: 0.990 - ETA: 2:50 - loss: 0.991 - ETA: 2:48 - loss: 0.991 - ETA: 2:46 - loss: 0.992 - ETA: 2:45 - loss: 0.992 - ETA: 2:43 - loss: 0.992 - ETA: 2:41 - loss: 0.993 - ETA: 2:40 - loss: 0.992 - ETA: 2:38 - loss: 0.992 - ETA: 2:36 - loss: 0.994 - ETA: 2:35 - loss: 0.994 - ETA: 2:33 - loss: 0.994 - ETA: 2:31 - loss: 0.997 - ETA: 2:30 - loss: 0.997 - ETA: 2:28 - loss: 0.996 - ETA: 2:26 - loss: 0.996 - ETA: 2:24 - loss: 0.997 - ETA: 2:23 - loss: 0.995 - ETA: 2:21 - loss: 0.995 - ETA: 2:19 - loss: 0.996 - ETA: 2:18 - loss: 0.996 - ETA: 2:16 - loss: 0.999 - ETA: 2:14 - loss: 0.999 - ETA: 2:13 - loss: 0.998 - ETA: 2:11 - loss: 0.999 - ETA: 2:09 - loss: 0.998 - ETA: 2:08 - loss: 0.999 - ETA: 2:06 - loss: 0.999 - ETA: 2:04 - loss: 0.999 - ETA: 2:03 - loss: 0.998 - ETA: 2:01 - loss: 0.998 - ETA: 1:59 - loss: 0.998 - ETA: 1:58 - loss: 0.998 - ETA: 1:56 - loss: 0.999 - ETA: 1:54 - loss: 0.999 - ETA: 1:53 - loss: 0.998 - ETA: 1:51 - loss: 0.998 - ETA: 1:49 - loss: 0.998 - ETA: 1:48 - loss: 0.997 - ETA: 1:46 - loss: 0.997 - ETA: 1:44 - loss: 0.996 - ETA: 1:43 - loss: 0.996 - ETA: 1:41 - loss: 0.995 - ETA: 1:39 - loss: 0.996 - ETA: 1:38 - loss: 0.996 - ETA: 1:36 - loss: 0.996 - ETA: 1:34 - loss: 0.996 - ETA: 1:33 - loss: 0.996 - ETA: 1:31 - loss: 0.995 - ETA: 1:29 - loss: 0.995 - ETA: 1:28 - loss: 0.994 - ETA: 1:26 - loss: 0.994 - ETA: 1:24 - loss: 0.995 - ETA: 1:23 - loss: 0.995 - ETA: 1:21 - loss: 0.996 - ETA: 1:19 - loss: 0.996 - ETA: 1:18 - loss: 0.995 - ETA: 1:16 - loss: 0.996 - ETA: 1:14 - loss: 0.995 - ETA: 1:13 - loss: 0.995 - ETA: 1:11 - loss: 0.996 - ETA: 1:09 - loss: 0.997 - ETA: 1:08 - loss: 0.996 - ETA: 1:06 - loss: 0.994 - ETA: 1:04 - loss: 0.994 - ETA: 1:03 - loss: 0.994 - ETA: 1:01 - loss: 0.994 - ETA: 59s - loss: 0.995 - ETA: 58s - loss: 0.99 - ETA: 56s - loss: 0.99 - ETA: 54s - loss: 0.99 - ETA: 53s - loss: 0.99 - ETA: 51s - loss: 0.99 - ETA: 49s - loss: 0.99 - ETA: 48s - loss: 0.99 - ETA: 46s - loss: 0.99 - ETA: 44s - loss: 0.99 - ETA: 43s - loss: 0.99 - ETA: 41s - loss: 0.99 - ETA: 39s - loss: 0.99 - ETA: 38s - loss: 0.99 - ETA: 36s - loss: 0.99 - ETA: 34s - loss: 0.99 - ETA: 33s - loss: 0.99 - ETA: 31s - loss: 0.99 - ETA: 29s - loss: 0.99 - ETA: 28s - loss: 0.99 - ETA: 26s - loss: 0.99 - ETA: 24s - loss: 0.99 - ETA: 23s - loss: 0.99 - ETA: 21s - loss: 0.99 - ETA: 19s - loss: 0.99 - ETA: 18s - loss: 0.99 - ETA: 16s - loss: 0.99 - ETA: 14s - loss: 0.99 - ETA: 13s - loss: 0.99 - ETA: 11s - loss: 0.99 - ETA: 9s - loss: 0.9997 - ETA: 8s - loss: 1.000 - ETA: 6s - loss: 0.999 - ETA: 4s - loss: 0.998 - ETA: 3s - loss: 0.998 - ETA: 1s - loss: 0.997 - 313s 2s/step - loss: 0.9979 - val_loss: 1.2682\n",
      "\n",
      "Epoch 00020: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:08 - loss: 0.988 - ETA: 5:06 - loss: 1.068 - ETA: 5:04 - loss: 1.000 - ETA: 5:02 - loss: 0.992 - ETA: 5:00 - loss: 0.996 - ETA: 4:58 - loss: 0.985 - ETA: 4:56 - loss: 0.989 - ETA: 4:54 - loss: 0.984 - ETA: 4:53 - loss: 0.995 - ETA: 4:51 - loss: 0.976 - ETA: 4:50 - loss: 0.976 - ETA: 4:48 - loss: 0.993 - ETA: 4:47 - loss: 0.995 - ETA: 4:45 - loss: 1.001 - ETA: 4:44 - loss: 1.000 - ETA: 4:42 - loss: 0.996 - ETA: 4:40 - loss: 0.987 - ETA: 4:39 - loss: 0.993 - ETA: 4:37 - loss: 1.000 - ETA: 4:35 - loss: 1.001 - ETA: 4:34 - loss: 1.002 - ETA: 4:32 - loss: 1.005 - ETA: 4:31 - loss: 1.007 - ETA: 4:29 - loss: 1.003 - ETA: 4:27 - loss: 1.005 - ETA: 4:26 - loss: 1.006 - ETA: 4:24 - loss: 1.011 - ETA: 4:22 - loss: 1.012 - ETA: 4:21 - loss: 1.013 - ETA: 4:19 - loss: 1.012 - ETA: 4:17 - loss: 1.011 - ETA: 4:16 - loss: 1.009 - ETA: 4:14 - loss: 1.010 - ETA: 4:12 - loss: 1.007 - ETA: 4:11 - loss: 1.008 - ETA: 4:09 - loss: 1.007 - ETA: 4:07 - loss: 1.002 - ETA: 4:06 - loss: 1.003 - ETA: 4:04 - loss: 1.005 - ETA: 4:03 - loss: 1.004 - ETA: 4:01 - loss: 1.002 - ETA: 3:59 - loss: 1.001 - ETA: 3:58 - loss: 1.000 - ETA: 3:56 - loss: 1.000 - ETA: 3:54 - loss: 1.002 - ETA: 3:53 - loss: 1.002 - ETA: 3:51 - loss: 1.004 - ETA: 3:49 - loss: 1.005 - ETA: 3:48 - loss: 1.007 - ETA: 3:46 - loss: 1.007 - ETA: 3:44 - loss: 1.008 - ETA: 3:43 - loss: 1.006 - ETA: 3:41 - loss: 1.007 - ETA: 3:40 - loss: 1.010 - ETA: 3:38 - loss: 1.009 - ETA: 3:36 - loss: 1.010 - ETA: 3:35 - loss: 1.008 - ETA: 3:33 - loss: 1.010 - ETA: 3:31 - loss: 1.012 - ETA: 3:30 - loss: 1.013 - ETA: 3:28 - loss: 1.014 - ETA: 3:26 - loss: 1.014 - ETA: 3:25 - loss: 1.013 - ETA: 3:23 - loss: 1.013 - ETA: 3:21 - loss: 1.013 - ETA: 3:20 - loss: 1.013 - ETA: 3:18 - loss: 1.012 - ETA: 3:16 - loss: 1.014 - ETA: 3:15 - loss: 1.013 - ETA: 3:13 - loss: 1.015 - ETA: 3:11 - loss: 1.015 - ETA: 3:10 - loss: 1.018 - ETA: 3:08 - loss: 1.018 - ETA: 3:06 - loss: 1.018 - ETA: 3:05 - loss: 1.019 - ETA: 3:03 - loss: 1.018 - ETA: 3:01 - loss: 1.018 - ETA: 3:00 - loss: 1.017 - ETA: 2:58 - loss: 1.019 - ETA: 2:56 - loss: 1.021 - ETA: 2:55 - loss: 1.021 - ETA: 2:53 - loss: 1.021 - ETA: 2:52 - loss: 1.021 - ETA: 2:50 - loss: 1.020 - ETA: 2:48 - loss: 1.018 - ETA: 2:47 - loss: 1.018 - ETA: 2:45 - loss: 1.017 - ETA: 2:43 - loss: 1.018 - ETA: 2:42 - loss: 1.017 - ETA: 2:40 - loss: 1.017 - ETA: 2:38 - loss: 1.017 - ETA: 2:37 - loss: 1.016 - ETA: 2:35 - loss: 1.015 - ETA: 2:33 - loss: 1.015 - ETA: 2:32 - loss: 1.014 - ETA: 2:30 - loss: 1.014 - ETA: 2:28 - loss: 1.013 - ETA: 2:27 - loss: 1.013 - ETA: 2:25 - loss: 1.012 - ETA: 2:23 - loss: 1.011 - ETA: 2:22 - loss: 1.011 - ETA: 2:20 - loss: 1.011 - ETA: 2:19 - loss: 1.011 - ETA: 2:17 - loss: 1.011 - ETA: 2:15 - loss: 1.012 - ETA: 2:14 - loss: 1.013 - ETA: 2:12 - loss: 1.014 - ETA: 2:10 - loss: 1.016 - ETA: 2:09 - loss: 1.016 - ETA: 2:07 - loss: 1.015 - ETA: 2:06 - loss: 1.015 - ETA: 2:04 - loss: 1.014 - ETA: 2:02 - loss: 1.013 - ETA: 2:01 - loss: 1.013 - ETA: 1:59 - loss: 1.014 - ETA: 1:57 - loss: 1.013 - ETA: 1:56 - loss: 1.013 - ETA: 1:54 - loss: 1.013 - ETA: 1:52 - loss: 1.012 - ETA: 1:51 - loss: 1.011 - ETA: 1:49 - loss: 1.011 - ETA: 1:47 - loss: 1.010 - ETA: 1:46 - loss: 1.008 - ETA: 1:44 - loss: 1.008 - ETA: 1:42 - loss: 1.008 - ETA: 1:41 - loss: 1.009 - ETA: 1:39 - loss: 1.008 - ETA: 1:37 - loss: 1.007 - ETA: 1:36 - loss: 1.007 - ETA: 1:34 - loss: 1.006 - ETA: 1:32 - loss: 1.005 - ETA: 1:31 - loss: 1.005 - ETA: 1:29 - loss: 1.005 - ETA: 1:27 - loss: 1.004 - ETA: 1:26 - loss: 1.004 - ETA: 1:24 - loss: 1.003 - ETA: 1:22 - loss: 1.003 - ETA: 1:21 - loss: 1.004 - ETA: 1:19 - loss: 1.004 - ETA: 1:17 - loss: 1.005 - ETA: 1:16 - loss: 1.006 - ETA: 1:14 - loss: 1.005 - ETA: 1:12 - loss: 1.004 - ETA: 1:11 - loss: 1.003 - ETA: 1:09 - loss: 1.003 - ETA: 1:07 - loss: 1.003 - ETA: 1:06 - loss: 1.005 - ETA: 1:04 - loss: 1.005 - ETA: 1:02 - loss: 1.005 - ETA: 1:01 - loss: 1.004 - ETA: 59s - loss: 1.004 - ETA: 57s - loss: 1.00 - ETA: 56s - loss: 1.00 - ETA: 54s - loss: 1.00 - ETA: 53s - loss: 1.00 - ETA: 51s - loss: 1.00 - ETA: 49s - loss: 1.00 - ETA: 48s - loss: 1.00 - ETA: 46s - loss: 1.00 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 1.00 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 1.00 - ETA: 29s - loss: 1.00 - ETA: 28s - loss: 1.00 - ETA: 26s - loss: 1.00 - ETA: 24s - loss: 1.00 - ETA: 23s - loss: 1.00 - ETA: 21s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 9s - loss: 1.0052 - ETA: 8s - loss: 1.005 - ETA: 6s - loss: 1.004 - ETA: 4s - loss: 1.004 - ETA: 3s - loss: 1.005 - ETA: 1s - loss: 1.004 - 312s 2s/step - loss: 1.0046 - val_loss: 1.3039\n",
      "\n",
      "Epoch 00021: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 22/30\n",
      "187/187 [==============================] - ETA: 5:08 - loss: 1.227 - ETA: 5:06 - loss: 1.134 - ETA: 5:03 - loss: 1.103 - ETA: 5:01 - loss: 1.050 - ETA: 4:59 - loss: 1.062 - ETA: 4:58 - loss: 1.029 - ETA: 4:57 - loss: 1.016 - ETA: 4:55 - loss: 1.028 - ETA: 4:53 - loss: 1.035 - ETA: 4:51 - loss: 1.031 - ETA: 4:50 - loss: 1.022 - ETA: 4:48 - loss: 1.034 - ETA: 4:47 - loss: 1.018 - ETA: 4:45 - loss: 1.015 - ETA: 4:43 - loss: 1.009 - ETA: 4:41 - loss: 1.009 - ETA: 4:40 - loss: 1.008 - ETA: 4:38 - loss: 1.011 - ETA: 4:36 - loss: 1.010 - ETA: 4:35 - loss: 1.020 - ETA: 4:33 - loss: 1.020 - ETA: 4:32 - loss: 1.024 - ETA: 4:30 - loss: 1.022 - ETA: 4:28 - loss: 1.020 - ETA: 4:27 - loss: 1.016 - ETA: 4:25 - loss: 1.008 - ETA: 4:23 - loss: 1.013 - ETA: 4:22 - loss: 1.010 - ETA: 4:20 - loss: 1.015 - ETA: 4:19 - loss: 1.014 - ETA: 4:17 - loss: 1.015 - ETA: 4:15 - loss: 1.017 - ETA: 4:14 - loss: 1.019 - ETA: 4:12 - loss: 1.019 - ETA: 4:11 - loss: 1.021 - ETA: 4:09 - loss: 1.023 - ETA: 4:07 - loss: 1.024 - ETA: 4:06 - loss: 1.031 - ETA: 4:04 - loss: 1.032 - ETA: 4:02 - loss: 1.029 - ETA: 4:01 - loss: 1.030 - ETA: 3:59 - loss: 1.031 - ETA: 3:57 - loss: 1.030 - ETA: 3:56 - loss: 1.033 - ETA: 3:54 - loss: 1.032 - ETA: 3:52 - loss: 1.031 - ETA: 3:51 - loss: 1.024 - ETA: 3:49 - loss: 1.025 - ETA: 3:48 - loss: 1.024 - ETA: 3:46 - loss: 1.025 - ETA: 3:45 - loss: 1.025 - ETA: 3:43 - loss: 1.026 - ETA: 3:41 - loss: 1.023 - ETA: 3:40 - loss: 1.025 - ETA: 3:38 - loss: 1.026 - ETA: 3:36 - loss: 1.023 - ETA: 3:35 - loss: 1.019 - ETA: 3:33 - loss: 1.017 - ETA: 3:31 - loss: 1.019 - ETA: 3:30 - loss: 1.018 - ETA: 3:28 - loss: 1.022 - ETA: 3:26 - loss: 1.022 - ETA: 3:25 - loss: 1.021 - ETA: 3:23 - loss: 1.023 - ETA: 3:21 - loss: 1.021 - ETA: 3:20 - loss: 1.021 - ETA: 3:18 - loss: 1.020 - ETA: 3:16 - loss: 1.024 - ETA: 3:15 - loss: 1.023 - ETA: 3:13 - loss: 1.022 - ETA: 3:12 - loss: 1.023 - ETA: 3:10 - loss: 1.023 - ETA: 3:08 - loss: 1.021 - ETA: 3:07 - loss: 1.019 - ETA: 3:05 - loss: 1.020 - ETA: 3:03 - loss: 1.021 - ETA: 3:02 - loss: 1.017 - ETA: 3:00 - loss: 1.015 - ETA: 2:58 - loss: 1.016 - ETA: 2:57 - loss: 1.016 - ETA: 2:55 - loss: 1.016 - ETA: 2:53 - loss: 1.016 - ETA: 2:52 - loss: 1.016 - ETA: 2:50 - loss: 1.016 - ETA: 2:48 - loss: 1.015 - ETA: 2:47 - loss: 1.014 - ETA: 2:45 - loss: 1.016 - ETA: 2:43 - loss: 1.015 - ETA: 2:42 - loss: 1.014 - ETA: 2:40 - loss: 1.014 - ETA: 2:38 - loss: 1.015 - ETA: 2:37 - loss: 1.016 - ETA: 2:35 - loss: 1.016 - ETA: 2:33 - loss: 1.016 - ETA: 2:32 - loss: 1.016 - ETA: 2:30 - loss: 1.016 - ETA: 2:29 - loss: 1.016 - ETA: 2:27 - loss: 1.016 - ETA: 2:25 - loss: 1.015 - ETA: 2:24 - loss: 1.016 - ETA: 2:22 - loss: 1.017 - ETA: 2:20 - loss: 1.015 - ETA: 2:19 - loss: 1.015 - ETA: 2:17 - loss: 1.016 - ETA: 2:16 - loss: 1.016 - ETA: 2:14 - loss: 1.016 - ETA: 2:12 - loss: 1.016 - ETA: 2:11 - loss: 1.015 - ETA: 2:09 - loss: 1.014 - ETA: 2:07 - loss: 1.014 - ETA: 2:06 - loss: 1.014 - ETA: 2:04 - loss: 1.013 - ETA: 2:02 - loss: 1.013 - ETA: 2:01 - loss: 1.013 - ETA: 1:59 - loss: 1.011 - ETA: 1:57 - loss: 1.012 - ETA: 1:56 - loss: 1.012 - ETA: 1:54 - loss: 1.013 - ETA: 1:52 - loss: 1.013 - ETA: 1:51 - loss: 1.012 - ETA: 1:49 - loss: 1.012 - ETA: 1:47 - loss: 1.011 - ETA: 1:46 - loss: 1.011 - ETA: 1:44 - loss: 1.011 - ETA: 1:42 - loss: 1.010 - ETA: 1:41 - loss: 1.010 - ETA: 1:39 - loss: 1.010 - ETA: 1:37 - loss: 1.010 - ETA: 1:36 - loss: 1.008 - ETA: 1:34 - loss: 1.008 - ETA: 1:32 - loss: 1.008 - ETA: 1:31 - loss: 1.008 - ETA: 1:29 - loss: 1.007 - ETA: 1:27 - loss: 1.006 - ETA: 1:26 - loss: 1.006 - ETA: 1:24 - loss: 1.007 - ETA: 1:22 - loss: 1.006 - ETA: 1:21 - loss: 1.006 - ETA: 1:19 - loss: 1.007 - ETA: 1:17 - loss: 1.006 - ETA: 1:16 - loss: 1.006 - ETA: 1:14 - loss: 1.005 - ETA: 1:12 - loss: 1.005 - ETA: 1:11 - loss: 1.006 - ETA: 1:09 - loss: 1.006 - ETA: 1:07 - loss: 1.005 - ETA: 1:06 - loss: 1.006 - ETA: 1:04 - loss: 1.007 - ETA: 1:03 - loss: 1.007 - ETA: 1:01 - loss: 1.007 - ETA: 59s - loss: 1.007 - ETA: 58s - loss: 1.00 - ETA: 56s - loss: 1.00 - ETA: 54s - loss: 1.00 - ETA: 53s - loss: 1.00 - ETA: 51s - loss: 1.00 - ETA: 49s - loss: 1.00 - ETA: 48s - loss: 1.00 - ETA: 46s - loss: 1.00 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 1.00 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 1.00 - ETA: 29s - loss: 1.00 - ETA: 28s - loss: 1.00 - ETA: 26s - loss: 1.00 - ETA: 24s - loss: 1.00 - ETA: 23s - loss: 1.00 - ETA: 21s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 9s - loss: 1.0088 - ETA: 8s - loss: 1.008 - ETA: 6s - loss: 1.008 - ETA: 4s - loss: 1.007 - ETA: 3s - loss: 1.007 - ETA: 1s - loss: 1.008 - 312s 2s/step - loss: 1.0085 - val_loss: 1.2786\n",
      "\n",
      "Epoch 00022: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:05 - loss: 1.002 - ETA: 5:05 - loss: 1.012 - ETA: 5:05 - loss: 1.039 - ETA: 5:03 - loss: 1.059 - ETA: 5:01 - loss: 1.027 - ETA: 4:59 - loss: 1.058 - ETA: 4:58 - loss: 1.031 - ETA: 4:57 - loss: 1.040 - ETA: 4:56 - loss: 1.047 - ETA: 4:55 - loss: 1.024 - ETA: 4:53 - loss: 1.015 - ETA: 4:51 - loss: 1.018 - ETA: 4:49 - loss: 1.018 - ETA: 4:47 - loss: 1.015 - ETA: 4:46 - loss: 1.011 - ETA: 4:44 - loss: 1.012 - ETA: 4:42 - loss: 1.012 - ETA: 4:40 - loss: 1.004 - ETA: 4:39 - loss: 1.004 - ETA: 4:37 - loss: 1.001 - ETA: 4:36 - loss: 1.005 - ETA: 4:34 - loss: 0.999 - ETA: 4:32 - loss: 0.996 - ETA: 4:30 - loss: 0.993 - ETA: 4:29 - loss: 0.994 - ETA: 4:27 - loss: 0.998 - ETA: 4:25 - loss: 0.997 - ETA: 4:24 - loss: 0.997 - ETA: 4:22 - loss: 0.997 - ETA: 4:20 - loss: 1.001 - ETA: 4:19 - loss: 1.005 - ETA: 4:17 - loss: 1.003 - ETA: 4:15 - loss: 1.004 - ETA: 4:14 - loss: 1.007 - ETA: 4:12 - loss: 1.010 - ETA: 4:11 - loss: 1.009 - ETA: 4:09 - loss: 1.015 - ETA: 4:07 - loss: 1.017 - ETA: 4:06 - loss: 1.018 - ETA: 4:04 - loss: 1.017 - ETA: 4:02 - loss: 1.016 - ETA: 4:01 - loss: 1.018 - ETA: 3:59 - loss: 1.022 - ETA: 3:57 - loss: 1.019 - ETA: 3:56 - loss: 1.021 - ETA: 3:54 - loss: 1.021 - ETA: 3:52 - loss: 1.019 - ETA: 3:50 - loss: 1.021 - ETA: 3:49 - loss: 1.018 - ETA: 3:47 - loss: 1.020 - ETA: 3:45 - loss: 1.020 - ETA: 3:44 - loss: 1.022 - ETA: 3:42 - loss: 1.024 - ETA: 3:40 - loss: 1.023 - ETA: 3:39 - loss: 1.021 - ETA: 3:37 - loss: 1.019 - ETA: 3:35 - loss: 1.017 - ETA: 3:34 - loss: 1.015 - ETA: 3:32 - loss: 1.016 - ETA: 3:30 - loss: 1.014 - ETA: 3:29 - loss: 1.014 - ETA: 3:27 - loss: 1.015 - ETA: 3:25 - loss: 1.016 - ETA: 3:24 - loss: 1.016 - ETA: 3:22 - loss: 1.016 - ETA: 3:20 - loss: 1.015 - ETA: 3:19 - loss: 1.017 - ETA: 3:17 - loss: 1.019 - ETA: 3:15 - loss: 1.021 - ETA: 3:14 - loss: 1.019 - ETA: 3:12 - loss: 1.019 - ETA: 3:10 - loss: 1.021 - ETA: 3:09 - loss: 1.021 - ETA: 3:07 - loss: 1.021 - ETA: 3:05 - loss: 1.017 - ETA: 3:04 - loss: 1.019 - ETA: 3:02 - loss: 1.018 - ETA: 3:00 - loss: 1.016 - ETA: 2:59 - loss: 1.017 - ETA: 2:57 - loss: 1.015 - ETA: 2:55 - loss: 1.014 - ETA: 2:54 - loss: 1.015 - ETA: 2:52 - loss: 1.013 - ETA: 2:50 - loss: 1.015 - ETA: 2:49 - loss: 1.014 - ETA: 2:47 - loss: 1.012 - ETA: 2:45 - loss: 1.012 - ETA: 2:44 - loss: 1.011 - ETA: 2:42 - loss: 1.011 - ETA: 2:40 - loss: 1.012 - ETA: 2:39 - loss: 1.013 - ETA: 2:37 - loss: 1.012 - ETA: 2:35 - loss: 1.013 - ETA: 2:34 - loss: 1.012 - ETA: 2:32 - loss: 1.012 - ETA: 2:30 - loss: 1.012 - ETA: 2:29 - loss: 1.014 - ETA: 2:27 - loss: 1.014 - ETA: 2:25 - loss: 1.014 - ETA: 2:24 - loss: 1.013 - ETA: 2:22 - loss: 1.014 - ETA: 2:20 - loss: 1.014 - ETA: 2:19 - loss: 1.013 - ETA: 2:17 - loss: 1.012 - ETA: 2:15 - loss: 1.011 - ETA: 2:14 - loss: 1.010 - ETA: 2:12 - loss: 1.010 - ETA: 2:10 - loss: 1.011 - ETA: 2:09 - loss: 1.011 - ETA: 2:07 - loss: 1.011 - ETA: 2:05 - loss: 1.012 - ETA: 2:04 - loss: 1.011 - ETA: 2:02 - loss: 1.011 - ETA: 2:00 - loss: 1.011 - ETA: 1:59 - loss: 1.012 - ETA: 1:57 - loss: 1.012 - ETA: 1:55 - loss: 1.012 - ETA: 1:54 - loss: 1.012 - ETA: 1:52 - loss: 1.012 - ETA: 1:51 - loss: 1.011 - ETA: 1:49 - loss: 1.011 - ETA: 1:47 - loss: 1.010 - ETA: 1:46 - loss: 1.010 - ETA: 1:44 - loss: 1.011 - ETA: 1:42 - loss: 1.010 - ETA: 1:41 - loss: 1.009 - ETA: 1:39 - loss: 1.008 - ETA: 1:37 - loss: 1.007 - ETA: 1:36 - loss: 1.007 - ETA: 1:34 - loss: 1.006 - ETA: 1:32 - loss: 1.008 - ETA: 1:31 - loss: 1.007 - ETA: 1:29 - loss: 1.007 - ETA: 1:27 - loss: 1.007 - ETA: 1:26 - loss: 1.008 - ETA: 1:24 - loss: 1.007 - ETA: 1:22 - loss: 1.007 - ETA: 1:21 - loss: 1.006 - ETA: 1:19 - loss: 1.007 - ETA: 1:17 - loss: 1.007 - ETA: 1:16 - loss: 1.007 - ETA: 1:14 - loss: 1.007 - ETA: 1:12 - loss: 1.008 - ETA: 1:11 - loss: 1.007 - ETA: 1:09 - loss: 1.008 - ETA: 1:07 - loss: 1.008 - ETA: 1:06 - loss: 1.008 - ETA: 1:04 - loss: 1.007 - ETA: 1:02 - loss: 1.008 - ETA: 1:01 - loss: 1.007 - ETA: 59s - loss: 1.008 - ETA: 57s - loss: 1.00 - ETA: 56s - loss: 1.00 - ETA: 54s - loss: 1.00 - ETA: 52s - loss: 1.00 - ETA: 51s - loss: 1.00 - ETA: 49s - loss: 1.00 - ETA: 48s - loss: 1.00 - ETA: 46s - loss: 1.00 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 1.00 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 1.00 - ETA: 29s - loss: 1.00 - ETA: 28s - loss: 1.00 - ETA: 26s - loss: 1.00 - ETA: 24s - loss: 1.00 - ETA: 23s - loss: 1.00 - ETA: 21s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 9s - loss: 1.0039 - ETA: 8s - loss: 1.003 - ETA: 6s - loss: 1.004 - ETA: 4s - loss: 1.005 - ETA: 3s - loss: 1.005 - ETA: 1s - loss: 1.005 - 312s 2s/step - loss: 1.0052 - val_loss: 1.2469\n",
      "\n",
      "Epoch 00023: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 24/30\n",
      "187/187 [==============================] - ETA: 5:06 - loss: 0.819 - ETA: 5:04 - loss: 0.926 - ETA: 5:04 - loss: 0.994 - ETA: 5:04 - loss: 0.976 - ETA: 5:03 - loss: 0.987 - ETA: 5:01 - loss: 0.991 - ETA: 5:00 - loss: 1.011 - ETA: 4:59 - loss: 1.011 - ETA: 4:57 - loss: 1.014 - ETA: 4:55 - loss: 1.001 - ETA: 4:53 - loss: 1.002 - ETA: 4:52 - loss: 1.006 - ETA: 4:50 - loss: 1.002 - ETA: 4:48 - loss: 1.021 - ETA: 4:46 - loss: 1.019 - ETA: 4:44 - loss: 1.022 - ETA: 4:43 - loss: 1.025 - ETA: 4:41 - loss: 1.017 - ETA: 4:39 - loss: 1.020 - ETA: 4:38 - loss: 1.015 - ETA: 4:37 - loss: 1.012 - ETA: 4:35 - loss: 1.021 - ETA: 4:34 - loss: 1.023 - ETA: 4:32 - loss: 1.014 - ETA: 4:30 - loss: 1.005 - ETA: 4:28 - loss: 1.002 - ETA: 4:26 - loss: 1.004 - ETA: 4:24 - loss: 1.012 - ETA: 4:23 - loss: 1.016 - ETA: 4:21 - loss: 1.014 - ETA: 4:19 - loss: 1.011 - ETA: 4:18 - loss: 1.009 - ETA: 4:16 - loss: 1.010 - ETA: 4:14 - loss: 1.012 - ETA: 4:13 - loss: 1.018 - ETA: 4:11 - loss: 1.016 - ETA: 4:09 - loss: 1.014 - ETA: 4:08 - loss: 1.020 - ETA: 4:06 - loss: 1.018 - ETA: 4:04 - loss: 1.016 - ETA: 4:03 - loss: 1.020 - ETA: 4:01 - loss: 1.018 - ETA: 3:59 - loss: 1.019 - ETA: 3:58 - loss: 1.021 - ETA: 3:56 - loss: 1.021 - ETA: 3:54 - loss: 1.018 - ETA: 3:52 - loss: 1.016 - ETA: 3:51 - loss: 1.017 - ETA: 3:49 - loss: 1.017 - ETA: 3:47 - loss: 1.017 - ETA: 3:46 - loss: 1.020 - ETA: 3:44 - loss: 1.015 - ETA: 3:42 - loss: 1.016 - ETA: 3:41 - loss: 1.018 - ETA: 3:39 - loss: 1.017 - ETA: 3:37 - loss: 1.019 - ETA: 3:36 - loss: 1.017 - ETA: 3:34 - loss: 1.014 - ETA: 3:32 - loss: 1.013 - ETA: 3:30 - loss: 1.012 - ETA: 3:29 - loss: 1.014 - ETA: 3:27 - loss: 1.018 - ETA: 3:26 - loss: 1.018 - ETA: 3:24 - loss: 1.017 - ETA: 3:22 - loss: 1.018 - ETA: 3:21 - loss: 1.017 - ETA: 3:19 - loss: 1.017 - ETA: 3:17 - loss: 1.018 - ETA: 3:16 - loss: 1.017 - ETA: 3:14 - loss: 1.017 - ETA: 3:12 - loss: 1.017 - ETA: 3:11 - loss: 1.017 - ETA: 3:09 - loss: 1.020 - ETA: 3:07 - loss: 1.017 - ETA: 3:06 - loss: 1.018 - ETA: 3:04 - loss: 1.017 - ETA: 3:02 - loss: 1.019 - ETA: 3:01 - loss: 1.018 - ETA: 2:59 - loss: 1.018 - ETA: 2:57 - loss: 1.019 - ETA: 2:56 - loss: 1.018 - ETA: 2:54 - loss: 1.017 - ETA: 2:52 - loss: 1.016 - ETA: 2:51 - loss: 1.017 - ETA: 2:49 - loss: 1.017 - ETA: 2:47 - loss: 1.017 - ETA: 2:46 - loss: 1.018 - ETA: 2:44 - loss: 1.019 - ETA: 2:42 - loss: 1.020 - ETA: 2:41 - loss: 1.021 - ETA: 2:39 - loss: 1.021 - ETA: 2:37 - loss: 1.021 - ETA: 2:36 - loss: 1.021 - ETA: 2:34 - loss: 1.020 - ETA: 2:32 - loss: 1.019 - ETA: 2:31 - loss: 1.019 - ETA: 2:29 - loss: 1.018 - ETA: 2:27 - loss: 1.019 - ETA: 2:26 - loss: 1.020 - ETA: 2:24 - loss: 1.021 - ETA: 2:22 - loss: 1.021 - ETA: 2:21 - loss: 1.021 - ETA: 2:19 - loss: 1.021 - ETA: 2:17 - loss: 1.020 - ETA: 2:16 - loss: 1.021 - ETA: 2:14 - loss: 1.021 - ETA: 2:12 - loss: 1.021 - ETA: 2:11 - loss: 1.021 - ETA: 2:09 - loss: 1.020 - ETA: 2:07 - loss: 1.020 - ETA: 2:06 - loss: 1.019 - ETA: 2:04 - loss: 1.019 - ETA: 2:02 - loss: 1.019 - ETA: 2:01 - loss: 1.018 - ETA: 1:59 - loss: 1.016 - ETA: 1:57 - loss: 1.016 - ETA: 1:56 - loss: 1.016 - ETA: 1:54 - loss: 1.016 - ETA: 1:52 - loss: 1.015 - ETA: 1:51 - loss: 1.016 - ETA: 1:49 - loss: 1.016 - ETA: 1:47 - loss: 1.015 - ETA: 1:46 - loss: 1.014 - ETA: 1:44 - loss: 1.015 - ETA: 1:43 - loss: 1.015 - ETA: 1:41 - loss: 1.014 - ETA: 1:39 - loss: 1.014 - ETA: 1:38 - loss: 1.014 - ETA: 1:36 - loss: 1.014 - ETA: 1:34 - loss: 1.013 - ETA: 1:33 - loss: 1.013 - ETA: 1:31 - loss: 1.014 - ETA: 1:29 - loss: 1.014 - ETA: 1:28 - loss: 1.014 - ETA: 1:26 - loss: 1.014 - ETA: 1:24 - loss: 1.013 - ETA: 1:23 - loss: 1.013 - ETA: 1:21 - loss: 1.013 - ETA: 1:19 - loss: 1.013 - ETA: 1:18 - loss: 1.013 - ETA: 1:16 - loss: 1.014 - ETA: 1:14 - loss: 1.014 - ETA: 1:13 - loss: 1.014 - ETA: 1:11 - loss: 1.015 - ETA: 1:09 - loss: 1.015 - ETA: 1:08 - loss: 1.017 - ETA: 1:06 - loss: 1.016 - ETA: 1:04 - loss: 1.014 - ETA: 1:03 - loss: 1.015 - ETA: 1:01 - loss: 1.014 - ETA: 59s - loss: 1.015 - ETA: 58s - loss: 1.01 - ETA: 56s - loss: 1.01 - ETA: 54s - loss: 1.01 - ETA: 53s - loss: 1.01 - ETA: 51s - loss: 1.01 - ETA: 49s - loss: 1.01 - ETA: 48s - loss: 1.01 - ETA: 46s - loss: 1.01 - ETA: 44s - loss: 1.01 - ETA: 43s - loss: 1.01 - ETA: 41s - loss: 1.01 - ETA: 39s - loss: 1.01 - ETA: 38s - loss: 1.01 - ETA: 36s - loss: 1.01 - ETA: 34s - loss: 1.01 - ETA: 33s - loss: 1.01 - ETA: 31s - loss: 1.01 - ETA: 29s - loss: 1.01 - ETA: 28s - loss: 1.01 - ETA: 26s - loss: 1.01 - ETA: 24s - loss: 1.01 - ETA: 23s - loss: 1.01 - ETA: 21s - loss: 1.01 - ETA: 19s - loss: 1.01 - ETA: 18s - loss: 1.01 - ETA: 16s - loss: 1.01 - ETA: 14s - loss: 1.01 - ETA: 13s - loss: 1.01 - ETA: 11s - loss: 1.01 - ETA: 9s - loss: 1.0146 - ETA: 8s - loss: 1.015 - ETA: 6s - loss: 1.015 - ETA: 4s - loss: 1.015 - ETA: 3s - loss: 1.015 - ETA: 1s - loss: 1.016 - 313s 2s/step - loss: 1.0161 - val_loss: 1.4152\n",
      "\n",
      "Epoch 00024: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:04 - loss: 1.028 - ETA: 5:04 - loss: 0.981 - ETA: 5:02 - loss: 1.007 - ETA: 5:01 - loss: 0.964 - ETA: 4:59 - loss: 0.961 - ETA: 4:58 - loss: 0.989 - ETA: 4:56 - loss: 0.983 - ETA: 4:55 - loss: 0.984 - ETA: 4:53 - loss: 0.991 - ETA: 4:51 - loss: 0.989 - ETA: 4:50 - loss: 0.984 - ETA: 4:48 - loss: 0.987 - ETA: 4:47 - loss: 0.989 - ETA: 4:45 - loss: 0.989 - ETA: 4:43 - loss: 0.993 - ETA: 4:42 - loss: 0.990 - ETA: 4:40 - loss: 0.985 - ETA: 4:38 - loss: 0.993 - ETA: 4:37 - loss: 0.995 - ETA: 4:35 - loss: 0.990 - ETA: 4:33 - loss: 0.984 - ETA: 4:32 - loss: 0.980 - ETA: 4:30 - loss: 0.978 - ETA: 4:29 - loss: 0.977 - ETA: 4:27 - loss: 0.978 - ETA: 4:25 - loss: 0.979 - ETA: 4:24 - loss: 0.981 - ETA: 4:22 - loss: 0.976 - ETA: 4:20 - loss: 0.976 - ETA: 4:19 - loss: 0.977 - ETA: 4:17 - loss: 0.984 - ETA: 4:15 - loss: 0.985 - ETA: 4:14 - loss: 0.984 - ETA: 4:12 - loss: 0.986 - ETA: 4:11 - loss: 0.988 - ETA: 4:09 - loss: 0.988 - ETA: 4:07 - loss: 0.986 - ETA: 4:06 - loss: 0.990 - ETA: 4:04 - loss: 0.988 - ETA: 4:02 - loss: 0.987 - ETA: 4:01 - loss: 0.990 - ETA: 3:59 - loss: 0.991 - ETA: 3:57 - loss: 0.994 - ETA: 3:56 - loss: 0.993 - ETA: 3:54 - loss: 0.996 - ETA: 3:52 - loss: 0.999 - ETA: 3:51 - loss: 1.002 - ETA: 3:49 - loss: 1.005 - ETA: 3:47 - loss: 1.007 - ETA: 3:46 - loss: 1.005 - ETA: 3:44 - loss: 1.005 - ETA: 3:42 - loss: 1.006 - ETA: 3:41 - loss: 1.004 - ETA: 3:39 - loss: 1.003 - ETA: 3:37 - loss: 1.004 - ETA: 3:36 - loss: 1.004 - ETA: 3:34 - loss: 1.003 - ETA: 3:32 - loss: 1.006 - ETA: 3:31 - loss: 1.005 - ETA: 3:29 - loss: 1.004 - ETA: 3:28 - loss: 1.007 - ETA: 3:26 - loss: 1.005 - ETA: 3:24 - loss: 1.003 - ETA: 3:23 - loss: 1.001 - ETA: 3:21 - loss: 0.999 - ETA: 3:19 - loss: 0.998 - ETA: 3:18 - loss: 1.000 - ETA: 3:16 - loss: 1.000 - ETA: 3:15 - loss: 1.000 - ETA: 3:13 - loss: 0.999 - ETA: 3:11 - loss: 1.001 - ETA: 3:10 - loss: 1.002 - ETA: 3:08 - loss: 1.001 - ETA: 3:06 - loss: 1.001 - ETA: 3:05 - loss: 1.002 - ETA: 3:03 - loss: 1.002 - ETA: 3:01 - loss: 1.005 - ETA: 3:00 - loss: 1.004 - ETA: 2:58 - loss: 1.004 - ETA: 2:56 - loss: 1.002 - ETA: 2:55 - loss: 1.002 - ETA: 2:53 - loss: 1.003 - ETA: 2:52 - loss: 1.003 - ETA: 2:50 - loss: 1.002 - ETA: 2:48 - loss: 1.000 - ETA: 2:47 - loss: 1.001 - ETA: 2:45 - loss: 1.000 - ETA: 2:43 - loss: 0.999 - ETA: 2:42 - loss: 1.000 - ETA: 2:40 - loss: 0.998 - ETA: 2:38 - loss: 0.999 - ETA: 2:37 - loss: 0.998 - ETA: 2:35 - loss: 0.998 - ETA: 2:33 - loss: 0.997 - ETA: 2:32 - loss: 0.997 - ETA: 2:30 - loss: 0.997 - ETA: 2:28 - loss: 0.998 - ETA: 2:27 - loss: 0.997 - ETA: 2:25 - loss: 0.998 - ETA: 2:23 - loss: 0.998 - ETA: 2:22 - loss: 0.999 - ETA: 2:20 - loss: 0.998 - ETA: 2:18 - loss: 0.999 - ETA: 2:17 - loss: 0.999 - ETA: 2:15 - loss: 0.999 - ETA: 2:13 - loss: 0.999 - ETA: 2:12 - loss: 1.001 - ETA: 2:10 - loss: 1.000 - ETA: 2:08 - loss: 1.000 - ETA: 2:07 - loss: 1.000 - ETA: 2:05 - loss: 0.999 - ETA: 2:04 - loss: 0.999 - ETA: 2:02 - loss: 0.999 - ETA: 2:00 - loss: 1.001 - ETA: 1:59 - loss: 0.999 - ETA: 1:57 - loss: 1.000 - ETA: 1:55 - loss: 0.998 - ETA: 1:54 - loss: 1.000 - ETA: 1:52 - loss: 1.001 - ETA: 1:50 - loss: 1.001 - ETA: 1:49 - loss: 1.001 - ETA: 1:47 - loss: 1.000 - ETA: 1:45 - loss: 1.000 - ETA: 1:44 - loss: 1.001 - ETA: 1:42 - loss: 1.000 - ETA: 1:40 - loss: 1.000 - ETA: 1:39 - loss: 1.001 - ETA: 1:37 - loss: 1.000 - ETA: 1:35 - loss: 1.000 - ETA: 1:34 - loss: 1.000 - ETA: 1:32 - loss: 1.000 - ETA: 1:30 - loss: 0.999 - ETA: 1:29 - loss: 1.000 - ETA: 1:27 - loss: 1.000 - ETA: 1:25 - loss: 1.000 - ETA: 1:24 - loss: 1.000 - ETA: 1:22 - loss: 0.999 - ETA: 1:21 - loss: 0.999 - ETA: 1:19 - loss: 0.999 - ETA: 1:17 - loss: 1.000 - ETA: 1:16 - loss: 0.999 - ETA: 1:14 - loss: 1.000 - ETA: 1:12 - loss: 1.000 - ETA: 1:11 - loss: 1.000 - ETA: 1:09 - loss: 1.001 - ETA: 1:07 - loss: 1.001 - ETA: 1:06 - loss: 1.001 - ETA: 1:04 - loss: 1.001 - ETA: 1:02 - loss: 1.001 - ETA: 1:01 - loss: 1.000 - ETA: 59s - loss: 1.000 - ETA: 57s - loss: 1.00 - ETA: 56s - loss: 1.00 - ETA: 54s - loss: 1.00 - ETA: 52s - loss: 1.00 - ETA: 51s - loss: 1.00 - ETA: 49s - loss: 1.00 - ETA: 47s - loss: 1.00 - ETA: 46s - loss: 1.00 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 1.00 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 1.00 - ETA: 29s - loss: 1.00 - ETA: 28s - loss: 1.00 - ETA: 26s - loss: 1.00 - ETA: 24s - loss: 1.00 - ETA: 23s - loss: 1.00 - ETA: 21s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 9s - loss: 1.0072 - ETA: 8s - loss: 1.007 - ETA: 6s - loss: 1.007 - ETA: 4s - loss: 1.006 - ETA: 3s - loss: 1.006 - ETA: 1s - loss: 1.006 - 312s 2s/step - loss: 1.0066 - val_loss: 1.4763\n",
      "\n",
      "Epoch 00025: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 26/30\n",
      "187/187 [==============================] - ETA: 5:05 - loss: 0.900 - ETA: 5:04 - loss: 0.949 - ETA: 5:02 - loss: 0.963 - ETA: 5:01 - loss: 0.938 - ETA: 5:00 - loss: 0.981 - ETA: 4:58 - loss: 1.000 - ETA: 4:56 - loss: 1.008 - ETA: 4:54 - loss: 1.014 - ETA: 4:53 - loss: 0.995 - ETA: 4:51 - loss: 1.002 - ETA: 4:50 - loss: 1.007 - ETA: 4:48 - loss: 1.011 - ETA: 4:47 - loss: 1.014 - ETA: 4:46 - loss: 1.013 - ETA: 4:45 - loss: 1.014 - ETA: 4:43 - loss: 1.008 - ETA: 4:41 - loss: 1.007 - ETA: 4:40 - loss: 1.006 - ETA: 4:38 - loss: 1.010 - ETA: 4:36 - loss: 1.013 - ETA: 4:35 - loss: 1.015 - ETA: 4:33 - loss: 1.017 - ETA: 4:31 - loss: 1.014 - ETA: 4:30 - loss: 1.013 - ETA: 4:28 - loss: 1.009 - ETA: 4:26 - loss: 1.006 - ETA: 4:24 - loss: 1.009 - ETA: 4:23 - loss: 1.011 - ETA: 4:21 - loss: 1.010 - ETA: 4:19 - loss: 1.011 - ETA: 4:18 - loss: 1.009 - ETA: 4:16 - loss: 1.012 - ETA: 4:15 - loss: 1.012 - ETA: 4:13 - loss: 1.011 - ETA: 4:11 - loss: 1.010 - ETA: 4:10 - loss: 1.012 - ETA: 4:08 - loss: 1.015 - ETA: 4:07 - loss: 1.018 - ETA: 4:05 - loss: 1.020 - ETA: 4:03 - loss: 1.021 - ETA: 4:02 - loss: 1.018 - ETA: 4:00 - loss: 1.020 - ETA: 3:58 - loss: 1.018 - ETA: 3:57 - loss: 1.015 - ETA: 3:55 - loss: 1.012 - ETA: 3:53 - loss: 1.015 - ETA: 3:52 - loss: 1.011 - ETA: 3:50 - loss: 1.013 - ETA: 3:48 - loss: 1.010 - ETA: 3:47 - loss: 1.007 - ETA: 3:45 - loss: 1.007 - ETA: 3:44 - loss: 1.007 - ETA: 3:42 - loss: 1.009 - ETA: 3:40 - loss: 1.009 - ETA: 3:39 - loss: 1.009 - ETA: 3:37 - loss: 1.011 - ETA: 3:35 - loss: 1.011 - ETA: 3:33 - loss: 1.013 - ETA: 3:32 - loss: 1.015 - ETA: 3:30 - loss: 1.016 - ETA: 3:28 - loss: 1.015 - ETA: 3:27 - loss: 1.012 - ETA: 3:25 - loss: 1.012 - ETA: 3:23 - loss: 1.009 - ETA: 3:22 - loss: 1.010 - ETA: 3:20 - loss: 1.012 - ETA: 3:18 - loss: 1.010 - ETA: 3:17 - loss: 1.010 - ETA: 3:15 - loss: 1.010 - ETA: 3:13 - loss: 1.012 - ETA: 3:12 - loss: 1.011 - ETA: 3:10 - loss: 1.012 - ETA: 3:09 - loss: 1.012 - ETA: 3:07 - loss: 1.010 - ETA: 3:05 - loss: 1.008 - ETA: 3:04 - loss: 1.008 - ETA: 3:02 - loss: 1.006 - ETA: 3:00 - loss: 1.006 - ETA: 2:59 - loss: 1.006 - ETA: 2:57 - loss: 1.007 - ETA: 2:55 - loss: 1.007 - ETA: 2:54 - loss: 1.006 - ETA: 2:52 - loss: 1.005 - ETA: 2:50 - loss: 1.005 - ETA: 2:49 - loss: 1.007 - ETA: 2:47 - loss: 1.007 - ETA: 2:45 - loss: 1.007 - ETA: 2:44 - loss: 1.008 - ETA: 2:42 - loss: 1.007 - ETA: 2:40 - loss: 1.008 - ETA: 2:39 - loss: 1.008 - ETA: 2:37 - loss: 1.006 - ETA: 2:35 - loss: 1.006 - ETA: 2:34 - loss: 1.006 - ETA: 2:32 - loss: 1.006 - ETA: 2:30 - loss: 1.005 - ETA: 2:29 - loss: 1.005 - ETA: 2:27 - loss: 1.006 - ETA: 2:25 - loss: 1.004 - ETA: 2:24 - loss: 1.005 - ETA: 2:22 - loss: 1.005 - ETA: 2:20 - loss: 1.005 - ETA: 2:19 - loss: 1.005 - ETA: 2:17 - loss: 1.003 - ETA: 2:15 - loss: 1.003 - ETA: 2:14 - loss: 1.002 - ETA: 2:12 - loss: 1.002 - ETA: 2:10 - loss: 1.002 - ETA: 2:09 - loss: 1.000 - ETA: 2:07 - loss: 1.001 - ETA: 2:05 - loss: 1.001 - ETA: 2:04 - loss: 1.001 - ETA: 2:02 - loss: 1.000 - ETA: 2:00 - loss: 1.001 - ETA: 1:59 - loss: 1.002 - ETA: 1:57 - loss: 1.004 - ETA: 1:56 - loss: 1.004 - ETA: 1:54 - loss: 1.005 - ETA: 1:52 - loss: 1.006 - ETA: 1:51 - loss: 1.007 - ETA: 1:49 - loss: 1.005 - ETA: 1:47 - loss: 1.007 - ETA: 1:46 - loss: 1.006 - ETA: 1:44 - loss: 1.007 - ETA: 1:42 - loss: 1.008 - ETA: 1:41 - loss: 1.008 - ETA: 1:39 - loss: 1.009 - ETA: 1:37 - loss: 1.009 - ETA: 1:36 - loss: 1.009 - ETA: 1:34 - loss: 1.009 - ETA: 1:32 - loss: 1.008 - ETA: 1:31 - loss: 1.007 - ETA: 1:29 - loss: 1.006 - ETA: 1:27 - loss: 1.006 - ETA: 1:26 - loss: 1.005 - ETA: 1:24 - loss: 1.006 - ETA: 1:22 - loss: 1.006 - ETA: 1:21 - loss: 1.006 - ETA: 1:19 - loss: 1.005 - ETA: 1:17 - loss: 1.005 - ETA: 1:16 - loss: 1.006 - ETA: 1:14 - loss: 1.005 - ETA: 1:12 - loss: 1.006 - ETA: 1:11 - loss: 1.005 - ETA: 1:09 - loss: 1.005 - ETA: 1:07 - loss: 1.005 - ETA: 1:06 - loss: 1.006 - ETA: 1:04 - loss: 1.007 - ETA: 1:02 - loss: 1.008 - ETA: 1:01 - loss: 1.007 - ETA: 59s - loss: 1.008 - ETA: 57s - loss: 1.00 - ETA: 56s - loss: 1.00 - ETA: 54s - loss: 1.00 - ETA: 52s - loss: 1.00 - ETA: 51s - loss: 1.00 - ETA: 49s - loss: 1.00 - ETA: 48s - loss: 1.00 - ETA: 46s - loss: 1.00 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 1.00 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 1.00 - ETA: 29s - loss: 1.00 - ETA: 28s - loss: 1.00 - ETA: 26s - loss: 1.00 - ETA: 24s - loss: 1.00 - ETA: 23s - loss: 1.00 - ETA: 21s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 9s - loss: 1.0046 - ETA: 8s - loss: 1.005 - ETA: 6s - loss: 1.005 - ETA: 4s - loss: 1.004 - ETA: 3s - loss: 1.004 - ETA: 1s - loss: 1.004 - 312s 2s/step - loss: 1.0046 - val_loss: 1.3051\n",
      "\n",
      "Epoch 00026: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:04 - loss: 0.869 - ETA: 5:04 - loss: 0.891 - ETA: 5:04 - loss: 0.976 - ETA: 5:03 - loss: 0.995 - ETA: 5:01 - loss: 1.000 - ETA: 5:00 - loss: 1.003 - ETA: 4:59 - loss: 0.995 - ETA: 4:57 - loss: 1.013 - ETA: 4:55 - loss: 1.007 - ETA: 4:53 - loss: 0.990 - ETA: 4:52 - loss: 0.993 - ETA: 4:50 - loss: 0.989 - ETA: 4:49 - loss: 0.989 - ETA: 4:47 - loss: 0.979 - ETA: 4:45 - loss: 0.999 - ETA: 4:43 - loss: 0.997 - ETA: 4:42 - loss: 0.993 - ETA: 4:40 - loss: 0.999 - ETA: 4:39 - loss: 0.997 - ETA: 4:37 - loss: 0.991 - ETA: 4:35 - loss: 0.982 - ETA: 4:34 - loss: 0.982 - ETA: 4:32 - loss: 0.988 - ETA: 4:30 - loss: 0.988 - ETA: 4:29 - loss: 0.986 - ETA: 4:27 - loss: 0.986 - ETA: 4:25 - loss: 0.986 - ETA: 4:24 - loss: 0.987 - ETA: 4:22 - loss: 0.988 - ETA: 4:20 - loss: 0.993 - ETA: 4:18 - loss: 0.989 - ETA: 4:17 - loss: 0.994 - ETA: 4:15 - loss: 0.995 - ETA: 4:13 - loss: 0.989 - ETA: 4:12 - loss: 0.992 - ETA: 4:10 - loss: 0.990 - ETA: 4:08 - loss: 0.990 - ETA: 4:06 - loss: 0.990 - ETA: 4:05 - loss: 0.990 - ETA: 4:03 - loss: 0.987 - ETA: 4:02 - loss: 0.989 - ETA: 4:00 - loss: 0.988 - ETA: 3:58 - loss: 0.991 - ETA: 3:57 - loss: 0.991 - ETA: 3:55 - loss: 0.989 - ETA: 3:53 - loss: 0.992 - ETA: 3:52 - loss: 0.991 - ETA: 3:50 - loss: 0.993 - ETA: 3:48 - loss: 0.991 - ETA: 3:47 - loss: 0.991 - ETA: 3:45 - loss: 0.989 - ETA: 3:43 - loss: 0.986 - ETA: 3:41 - loss: 0.985 - ETA: 3:40 - loss: 0.985 - ETA: 3:38 - loss: 0.987 - ETA: 3:36 - loss: 0.989 - ETA: 3:35 - loss: 0.990 - ETA: 3:33 - loss: 0.990 - ETA: 3:32 - loss: 0.990 - ETA: 3:30 - loss: 0.988 - ETA: 3:28 - loss: 0.988 - ETA: 3:26 - loss: 0.986 - ETA: 3:25 - loss: 0.985 - ETA: 3:23 - loss: 0.986 - ETA: 3:21 - loss: 0.986 - ETA: 3:20 - loss: 0.989 - ETA: 3:18 - loss: 0.990 - ETA: 3:16 - loss: 0.993 - ETA: 3:15 - loss: 0.992 - ETA: 3:13 - loss: 0.994 - ETA: 3:11 - loss: 0.995 - ETA: 3:10 - loss: 0.995 - ETA: 3:08 - loss: 0.995 - ETA: 3:06 - loss: 0.996 - ETA: 3:05 - loss: 0.998 - ETA: 3:03 - loss: 0.998 - ETA: 3:02 - loss: 1.000 - ETA: 3:00 - loss: 1.002 - ETA: 2:58 - loss: 1.001 - ETA: 2:57 - loss: 1.002 - ETA: 2:55 - loss: 1.003 - ETA: 2:53 - loss: 1.003 - ETA: 2:52 - loss: 1.001 - ETA: 2:50 - loss: 1.001 - ETA: 2:48 - loss: 0.999 - ETA: 2:47 - loss: 0.999 - ETA: 2:45 - loss: 1.000 - ETA: 2:43 - loss: 1.001 - ETA: 2:42 - loss: 1.001 - ETA: 2:40 - loss: 1.002 - ETA: 2:38 - loss: 1.002 - ETA: 2:37 - loss: 1.000 - ETA: 2:35 - loss: 0.999 - ETA: 2:34 - loss: 0.999 - ETA: 2:32 - loss: 0.997 - ETA: 2:30 - loss: 0.996 - ETA: 2:29 - loss: 0.996 - ETA: 2:27 - loss: 0.999 - ETA: 2:25 - loss: 0.998 - ETA: 2:24 - loss: 0.997 - ETA: 2:22 - loss: 0.998 - ETA: 2:20 - loss: 0.997 - ETA: 2:19 - loss: 0.997 - ETA: 2:17 - loss: 0.998 - ETA: 2:15 - loss: 0.999 - ETA: 2:14 - loss: 1.000 - ETA: 2:12 - loss: 1.000 - ETA: 2:10 - loss: 1.000 - ETA: 2:09 - loss: 1.001 - ETA: 2:07 - loss: 0.999 - ETA: 2:05 - loss: 0.998 - ETA: 2:04 - loss: 0.998 - ETA: 2:02 - loss: 0.999 - ETA: 2:00 - loss: 0.999 - ETA: 1:59 - loss: 1.000 - ETA: 1:57 - loss: 0.999 - ETA: 1:55 - loss: 0.999 - ETA: 1:54 - loss: 1.000 - ETA: 1:52 - loss: 0.999 - ETA: 1:50 - loss: 0.998 - ETA: 1:49 - loss: 0.999 - ETA: 1:47 - loss: 0.998 - ETA: 1:45 - loss: 0.999 - ETA: 1:44 - loss: 0.999 - ETA: 1:42 - loss: 1.001 - ETA: 1:41 - loss: 1.001 - ETA: 1:39 - loss: 1.001 - ETA: 1:37 - loss: 1.000 - ETA: 1:36 - loss: 1.000 - ETA: 1:34 - loss: 0.999 - ETA: 1:32 - loss: 0.998 - ETA: 1:31 - loss: 0.998 - ETA: 1:29 - loss: 0.997 - ETA: 1:27 - loss: 0.996 - ETA: 1:26 - loss: 0.997 - ETA: 1:24 - loss: 0.997 - ETA: 1:22 - loss: 0.998 - ETA: 1:21 - loss: 0.997 - ETA: 1:19 - loss: 0.997 - ETA: 1:17 - loss: 0.997 - ETA: 1:16 - loss: 0.997 - ETA: 1:14 - loss: 0.996 - ETA: 1:12 - loss: 0.996 - ETA: 1:11 - loss: 0.997 - ETA: 1:09 - loss: 0.997 - ETA: 1:07 - loss: 0.997 - ETA: 1:06 - loss: 0.997 - ETA: 1:04 - loss: 0.997 - ETA: 1:02 - loss: 0.997 - ETA: 1:01 - loss: 0.997 - ETA: 59s - loss: 0.997 - ETA: 57s - loss: 0.99 - ETA: 56s - loss: 0.99 - ETA: 54s - loss: 0.99 - ETA: 52s - loss: 0.99 - ETA: 51s - loss: 0.99 - ETA: 49s - loss: 0.99 - ETA: 48s - loss: 0.99 - ETA: 46s - loss: 0.99 - ETA: 44s - loss: 0.99 - ETA: 43s - loss: 0.99 - ETA: 41s - loss: 0.99 - ETA: 39s - loss: 0.99 - ETA: 38s - loss: 0.99 - ETA: 36s - loss: 0.99 - ETA: 34s - loss: 0.99 - ETA: 33s - loss: 0.99 - ETA: 31s - loss: 0.99 - ETA: 29s - loss: 0.99 - ETA: 28s - loss: 0.99 - ETA: 26s - loss: 0.99 - ETA: 24s - loss: 0.99 - ETA: 23s - loss: 0.99 - ETA: 21s - loss: 0.99 - ETA: 19s - loss: 0.99 - ETA: 18s - loss: 0.99 - ETA: 16s - loss: 0.99 - ETA: 14s - loss: 0.99 - ETA: 13s - loss: 0.99 - ETA: 11s - loss: 0.99 - ETA: 9s - loss: 0.9984 - ETA: 8s - loss: 0.997 - ETA: 6s - loss: 0.997 - ETA: 4s - loss: 0.998 - ETA: 3s - loss: 0.997 - ETA: 1s - loss: 0.998 - 312s 2s/step - loss: 0.9975 - val_loss: 1.3459\n",
      "\n",
      "Epoch 00027: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 28/30\n",
      "187/187 [==============================] - ETA: 5:06 - loss: 0.943 - ETA: 5:06 - loss: 1.002 - ETA: 5:03 - loss: 0.975 - ETA: 5:02 - loss: 0.981 - ETA: 5:01 - loss: 0.975 - ETA: 4:58 - loss: 0.969 - ETA: 4:57 - loss: 0.962 - ETA: 4:56 - loss: 0.971 - ETA: 4:54 - loss: 0.967 - ETA: 4:53 - loss: 0.963 - ETA: 4:51 - loss: 0.981 - ETA: 4:50 - loss: 0.984 - ETA: 4:48 - loss: 0.993 - ETA: 4:46 - loss: 0.998 - ETA: 4:44 - loss: 1.019 - ETA: 4:43 - loss: 1.018 - ETA: 4:41 - loss: 1.007 - ETA: 4:39 - loss: 1.005 - ETA: 4:38 - loss: 1.007 - ETA: 4:36 - loss: 1.006 - ETA: 4:34 - loss: 1.002 - ETA: 4:33 - loss: 1.004 - ETA: 4:31 - loss: 1.005 - ETA: 4:29 - loss: 1.006 - ETA: 4:28 - loss: 0.999 - ETA: 4:26 - loss: 0.999 - ETA: 4:24 - loss: 0.997 - ETA: 4:23 - loss: 0.994 - ETA: 4:21 - loss: 0.996 - ETA: 4:19 - loss: 0.999 - ETA: 4:18 - loss: 1.000 - ETA: 4:17 - loss: 0.997 - ETA: 4:15 - loss: 0.993 - ETA: 4:13 - loss: 0.990 - ETA: 4:12 - loss: 0.989 - ETA: 4:10 - loss: 0.986 - ETA: 4:08 - loss: 0.986 - ETA: 4:07 - loss: 0.984 - ETA: 4:05 - loss: 0.983 - ETA: 4:03 - loss: 0.984 - ETA: 4:02 - loss: 0.983 - ETA: 4:00 - loss: 0.980 - ETA: 3:58 - loss: 0.983 - ETA: 3:57 - loss: 0.984 - ETA: 3:55 - loss: 0.980 - ETA: 3:54 - loss: 0.982 - ETA: 3:52 - loss: 0.982 - ETA: 3:50 - loss: 0.982 - ETA: 3:49 - loss: 0.984 - ETA: 3:47 - loss: 0.985 - ETA: 3:46 - loss: 0.984 - ETA: 3:44 - loss: 0.982 - ETA: 3:42 - loss: 0.981 - ETA: 3:41 - loss: 0.978 - ETA: 3:39 - loss: 0.978 - ETA: 3:37 - loss: 0.978 - ETA: 3:36 - loss: 0.978 - ETA: 3:34 - loss: 0.977 - ETA: 3:32 - loss: 0.974 - ETA: 3:31 - loss: 0.978 - ETA: 3:29 - loss: 0.978 - ETA: 3:27 - loss: 0.976 - ETA: 3:26 - loss: 0.979 - ETA: 3:24 - loss: 0.976 - ETA: 3:22 - loss: 0.976 - ETA: 3:21 - loss: 0.977 - ETA: 3:19 - loss: 0.978 - ETA: 3:18 - loss: 0.978 - ETA: 3:16 - loss: 0.978 - ETA: 3:14 - loss: 0.977 - ETA: 3:13 - loss: 0.980 - ETA: 3:11 - loss: 0.981 - ETA: 3:09 - loss: 0.982 - ETA: 3:08 - loss: 0.985 - ETA: 3:06 - loss: 0.984 - ETA: 3:04 - loss: 0.984 - ETA: 3:03 - loss: 0.983 - ETA: 3:01 - loss: 0.983 - ETA: 2:59 - loss: 0.982 - ETA: 2:58 - loss: 0.981 - ETA: 2:56 - loss: 0.982 - ETA: 2:54 - loss: 0.982 - ETA: 2:53 - loss: 0.985 - ETA: 2:51 - loss: 0.988 - ETA: 2:49 - loss: 0.987 - ETA: 2:48 - loss: 0.987 - ETA: 2:46 - loss: 0.988 - ETA: 2:44 - loss: 0.988 - ETA: 2:43 - loss: 0.987 - ETA: 2:41 - loss: 0.988 - ETA: 2:39 - loss: 0.987 - ETA: 2:38 - loss: 0.988 - ETA: 2:36 - loss: 0.987 - ETA: 2:34 - loss: 0.985 - ETA: 2:33 - loss: 0.987 - ETA: 2:31 - loss: 0.988 - ETA: 2:29 - loss: 0.987 - ETA: 2:28 - loss: 0.987 - ETA: 2:26 - loss: 0.987 - ETA: 2:24 - loss: 0.987 - ETA: 2:23 - loss: 0.987 - ETA: 2:21 - loss: 0.987 - ETA: 2:19 - loss: 0.985 - ETA: 2:18 - loss: 0.986 - ETA: 2:16 - loss: 0.986 - ETA: 2:14 - loss: 0.987 - ETA: 2:13 - loss: 0.987 - ETA: 2:11 - loss: 0.986 - ETA: 2:09 - loss: 0.988 - ETA: 2:08 - loss: 0.988 - ETA: 2:06 - loss: 0.987 - ETA: 2:04 - loss: 0.987 - ETA: 2:03 - loss: 0.987 - ETA: 2:01 - loss: 0.987 - ETA: 1:59 - loss: 0.989 - ETA: 1:58 - loss: 0.988 - ETA: 1:56 - loss: 0.989 - ETA: 1:54 - loss: 0.988 - ETA: 1:53 - loss: 0.988 - ETA: 1:51 - loss: 0.988 - ETA: 1:49 - loss: 0.988 - ETA: 1:48 - loss: 0.988 - ETA: 1:46 - loss: 0.987 - ETA: 1:44 - loss: 0.986 - ETA: 1:43 - loss: 0.988 - ETA: 1:41 - loss: 0.988 - ETA: 1:39 - loss: 0.988 - ETA: 1:38 - loss: 0.988 - ETA: 1:36 - loss: 0.988 - ETA: 1:34 - loss: 0.989 - ETA: 1:33 - loss: 0.989 - ETA: 1:31 - loss: 0.990 - ETA: 1:29 - loss: 0.992 - ETA: 1:28 - loss: 0.992 - ETA: 1:26 - loss: 0.992 - ETA: 1:24 - loss: 0.992 - ETA: 1:23 - loss: 0.991 - ETA: 1:21 - loss: 0.993 - ETA: 1:19 - loss: 0.993 - ETA: 1:18 - loss: 0.993 - ETA: 1:16 - loss: 0.993 - ETA: 1:14 - loss: 0.993 - ETA: 1:13 - loss: 0.994 - ETA: 1:11 - loss: 0.995 - ETA: 1:09 - loss: 0.996 - ETA: 1:08 - loss: 0.997 - ETA: 1:06 - loss: 0.998 - ETA: 1:04 - loss: 0.998 - ETA: 1:03 - loss: 0.998 - ETA: 1:01 - loss: 0.998 - ETA: 59s - loss: 0.998 - ETA: 58s - loss: 0.99 - ETA: 56s - loss: 0.99 - ETA: 54s - loss: 0.99 - ETA: 53s - loss: 0.99 - ETA: 51s - loss: 0.99 - ETA: 49s - loss: 0.99 - ETA: 48s - loss: 0.99 - ETA: 46s - loss: 0.99 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 1.00 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 1.00 - ETA: 29s - loss: 1.00 - ETA: 28s - loss: 1.00 - ETA: 26s - loss: 1.00 - ETA: 24s - loss: 1.00 - ETA: 23s - loss: 1.00 - ETA: 21s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 9s - loss: 1.0046 - ETA: 8s - loss: 1.004 - ETA: 6s - loss: 1.004 - ETA: 4s - loss: 1.004 - ETA: 3s - loss: 1.003 - ETA: 1s - loss: 1.003 - 313s 2s/step - loss: 1.0035 - val_loss: 1.3958\n",
      "\n",
      "Epoch 00028: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 5:04 - loss: 1.003 - ETA: 5:03 - loss: 1.095 - ETA: 5:02 - loss: 1.039 - ETA: 5:01 - loss: 1.045 - ETA: 4:59 - loss: 1.061 - ETA: 4:57 - loss: 1.035 - ETA: 4:56 - loss: 1.043 - ETA: 4:54 - loss: 1.046 - ETA: 4:53 - loss: 1.038 - ETA: 4:51 - loss: 1.042 - ETA: 4:50 - loss: 1.034 - ETA: 4:48 - loss: 1.025 - ETA: 4:46 - loss: 1.029 - ETA: 4:45 - loss: 1.024 - ETA: 4:43 - loss: 1.031 - ETA: 4:42 - loss: 1.039 - ETA: 4:40 - loss: 1.042 - ETA: 4:38 - loss: 1.050 - ETA: 4:37 - loss: 1.047 - ETA: 4:35 - loss: 1.040 - ETA: 4:34 - loss: 1.050 - ETA: 4:32 - loss: 1.043 - ETA: 4:30 - loss: 1.034 - ETA: 4:29 - loss: 1.034 - ETA: 4:27 - loss: 1.034 - ETA: 4:25 - loss: 1.025 - ETA: 4:24 - loss: 1.024 - ETA: 4:22 - loss: 1.021 - ETA: 4:20 - loss: 1.025 - ETA: 4:19 - loss: 1.028 - ETA: 4:17 - loss: 1.030 - ETA: 4:15 - loss: 1.032 - ETA: 4:14 - loss: 1.029 - ETA: 4:12 - loss: 1.027 - ETA: 4:10 - loss: 1.029 - ETA: 4:09 - loss: 1.031 - ETA: 4:07 - loss: 1.031 - ETA: 4:05 - loss: 1.032 - ETA: 4:04 - loss: 1.031 - ETA: 4:02 - loss: 1.027 - ETA: 4:01 - loss: 1.024 - ETA: 3:59 - loss: 1.019 - ETA: 3:57 - loss: 1.019 - ETA: 3:56 - loss: 1.018 - ETA: 3:54 - loss: 1.019 - ETA: 3:52 - loss: 1.018 - ETA: 3:51 - loss: 1.016 - ETA: 3:49 - loss: 1.018 - ETA: 3:47 - loss: 1.019 - ETA: 3:46 - loss: 1.016 - ETA: 3:44 - loss: 1.019 - ETA: 3:42 - loss: 1.017 - ETA: 3:41 - loss: 1.018 - ETA: 3:39 - loss: 1.017 - ETA: 3:37 - loss: 1.015 - ETA: 3:36 - loss: 1.017 - ETA: 3:34 - loss: 1.014 - ETA: 3:33 - loss: 1.015 - ETA: 3:31 - loss: 1.015 - ETA: 3:29 - loss: 1.015 - ETA: 3:28 - loss: 1.016 - ETA: 3:26 - loss: 1.014 - ETA: 3:25 - loss: 1.012 - ETA: 3:23 - loss: 1.009 - ETA: 3:21 - loss: 1.010 - ETA: 3:20 - loss: 1.010 - ETA: 3:18 - loss: 1.012 - ETA: 3:16 - loss: 1.012 - ETA: 3:15 - loss: 1.011 - ETA: 3:13 - loss: 1.011 - ETA: 3:11 - loss: 1.013 - ETA: 3:10 - loss: 1.014 - ETA: 3:08 - loss: 1.011 - ETA: 3:06 - loss: 1.011 - ETA: 3:05 - loss: 1.011 - ETA: 3:03 - loss: 1.011 - ETA: 3:01 - loss: 1.010 - ETA: 3:00 - loss: 1.008 - ETA: 2:58 - loss: 1.009 - ETA: 2:56 - loss: 1.009 - ETA: 2:55 - loss: 1.010 - ETA: 2:53 - loss: 1.010 - ETA: 2:51 - loss: 1.009 - ETA: 2:50 - loss: 1.008 - ETA: 2:48 - loss: 1.008 - ETA: 2:46 - loss: 1.009 - ETA: 2:45 - loss: 1.009 - ETA: 2:43 - loss: 1.009 - ETA: 2:41 - loss: 1.010 - ETA: 2:40 - loss: 1.012 - ETA: 2:38 - loss: 1.012 - ETA: 2:37 - loss: 1.014 - ETA: 2:35 - loss: 1.013 - ETA: 2:33 - loss: 1.012 - ETA: 2:32 - loss: 1.012 - ETA: 2:30 - loss: 1.012 - ETA: 2:28 - loss: 1.012 - ETA: 2:27 - loss: 1.012 - ETA: 2:25 - loss: 1.013 - ETA: 2:23 - loss: 1.011 - ETA: 2:22 - loss: 1.011 - ETA: 2:20 - loss: 1.011 - ETA: 2:18 - loss: 1.010 - ETA: 2:17 - loss: 1.008 - ETA: 2:15 - loss: 1.010 - ETA: 2:13 - loss: 1.009 - ETA: 2:12 - loss: 1.009 - ETA: 2:10 - loss: 1.010 - ETA: 2:08 - loss: 1.010 - ETA: 2:07 - loss: 1.010 - ETA: 2:05 - loss: 1.008 - ETA: 2:03 - loss: 1.007 - ETA: 2:02 - loss: 1.009 - ETA: 2:00 - loss: 1.010 - ETA: 1:59 - loss: 1.011 - ETA: 1:57 - loss: 1.010 - ETA: 1:55 - loss: 1.009 - ETA: 1:54 - loss: 1.009 - ETA: 1:52 - loss: 1.009 - ETA: 1:50 - loss: 1.006 - ETA: 1:49 - loss: 1.006 - ETA: 1:47 - loss: 1.006 - ETA: 1:45 - loss: 1.005 - ETA: 1:44 - loss: 1.004 - ETA: 1:42 - loss: 1.005 - ETA: 1:40 - loss: 1.005 - ETA: 1:39 - loss: 1.004 - ETA: 1:37 - loss: 1.004 - ETA: 1:35 - loss: 1.003 - ETA: 1:34 - loss: 1.003 - ETA: 1:32 - loss: 1.003 - ETA: 1:30 - loss: 1.003 - ETA: 1:29 - loss: 1.002 - ETA: 1:27 - loss: 1.002 - ETA: 1:25 - loss: 1.002 - ETA: 1:24 - loss: 1.001 - ETA: 1:22 - loss: 1.000 - ETA: 1:21 - loss: 1.000 - ETA: 1:19 - loss: 1.001 - ETA: 1:17 - loss: 1.001 - ETA: 1:16 - loss: 1.000 - ETA: 1:14 - loss: 1.000 - ETA: 1:12 - loss: 1.000 - ETA: 1:11 - loss: 1.000 - ETA: 1:09 - loss: 0.999 - ETA: 1:07 - loss: 0.998 - ETA: 1:06 - loss: 0.999 - ETA: 1:04 - loss: 0.999 - ETA: 1:02 - loss: 0.998 - ETA: 1:01 - loss: 0.998 - ETA: 59s - loss: 0.997 - ETA: 57s - loss: 0.99 - ETA: 56s - loss: 0.99 - ETA: 54s - loss: 0.99 - ETA: 52s - loss: 0.99 - ETA: 51s - loss: 0.99 - ETA: 49s - loss: 0.99 - ETA: 47s - loss: 0.99 - ETA: 46s - loss: 0.99 - ETA: 44s - loss: 0.99 - ETA: 43s - loss: 0.99 - ETA: 41s - loss: 0.99 - ETA: 39s - loss: 0.99 - ETA: 38s - loss: 0.99 - ETA: 36s - loss: 0.99 - ETA: 34s - loss: 0.99 - ETA: 33s - loss: 0.99 - ETA: 31s - loss: 0.99 - ETA: 29s - loss: 0.99 - ETA: 28s - loss: 0.99 - ETA: 26s - loss: 0.99 - ETA: 24s - loss: 0.99 - ETA: 23s - loss: 0.99 - ETA: 21s - loss: 0.99 - ETA: 19s - loss: 0.99 - ETA: 18s - loss: 0.99 - ETA: 16s - loss: 0.99 - ETA: 14s - loss: 0.99 - ETA: 13s - loss: 0.99 - ETA: 11s - loss: 0.99 - ETA: 9s - loss: 0.9960 - ETA: 8s - loss: 0.996 - ETA: 6s - loss: 0.996 - ETA: 4s - loss: 0.996 - ETA: 3s - loss: 0.995 - ETA: 1s - loss: 0.995 - 312s 2s/step - loss: 0.9949 - val_loss: 1.2651\n",
      "\n",
      "Epoch 00029: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 30/30\n",
      "187/187 [==============================] - ETA: 5:04 - loss: 1.038 - ETA: 5:04 - loss: 1.033 - ETA: 5:03 - loss: 1.017 - ETA: 5:02 - loss: 1.034 - ETA: 5:00 - loss: 1.059 - ETA: 4:58 - loss: 1.062 - ETA: 4:57 - loss: 1.072 - ETA: 4:55 - loss: 1.065 - ETA: 4:53 - loss: 1.071 - ETA: 4:52 - loss: 1.064 - ETA: 4:50 - loss: 1.061 - ETA: 4:48 - loss: 1.055 - ETA: 4:47 - loss: 1.049 - ETA: 4:45 - loss: 1.037 - ETA: 4:44 - loss: 1.034 - ETA: 4:42 - loss: 1.030 - ETA: 4:40 - loss: 1.023 - ETA: 4:39 - loss: 1.025 - ETA: 4:37 - loss: 1.019 - ETA: 4:36 - loss: 1.013 - ETA: 4:34 - loss: 1.011 - ETA: 4:32 - loss: 1.011 - ETA: 4:30 - loss: 1.017 - ETA: 4:29 - loss: 1.015 - ETA: 4:27 - loss: 1.006 - ETA: 4:25 - loss: 1.009 - ETA: 4:24 - loss: 1.014 - ETA: 4:22 - loss: 1.016 - ETA: 4:21 - loss: 1.015 - ETA: 4:19 - loss: 1.018 - ETA: 4:17 - loss: 1.015 - ETA: 4:16 - loss: 1.015 - ETA: 4:14 - loss: 1.017 - ETA: 4:12 - loss: 1.020 - ETA: 4:11 - loss: 1.022 - ETA: 4:09 - loss: 1.022 - ETA: 4:07 - loss: 1.022 - ETA: 4:06 - loss: 1.020 - ETA: 4:04 - loss: 1.018 - ETA: 4:02 - loss: 1.016 - ETA: 4:01 - loss: 1.017 - ETA: 3:59 - loss: 1.018 - ETA: 3:57 - loss: 1.018 - ETA: 3:56 - loss: 1.013 - ETA: 3:54 - loss: 1.014 - ETA: 3:53 - loss: 1.012 - ETA: 3:51 - loss: 1.010 - ETA: 3:49 - loss: 1.010 - ETA: 3:48 - loss: 1.010 - ETA: 3:46 - loss: 1.012 - ETA: 3:44 - loss: 1.014 - ETA: 3:43 - loss: 1.011 - ETA: 3:41 - loss: 1.008 - ETA: 3:39 - loss: 1.006 - ETA: 3:38 - loss: 1.005 - ETA: 3:36 - loss: 1.007 - ETA: 3:34 - loss: 1.008 - ETA: 3:33 - loss: 1.007 - ETA: 3:31 - loss: 1.005 - ETA: 3:29 - loss: 1.005 - ETA: 3:28 - loss: 1.006 - ETA: 3:26 - loss: 1.005 - ETA: 3:25 - loss: 1.003 - ETA: 3:23 - loss: 1.005 - ETA: 3:21 - loss: 1.006 - ETA: 3:20 - loss: 1.005 - ETA: 3:18 - loss: 1.003 - ETA: 3:16 - loss: 1.002 - ETA: 3:15 - loss: 1.004 - ETA: 3:13 - loss: 1.003 - ETA: 3:11 - loss: 1.003 - ETA: 3:10 - loss: 1.006 - ETA: 3:08 - loss: 1.007 - ETA: 3:07 - loss: 1.007 - ETA: 3:05 - loss: 1.005 - ETA: 3:03 - loss: 1.005 - ETA: 3:02 - loss: 1.005 - ETA: 3:00 - loss: 1.004 - ETA: 2:58 - loss: 1.004 - ETA: 2:57 - loss: 1.001 - ETA: 2:55 - loss: 1.003 - ETA: 2:53 - loss: 1.003 - ETA: 2:52 - loss: 1.003 - ETA: 2:50 - loss: 1.002 - ETA: 2:48 - loss: 1.002 - ETA: 2:47 - loss: 1.001 - ETA: 2:45 - loss: 1.003 - ETA: 2:43 - loss: 1.002 - ETA: 2:42 - loss: 1.000 - ETA: 2:40 - loss: 1.001 - ETA: 2:38 - loss: 1.002 - ETA: 2:37 - loss: 1.003 - ETA: 2:35 - loss: 1.002 - ETA: 2:33 - loss: 1.002 - ETA: 2:32 - loss: 1.002 - ETA: 2:30 - loss: 1.002 - ETA: 2:28 - loss: 1.002 - ETA: 2:27 - loss: 1.003 - ETA: 2:25 - loss: 1.002 - ETA: 2:23 - loss: 1.002 - ETA: 2:22 - loss: 1.002 - ETA: 2:20 - loss: 1.002 - ETA: 2:18 - loss: 1.001 - ETA: 2:17 - loss: 1.002 - ETA: 2:15 - loss: 1.002 - ETA: 2:13 - loss: 1.004 - ETA: 2:12 - loss: 1.004 - ETA: 2:10 - loss: 1.006 - ETA: 2:08 - loss: 1.005 - ETA: 2:07 - loss: 1.004 - ETA: 2:05 - loss: 1.005 - ETA: 2:04 - loss: 1.006 - ETA: 2:02 - loss: 1.007 - ETA: 2:00 - loss: 1.006 - ETA: 1:59 - loss: 1.005 - ETA: 1:57 - loss: 1.005 - ETA: 1:55 - loss: 1.006 - ETA: 1:54 - loss: 1.007 - ETA: 1:52 - loss: 1.007 - ETA: 1:50 - loss: 1.007 - ETA: 1:49 - loss: 1.007 - ETA: 1:47 - loss: 1.007 - ETA: 1:45 - loss: 1.008 - ETA: 1:44 - loss: 1.008 - ETA: 1:42 - loss: 1.008 - ETA: 1:40 - loss: 1.007 - ETA: 1:39 - loss: 1.006 - ETA: 1:37 - loss: 1.005 - ETA: 1:35 - loss: 1.004 - ETA: 1:34 - loss: 1.004 - ETA: 1:32 - loss: 1.002 - ETA: 1:30 - loss: 1.002 - ETA: 1:29 - loss: 1.001 - ETA: 1:27 - loss: 1.003 - ETA: 1:26 - loss: 1.004 - ETA: 1:24 - loss: 1.005 - ETA: 1:22 - loss: 1.004 - ETA: 1:21 - loss: 1.003 - ETA: 1:19 - loss: 1.004 - ETA: 1:17 - loss: 1.004 - ETA: 1:16 - loss: 1.003 - ETA: 1:14 - loss: 1.003 - ETA: 1:12 - loss: 1.002 - ETA: 1:11 - loss: 1.001 - ETA: 1:09 - loss: 1.001 - ETA: 1:07 - loss: 1.001 - ETA: 1:06 - loss: 1.001 - ETA: 1:04 - loss: 1.002 - ETA: 1:02 - loss: 1.002 - ETA: 1:01 - loss: 1.003 - ETA: 59s - loss: 1.001 - ETA: 57s - loss: 1.00 - ETA: 56s - loss: 1.00 - ETA: 54s - loss: 1.00 - ETA: 52s - loss: 1.00 - ETA: 51s - loss: 1.00 - ETA: 49s - loss: 1.00 - ETA: 47s - loss: 1.00 - ETA: 46s - loss: 1.00 - ETA: 44s - loss: 1.00 - ETA: 43s - loss: 1.00 - ETA: 41s - loss: 1.00 - ETA: 39s - loss: 1.00 - ETA: 38s - loss: 1.00 - ETA: 36s - loss: 1.00 - ETA: 34s - loss: 0.99 - ETA: 33s - loss: 1.00 - ETA: 31s - loss: 0.99 - ETA: 29s - loss: 0.99 - ETA: 28s - loss: 0.99 - ETA: 26s - loss: 0.99 - ETA: 24s - loss: 0.99 - ETA: 23s - loss: 0.99 - ETA: 21s - loss: 0.99 - ETA: 19s - loss: 0.99 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 0.99 - ETA: 14s - loss: 0.99 - ETA: 13s - loss: 0.99 - ETA: 11s - loss: 0.99 - ETA: 9s - loss: 1.0004 - ETA: 8s - loss: 1.001 - ETA: 6s - loss: 1.001 - ETA: 4s - loss: 1.001 - ETA: 3s - loss: 1.001 - ETA: 1s - loss: 1.001 - 312s 2s/step - loss: 1.0006 - val_loss: 1.4113\n",
      "\n",
      "Epoch 00030: saving model to ./model_files/weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "start = time.time()\n",
    "callbacks = decoder_model.fit_generator(generator=generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[checkpoints, reduce_lr],\n",
    "                            validation_data=val_generator,\n",
    "                            validation_steps=5)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training: 9415.719113588333 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Time for training: {} seconds\".format(time_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./callbacks'):\n",
    "    os.mkdir('./callbacks')   \n",
    "columns = callbacks.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_df = pd.DataFrame(callbacks.history)\n",
    "callback_df.to_csv(callbacks_path, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
