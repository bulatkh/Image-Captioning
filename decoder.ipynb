{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "from keras.applications import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, Callback, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, GRU, Flatten, Dropout, BatchNormalization, RepeatVector, concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import coco_parse\n",
    "import flickr8k_parse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import text_processing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building decoder, it is necessary to encode captions into one-hot vectors which further would be used in embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_path = 'D:/coco/annotations/'\n",
    "# images_path = 'D:/coco/images/'\n",
    "\n",
    "# # parse JSON file with captions to get paths to images with captions\n",
    "# val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=False)\n",
    "# val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "# train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=True)\n",
    "# train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)\n",
    "\n",
    "# ### Extract captions\n",
    "# train_captions = coco_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "# val_captions = coco_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'D:/Flickr8k/images/'\n",
    "annotations_path = 'D:/Flickr8k/annotations/'\n",
    "captions_file = 'D:/Flickr8k/annotations/Flickr8k.token.txt'\n",
    "train_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.trainImages.txt'\n",
    "dev_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.devImages.txt'\n",
    "test_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.testImages.txt'\n",
    "\n",
    "filenames_with_all_captions = flickr8k_parse.generate_filenames_with_all_captions(captions_file, images_path)\n",
    "\n",
    "train_filenames_with_all_captions = flickr8k_parse.generate_set(train_txt_path, filenames_with_all_captions, images_path)\n",
    "val_filenames_with_all_captions = flickr8k_parse.generate_set(dev_txt_path, filenames_with_all_captions, images_path)\n",
    "test_filenames_with_all_captions = flickr8k_parse.generate_set(test_txt_path, filenames_with_all_captions, images_path)\n",
    "\n",
    "train_captions = flickr8k_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "val_captions = flickr8k_parse.make_list_of_captions(val_filenames_with_all_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A black dog is running after a white dog in the snow .',\n",
       " 'Black dog chasing brown dog through snow',\n",
       " 'Two dogs chase each other across the snowy ground .',\n",
       " 'Two dogs play together in the snow .',\n",
       " 'Two dogs running through a low lying body of water .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the boy laying face down on a skateboard is being pushed along the ground by another boy .',\n",
       " 'Two girls play on a skateboard in a courtyard .',\n",
       " 'Two people play on a long skateboard .',\n",
       " 'Two small children in red shirts playing on a skateboard .',\n",
       " 'two young children on a skateboard going across a sidewalk']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess captions\n",
    "text_processing.preprocess_captions(val_captions)\n",
    "text_processing.preprocess_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the boy laying face down on a skateboard is being pushed along the ground by another boy ',\n",
       " 'two girls play on a skateboard in a courtyard ',\n",
       " 'two people play on a long skateboard ',\n",
       " 'two small children in red shirts playing on a skateboard ',\n",
       " 'two young children on a skateboard going across a sidewalk']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a black dog is running after a white dog in the snow ',\n",
       " 'black dog chasing brown dog through snow',\n",
       " 'two dogs chase each other across the snowy ground ',\n",
       " 'two dogs play together in the snow ',\n",
       " 'two dogs running through a low lying body of water ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add markers of captions' starts and ends\n",
    "text_processing.add_start_and_end_to_captions(train_captions)\n",
    "text_processing.add_start_and_end_to_captions(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black dog is running after a white dog in the snow <eos>',\n",
       " '<sos> black dog chasing brown dog through snow <eos>',\n",
       " '<sos> two dogs chase each other across the snowy ground <eos>',\n",
       " '<sos> two dogs play together in the snow <eos>',\n",
       " '<sos> two dogs running through a low lying body of water <eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> the boy laying face down on a skateboard is being pushed along the ground by another boy <eos>',\n",
       " '<sos> two girls play on a skateboard in a courtyard <eos>',\n",
       " '<sos> two people play on a long skateboard <eos>',\n",
       " '<sos> two small children in red shirts playing on a skateboard <eos>',\n",
       " '<sos> two young children on a skateboard going across a sidewalk <eos>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vocabulary from the training captions\n",
    "train_vocab = text_processing.Vocabulary()\n",
    "for caption_list in train_captions:\n",
    "    for caption in caption_list:\n",
    "        tmp_caption_list = caption.split()\n",
    "        for word in tmp_caption_list:\n",
    "            train_vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.save_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create transformed captions list - substitute words by their IDs from vocabulary\n",
    "def tokenise_captions(set_captions, vocabulary):\n",
    "    captions_tokens = [] \n",
    "    for captions in set_captions:\n",
    "        tmp_captions_for_img = []\n",
    "        for caption in captions:\n",
    "            caption_words = caption.split()\n",
    "            tmp = []\n",
    "            for word in caption_words:\n",
    "                if word in vocabulary.word_to_id:\n",
    "                    tmp.append(vocabulary.get_id_by_word(word))\n",
    "                else:\n",
    "                    tmp.append(0)\n",
    "            tmp_captions_for_img.append(tmp)\n",
    "        captions_tokens.append(tmp_captions_for_img)\n",
    "    return captions_tokens\n",
    "\n",
    "train_captions_tokens = tokenise_captions(train_captions, train_vocab)\n",
    "val_captions_tokens = tokenise_captions(val_captions, train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 2, 8, 4, 9, 10, 11, 12],\n",
       " [1, 3, 4, 13, 14, 4, 15, 11, 12],\n",
       " [1, 16, 17, 18, 19, 20, 21, 10, 22, 23, 12],\n",
       " [1, 16, 17, 24, 25, 9, 10, 11, 12],\n",
       " [1, 16, 17, 6, 15, 2, 26, 27, 28, 29, 30, 12]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  10,\n",
       "  50,\n",
       "  1325,\n",
       "  622,\n",
       "  94,\n",
       "  72,\n",
       "  2,\n",
       "  678,\n",
       "  5,\n",
       "  824,\n",
       "  2294,\n",
       "  67,\n",
       "  10,\n",
       "  23,\n",
       "  42,\n",
       "  265,\n",
       "  50,\n",
       "  12],\n",
       " [1, 16, 185, 24, 72, 2, 678, 9, 2, 3708, 12],\n",
       " [1, 16, 111, 24, 72, 2, 296, 678, 12],\n",
       " [1, 16, 280, 246, 9, 77, 1150, 40, 72, 2, 678, 12],\n",
       " [1, 16, 129, 246, 72, 2, 678, 852, 21, 2, 689, 12]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black dog is running after a white dog in the snow <eos>',\n",
       " '<sos> black dog chasing brown dog through snow <eos>',\n",
       " '<sos> two dogs chase each other across the snowy ground <eos>',\n",
       " '<sos> two dogs play together in the snow <eos>',\n",
       " '<sos> two dogs running through a low lying body of water <eos>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_one_hot_encode(batch, number_of_words):\n",
    "    \"\"\" \n",
    "    Applies one-hot encoding to the input batch\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = batch.shape[0]\n",
    "    sentence_size = batch.shape[1]\n",
    "    \n",
    "    one_hot_batch = np.zeros((batch_size, sentence_size, number_of_words))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(sentence_size):\n",
    "            one_hot_batch[i, j, batch[i, j]] = 1\n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(transfer_values, captions_tokens, number_of_words, gru=True, max_length_lstm=40, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate a batch of input-output data pairs:\n",
    "        input_data = {\n",
    "            transfer_values,\n",
    "            input_tokens\n",
    "        }\n",
    "        \n",
    "        output_data = {\n",
    "            output_tokens\n",
    "        }\n",
    "        \n",
    "     Parameters:\n",
    "        -----------\n",
    "        transfer_values: np.array\n",
    "            Encoded images features\n",
    "            \n",
    "        captions: list\n",
    "            list with all the captions\n",
    "        \n",
    "        \n",
    "        batch_size: int\n",
    "            The number of examples in a batch\n",
    "        -----------\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        number_of_images = len(captions_tokens)\n",
    "        \n",
    "        indices = np.random.randint(0, len(transfer_values), size=batch_size)\n",
    "\n",
    "        captions_batch = []\n",
    "        ### Randomly select one caption for each example index\n",
    "        for ind in indices:\n",
    "            num_captions = len(captions_tokens[ind])\n",
    "            selected_caption = captions_tokens[ind][np.random.randint(0, num_captions - 1)]\n",
    "            captions_batch.append(selected_caption)\n",
    "\n",
    "        if not gru:\n",
    "            captions_batch_padded = pad_sequences(captions_batch, \n",
    "                                              maxlen=max_length_lstm + 1, \n",
    "                                              padding='post', \n",
    "                                              value=0)\n",
    "        else:\n",
    "            ### Find the largest caption length and pad the remaining to be the same size\n",
    "            max_caption_size = max([len(cap) for cap in captions_batch])\n",
    "            captions_batch_padded = pad_sequences(captions_batch, \n",
    "                                              maxlen=max_caption_size, \n",
    "                                              padding='post', \n",
    "                                              value=0)\n",
    "        ### Input tokens are the initial ones starting from index 1\n",
    "        ### Output tokens are the initial ones shifted to the right\n",
    "        input_tokens = captions_batch_padded[:, :-1]\n",
    "        output_tokens = captions_batch_padded[:, 1:]\n",
    "\n",
    "        output_tokens = batch_one_hot_encode(output_tokens, number_of_words)\n",
    "\n",
    "        input_transfer_values = transfer_values[indices]\n",
    "\n",
    "        input_data = {\n",
    "            'encoder_input': input_transfer_values,\n",
    "            'decoder_input': input_tokens\n",
    "        }\n",
    "\n",
    "        output_data = {\n",
    "            'decoder_output': output_tokens\n",
    "        }\n",
    "\n",
    "        yield (input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_values = np.load('./cnn_features/vgg16_flickr8k_train.npy')\n",
    "val_transfer_values = np.load('./cnn_features/vgg16_flickr8k_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "steps_per_epoch = int(len(train_captions) / batch_size)\n",
    "initial_state_size = 512\n",
    "embedding_out_size = 512\n",
    "number_of_gru = 2\n",
    "batch_norm = False\n",
    "dropout = False\n",
    "gru = True\n",
    "max_len = 40\n",
    "path_checkpoint = './decoders/VGG16_GRU_flickr8k_2l_64b.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder input part\n",
    "encoder_input = Input(shape=(4096,), name='encoder_input')\n",
    "encoder_reduction = Dense(initial_state_size, activation='relu', name='encoder_reduction')\n",
    "if batch_norm:\n",
    "    bn1 = BatchNormalization()\n",
    "### For LSTM\n",
    "if not gru:\n",
    "    repeat = RepeatVector(max_len)\n",
    "### Decoder input and embedding\n",
    "if gru:\n",
    "    decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "else:\n",
    "    decoder_input = Input(shape=(40,), name='decoder_input')\n",
    "embedding = Embedding(input_dim=train_vocab.number_of_words, output_dim=embedding_out_size, name='embedding')\n",
    "if dropout:\n",
    "    drop1 = Dropout(0.5)\n",
    "### GRU1\n",
    "if gru:\n",
    "    gru1 = GRU(initial_state_size, name='GRU1', return_sequences=True)\n",
    "else:\n",
    "    lstm1 = LSTM(initial_state_size, name='LSTM1', return_sequences=True)\n",
    "if batch_norm:\n",
    "    bn2 = BatchNormalization()\n",
    "### GRU2    \n",
    "if number_of_gru >= 2:\n",
    "    if gru:\n",
    "        gru2 = GRU(initial_state_size, name='GRU2', return_sequences=True)\n",
    "    else:\n",
    "        lstm2 = LSTM(initial_state_size, name='LSTM2', return_sequences=True)\n",
    "    if batch_norm:\n",
    "        bn3 = BatchNormalization()\n",
    "### GRU3        \n",
    "if number_of_gru == 3:\n",
    "    if gru:\n",
    "        gru3 = GRU(initial_state_size, name='GRU3', return_sequences=True)\n",
    "    else:\n",
    "        lstm3 = LSTM(initial_state_size, name='LSTM3', return_sequences=True)\n",
    "    if batch_norm:\n",
    "        bn4 = BatchNormalization()\n",
    "\n",
    "decoder_dense = Dense(train_vocab.number_of_words, activation='softmax', name='decoder_output')\n",
    "\n",
    "def connect_transfer_values_gru(transfer_values):\n",
    "    \n",
    "    initial_state = encoder_reduction(transfer_values)\n",
    "    if batch_norm:\n",
    "        initial_state = bn1(initial_state)\n",
    "\n",
    "    X = decoder_input\n",
    "    X = embedding(X)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "    \n",
    "    X = gru1(X, initial_state=initial_state)\n",
    "    if batch_norm:\n",
    "        X = bn2(X)\n",
    "    if number_of_gru >= 2:\n",
    "        X = gru2(X, initial_state=initial_state)\n",
    "        if batch_norm:\n",
    "            X = bn3(X)\n",
    "    if number_of_gru == 3:\n",
    "        X = gru3(X, initial_state=initial_state)\n",
    "        if batch_norm:\n",
    "            X = bn4(X)\n",
    "\n",
    "    decoder_output = decoder_dense(X)\n",
    "    \n",
    "    return decoder_output\n",
    "\n",
    "def connect_transfer_values_lstm(transfer_values):\n",
    "    initial_state = encoder_reduction(transfer_values)\n",
    "    if batch_norm:\n",
    "        initial_state = bn1(initial_state)\n",
    "    initial_state = repeat(initial_state)\n",
    "    \n",
    "    X = decoder_input\n",
    "    X = embedding(X)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "        \n",
    "    X = concatenate([initial_state, X])\n",
    "    \n",
    "    X = lstm1(X)\n",
    "    if batch_norm:\n",
    "        X = bn2(X)\n",
    "    if number_of_gru >= 2:\n",
    "        X = lstm2(X)\n",
    "        if batch_norm:\n",
    "            X = bn3(X)\n",
    "    if number_of_gru == 3:\n",
    "        X = lstm3(X)\n",
    "        if batch_norm:\n",
    "            X = bn4(X)\n",
    "    \n",
    "    decoder_output = decoder_dense(X)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if gru:\n",
    "    decoder_output = connect_transfer_values_gru(transfer_values=encoder_input)\n",
    "else:\n",
    "    decoder_output = connect_transfer_values_lstm(transfer_values=encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    3774976     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_reduction (Dense)       (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "GRU1 (GRU)                      (None, None, 512)    1574400     embedding[0][0]                  \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "GRU2 (GRU)                      (None, None, 512)    1574400     GRU1[0][0]                       \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 7373)   3782349     GRU2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 12,803,789\n",
      "Trainable params: 12,803,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)\n",
    "val_generator = generate_batch(val_transfer_values, val_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> the two boys are wearing ice skates and hockey equipment <eos> <sos> a man looks at the city <eos> <sos> people camp with the mountains in the background <eos> <sos> basketball players try to block a ball from going into the goal <eos> <sos> a young girl in a blue and white cheerleading costume holds her right arm up while her other hand is on her hip <eos> <sos> a young boy makes a splash in the water near the rocks <eos> <sos> a little girl in a pink and white flowered dress and blue sweater swinging <eos> <sos> a lone swinger on a swing ride at the fair <eos> <sos> a boy holds a green apple in his mouth <eos> <sos> a boy jumps <eos> <sos> a wrestler is ready to jump on another wrestler outside the ring <eos> <sos> a baby hangs off an adult s back while laughing <eos> <sos> a woman wearing a headscarf is near many tulips <eos> <sos> a female with glasses a brown shirt and a backpack <eos> <sos> two brown dogs with blue collars are running in the grass <eos> <sos> four small dogs running through a green field with yellow flowers in the background <eos> <sos> men dressed in tutus perform a musical act onstage <eos> <sos> a man climbs up a cliff and reaches an overhang <eos> <sos> a woman standing in front of water and dry grass smiles <eos> <sos> a girl runs along a river bank with two black dogs <eos> <sos> two children smile for the camera <eos> <sos> a young man and an older woman sitting together in a restaurant <eos> <sos> a woman in blue jeans is standing in front of a group of children beside a stone built house <eos> <sos> a woman with a blue umbrella stands next to a parking meter <eos> <sos> a shirtless man rock climbs <eos> <sos> a woman teaches a dog to jump for treats outside on the grass <eos> <sos> man walking with red skateboard <eos> <sos> a warmly dressed boy pretending an icicle is a gun <eos> <sos> man falling off a tightrope <eos> <sos> a little boy in black slides headfirst down a tan tube with his mouth open wide <eos> <sos> a climber approaches the top of a cliff high above the surrounding terrain <eos> <sos> a couple of several people sitting on a ledge overlooking the beach <eos> <sos> a dog drinking from a creek <eos> <sos> a man in white a man in black with a red headscarf and a blond woman smoking <eos> <sos> a toddler in dirty jeans attampts to pull a younger child in a wagon <eos> <sos> girls in bright costumes holding little signs that say iove you <eos> <sos> a young boy jumps on the brown couch in his gray and blue spiderman outfit <eos> <sos> a girl sits outside at a large fountain <eos> <sos> a brown dog runs through a field <eos> <sos> person in yellow shorts grey shirt with logo jumping from bank into the water <eos> <sos> two men are about to enter an ice fishing tent on a snow covered lake <eos> <sos> a light brown dog is walking down a blue and yellow ramp <eos> <sos> men are standing beside a stonesign <eos> <sos> a man does a wheelie on his bicycle on the sidewalk <eos> <sos> a girl eating a peach <eos> <sos> a man posing in front of a crowd on a busy street <eos> <sos> there are three basketball players <eos> <sos> a boy in a camouflage coat is jumping onto a snowboard <eos> <sos> a little girl swings a woman stands behind her <eos> <sos> two children play outside in the yard <eos> <sos> two dogs are on the carpet in front of a cardboard box <eos> <sos> dog running though the snow in a green cover <eos> <sos> the boy is running through the sprinklers <eos> <sos> a young boy is licking colored frosting off a young man s face <eos> <sos> a dog jumps over another dog as both animals are trying to catch the same ball <eos> <sos> two woman smiling standing next to each other <eos> <sos> a boy and girl going down a metal slide <eos> <sos> a woman puts a harness on her small brown dog while in the snow <eos> <sos> three children are standing with baskets on numbered blocks <eos> <sos> a wild animal races across an uncut field with a minimal amount of trees <eos> <sos> a young man is pulling tricks on his bike in front of a warehouse <eos> <sos> two little girls are standing with bunch of stuffed animals on a white stone path <eos> <sos> a boy with a basketball <eos> <sos> a kid jumps in a puddle <eos>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([x for x in [train_vocab.get_word_by_id(word) for x in next(generator)[0]['decoder_input'] for word in x if word != 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "During the training process, it is a good idea to save the weights periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('./decoders/')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "\n",
    "checkpoints = ModelCheckpoint(path_checkpoint, verbose=1, save_weights_only=True, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, verbose=1, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     decoder_model.load_weights(path_checkpoint)\n",
    "# except:\n",
    "#     print(\"Error while loading weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "93/93 [==============================] - ETA: 7:24 - loss: 8.896 - ETA: 4:37 - loss: 8.536 - ETA: 3:42 - loss: 6.927 - ETA: 3:14 - loss: 5.857 - ETA: 2:56 - loss: 5.792 - ETA: 2:44 - loss: 5.181 - ETA: 2:35 - loss: 5.206 - ETA: 2:28 - loss: 4.837 - ETA: 2:22 - loss: 4.549 - ETA: 2:17 - loss: 4.299 - ETA: 2:13 - loss: 4.098 - ETA: 2:09 - loss: 3.907 - ETA: 2:05 - loss: 3.745 - ETA: 2:02 - loss: 3.611 - ETA: 2:00 - loss: 3.476 - ETA: 1:57 - loss: 3.355 - ETA: 1:54 - loss: 3.249 - ETA: 1:52 - loss: 3.162 - ETA: 1:50 - loss: 3.087 - ETA: 1:48 - loss: 3.017 - ETA: 1:46 - loss: 2.957 - ETA: 1:44 - loss: 2.898 - ETA: 1:42 - loss: 2.842 - ETA: 1:41 - loss: 2.790 - ETA: 1:39 - loss: 2.746 - ETA: 1:38 - loss: 2.708 - ETA: 1:36 - loss: 2.666 - ETA: 1:34 - loss: 2.627 - ETA: 1:33 - loss: 2.594 - ETA: 1:31 - loss: 2.562 - ETA: 1:30 - loss: 2.531 - ETA: 1:28 - loss: 2.507 - ETA: 1:27 - loss: 2.480 - ETA: 1:25 - loss: 2.452 - ETA: 1:24 - loss: 2.427 - ETA: 1:22 - loss: 2.401 - ETA: 1:21 - loss: 2.378 - ETA: 1:19 - loss: 2.356 - ETA: 1:18 - loss: 2.334 - ETA: 1:16 - loss: 2.315 - ETA: 1:15 - loss: 2.296 - ETA: 1:13 - loss: 2.279 - ETA: 1:12 - loss: 2.264 - ETA: 1:10 - loss: 2.248 - ETA: 1:09 - loss: 2.231 - ETA: 1:07 - loss: 2.212 - ETA: 1:06 - loss: 2.199 - ETA: 1:04 - loss: 2.186 - ETA: 1:03 - loss: 2.174 - ETA: 1:01 - loss: 2.162 - ETA: 1:00 - loss: 2.151 - ETA: 58s - loss: 2.137 - ETA: 57s - loss: 2.12 - ETA: 55s - loss: 2.11 - ETA: 54s - loss: 2.10 - ETA: 52s - loss: 2.09 - ETA: 51s - loss: 2.07 - ETA: 49s - loss: 2.06 - ETA: 48s - loss: 2.05 - ETA: 46s - loss: 2.05 - ETA: 45s - loss: 2.03 - ETA: 43s - loss: 2.03 - ETA: 42s - loss: 2.02 - ETA: 41s - loss: 2.01 - ETA: 39s - loss: 2.00 - ETA: 38s - loss: 1.99 - ETA: 37s - loss: 1.98 - ETA: 35s - loss: 1.98 - ETA: 34s - loss: 1.97 - ETA: 32s - loss: 1.96 - ETA: 31s - loss: 1.95 - ETA: 30s - loss: 1.95 - ETA: 28s - loss: 1.94 - ETA: 27s - loss: 1.93 - ETA: 26s - loss: 1.93 - ETA: 24s - loss: 1.92 - ETA: 23s - loss: 1.91 - ETA: 21s - loss: 1.91 - ETA: 20s - loss: 1.90 - ETA: 18s - loss: 1.90 - ETA: 17s - loss: 1.89 - ETA: 16s - loss: 1.89 - ETA: 14s - loss: 1.88 - ETA: 13s - loss: 1.88 - ETA: 11s - loss: 1.87 - ETA: 10s - loss: 1.86 - ETA: 8s - loss: 1.8609 - ETA: 7s - loss: 1.855 - ETA: 5s - loss: 1.849 - ETA: 4s - loss: 1.844 - ETA: 2s - loss: 1.839 - ETA: 1s - loss: 1.835 - 139s 1s/step - loss: 1.8309 - val_loss: 1.4423\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.44233, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 2/20\n",
      "93/93 [==============================] - ETA: 2:10 - loss: 1.456 - ETA: 2:09 - loss: 1.449 - ETA: 2:08 - loss: 1.381 - ETA: 2:05 - loss: 1.369 - ETA: 2:03 - loss: 1.386 - ETA: 2:02 - loss: 1.368 - ETA: 2:00 - loss: 1.378 - ETA: 1:58 - loss: 1.369 - ETA: 1:55 - loss: 1.367 - ETA: 1:53 - loss: 1.371 - ETA: 1:51 - loss: 1.367 - ETA: 1:49 - loss: 1.365 - ETA: 1:48 - loss: 1.365 - ETA: 1:46 - loss: 1.355 - ETA: 1:44 - loss: 1.358 - ETA: 1:43 - loss: 1.356 - ETA: 1:41 - loss: 1.352 - ETA: 1:40 - loss: 1.351 - ETA: 1:39 - loss: 1.353 - ETA: 1:38 - loss: 1.349 - ETA: 1:37 - loss: 1.347 - ETA: 1:36 - loss: 1.340 - ETA: 1:35 - loss: 1.333 - ETA: 1:34 - loss: 1.332 - ETA: 1:33 - loss: 1.331 - ETA: 1:32 - loss: 1.333 - ETA: 1:31 - loss: 1.332 - ETA: 1:30 - loss: 1.330 - ETA: 1:28 - loss: 1.331 - ETA: 1:27 - loss: 1.330 - ETA: 1:26 - loss: 1.326 - ETA: 1:25 - loss: 1.325 - ETA: 1:24 - loss: 1.322 - ETA: 1:22 - loss: 1.322 - ETA: 1:21 - loss: 1.318 - ETA: 1:20 - loss: 1.317 - ETA: 1:18 - loss: 1.317 - ETA: 1:17 - loss: 1.314 - ETA: 1:16 - loss: 1.313 - ETA: 1:15 - loss: 1.307 - ETA: 1:13 - loss: 1.308 - ETA: 1:12 - loss: 1.309 - ETA: 1:11 - loss: 1.309 - ETA: 1:09 - loss: 1.312 - ETA: 1:08 - loss: 1.312 - ETA: 1:07 - loss: 1.309 - ETA: 1:05 - loss: 1.308 - ETA: 1:04 - loss: 1.307 - ETA: 1:02 - loss: 1.305 - ETA: 1:01 - loss: 1.303 - ETA: 1:00 - loss: 1.301 - ETA: 58s - loss: 1.298 - ETA: 57s - loss: 1.29 - ETA: 55s - loss: 1.29 - ETA: 54s - loss: 1.29 - ETA: 52s - loss: 1.29 - ETA: 51s - loss: 1.29 - ETA: 49s - loss: 1.29 - ETA: 48s - loss: 1.29 - ETA: 47s - loss: 1.29 - ETA: 45s - loss: 1.29 - ETA: 44s - loss: 1.29 - ETA: 42s - loss: 1.28 - ETA: 41s - loss: 1.28 - ETA: 39s - loss: 1.28 - ETA: 38s - loss: 1.28 - ETA: 37s - loss: 1.28 - ETA: 35s - loss: 1.28 - ETA: 34s - loss: 1.28 - ETA: 32s - loss: 1.28 - ETA: 31s - loss: 1.27 - ETA: 29s - loss: 1.27 - ETA: 28s - loss: 1.27 - ETA: 27s - loss: 1.27 - ETA: 25s - loss: 1.27 - ETA: 24s - loss: 1.27 - ETA: 22s - loss: 1.27 - ETA: 21s - loss: 1.27 - ETA: 19s - loss: 1.27 - ETA: 18s - loss: 1.27 - ETA: 17s - loss: 1.27 - ETA: 15s - loss: 1.27 - ETA: 14s - loss: 1.27 - ETA: 12s - loss: 1.26 - ETA: 11s - loss: 1.26 - ETA: 9s - loss: 1.2664 - ETA: 8s - loss: 1.267 - ETA: 7s - loss: 1.267 - ETA: 5s - loss: 1.266 - ETA: 4s - loss: 1.265 - ETA: 2s - loss: 1.264 - ETA: 1s - loss: 1.264 - 135s 1s/step - loss: 1.2628 - val_loss: 1.2075\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.44233 to 1.20751, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 3/20\n",
      "93/93 [==============================] - ETA: 2:08 - loss: 1.174 - ETA: 2:08 - loss: 1.156 - ETA: 2:07 - loss: 1.162 - ETA: 2:05 - loss: 1.132 - ETA: 2:03 - loss: 1.146 - ETA: 2:02 - loss: 1.143 - ETA: 2:00 - loss: 1.160 - ETA: 1:59 - loss: 1.151 - ETA: 1:58 - loss: 1.158 - ETA: 1:56 - loss: 1.156 - ETA: 1:55 - loss: 1.162 - ETA: 1:53 - loss: 1.168 - ETA: 1:52 - loss: 1.166 - ETA: 1:51 - loss: 1.173 - ETA: 1:49 - loss: 1.172 - ETA: 1:48 - loss: 1.168 - ETA: 1:46 - loss: 1.161 - ETA: 1:45 - loss: 1.161 - ETA: 1:44 - loss: 1.161 - ETA: 1:43 - loss: 1.160 - ETA: 1:41 - loss: 1.161 - ETA: 1:40 - loss: 1.156 - ETA: 1:38 - loss: 1.154 - ETA: 1:37 - loss: 1.156 - ETA: 1:35 - loss: 1.158 - ETA: 1:34 - loss: 1.156 - ETA: 1:33 - loss: 1.154 - ETA: 1:31 - loss: 1.151 - ETA: 1:30 - loss: 1.148 - ETA: 1:29 - loss: 1.144 - ETA: 1:27 - loss: 1.144 - ETA: 1:26 - loss: 1.143 - ETA: 1:25 - loss: 1.144 - ETA: 1:23 - loss: 1.142 - ETA: 1:22 - loss: 1.140 - ETA: 1:21 - loss: 1.142 - ETA: 1:19 - loss: 1.144 - ETA: 1:18 - loss: 1.142 - ETA: 1:16 - loss: 1.144 - ETA: 1:15 - loss: 1.144 - ETA: 1:13 - loss: 1.143 - ETA: 1:12 - loss: 1.141 - ETA: 1:10 - loss: 1.141 - ETA: 1:09 - loss: 1.140 - ETA: 1:07 - loss: 1.139 - ETA: 1:05 - loss: 1.138 - ETA: 1:04 - loss: 1.139 - ETA: 1:02 - loss: 1.138 - ETA: 1:01 - loss: 1.139 - ETA: 1:00 - loss: 1.138 - ETA: 58s - loss: 1.136 - ETA: 57s - loss: 1.13 - ETA: 55s - loss: 1.13 - ETA: 54s - loss: 1.13 - ETA: 53s - loss: 1.13 - ETA: 51s - loss: 1.13 - ETA: 50s - loss: 1.13 - ETA: 48s - loss: 1.13 - ETA: 47s - loss: 1.13 - ETA: 46s - loss: 1.13 - ETA: 44s - loss: 1.12 - ETA: 43s - loss: 1.12 - ETA: 41s - loss: 1.12 - ETA: 40s - loss: 1.12 - ETA: 38s - loss: 1.12 - ETA: 37s - loss: 1.12 - ETA: 36s - loss: 1.12 - ETA: 34s - loss: 1.12 - ETA: 33s - loss: 1.12 - ETA: 31s - loss: 1.12 - ETA: 30s - loss: 1.12 - ETA: 29s - loss: 1.12 - ETA: 27s - loss: 1.12 - ETA: 26s - loss: 1.12 - ETA: 24s - loss: 1.12 - ETA: 23s - loss: 1.12 - ETA: 22s - loss: 1.12 - ETA: 20s - loss: 1.12 - ETA: 19s - loss: 1.12 - ETA: 17s - loss: 1.12 - ETA: 16s - loss: 1.12 - ETA: 15s - loss: 1.12 - ETA: 13s - loss: 1.12 - ETA: 12s - loss: 1.12 - ETA: 10s - loss: 1.12 - ETA: 9s - loss: 1.1234 - ETA: 8s - loss: 1.123 - ETA: 6s - loss: 1.123 - ETA: 5s - loss: 1.125 - ETA: 4s - loss: 1.123 - ETA: 2s - loss: 1.123 - ETA: 1s - loss: 1.122 - 129s 1s/step - loss: 1.1213 - val_loss: 1.0772\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.20751 to 1.07715, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 1:58 - loss: 1.073 - ETA: 1:58 - loss: 1.028 - ETA: 1:57 - loss: 1.007 - ETA: 1:55 - loss: 1.023 - ETA: 1:54 - loss: 1.032 - ETA: 1:53 - loss: 1.046 - ETA: 1:51 - loss: 1.040 - ETA: 1:50 - loss: 1.053 - ETA: 1:49 - loss: 1.065 - ETA: 1:48 - loss: 1.067 - ETA: 1:46 - loss: 1.062 - ETA: 1:45 - loss: 1.057 - ETA: 1:44 - loss: 1.056 - ETA: 1:42 - loss: 1.058 - ETA: 1:41 - loss: 1.061 - ETA: 1:40 - loss: 1.064 - ETA: 1:38 - loss: 1.064 - ETA: 1:37 - loss: 1.065 - ETA: 1:36 - loss: 1.066 - ETA: 1:34 - loss: 1.067 - ETA: 1:33 - loss: 1.069 - ETA: 1:32 - loss: 1.069 - ETA: 1:31 - loss: 1.073 - ETA: 1:29 - loss: 1.074 - ETA: 1:28 - loss: 1.078 - ETA: 1:27 - loss: 1.078 - ETA: 1:25 - loss: 1.079 - ETA: 1:24 - loss: 1.078 - ETA: 1:23 - loss: 1.076 - ETA: 1:22 - loss: 1.076 - ETA: 1:20 - loss: 1.076 - ETA: 1:19 - loss: 1.078 - ETA: 1:18 - loss: 1.077 - ETA: 1:16 - loss: 1.078 - ETA: 1:15 - loss: 1.077 - ETA: 1:14 - loss: 1.076 - ETA: 1:12 - loss: 1.073 - ETA: 1:11 - loss: 1.071 - ETA: 1:10 - loss: 1.072 - ETA: 1:09 - loss: 1.071 - ETA: 1:07 - loss: 1.071 - ETA: 1:06 - loss: 1.071 - ETA: 1:05 - loss: 1.070 - ETA: 1:03 - loss: 1.071 - ETA: 1:02 - loss: 1.072 - ETA: 1:01 - loss: 1.069 - ETA: 1:00 - loss: 1.068 - ETA: 58s - loss: 1.069 - ETA: 57s - loss: 1.07 - ETA: 56s - loss: 1.07 - ETA: 55s - loss: 1.07 - ETA: 54s - loss: 1.07 - ETA: 52s - loss: 1.07 - ETA: 51s - loss: 1.07 - ETA: 50s - loss: 1.07 - ETA: 48s - loss: 1.07 - ETA: 47s - loss: 1.06 - ETA: 46s - loss: 1.06 - ETA: 44s - loss: 1.07 - ETA: 43s - loss: 1.07 - ETA: 42s - loss: 1.07 - ETA: 41s - loss: 1.07 - ETA: 39s - loss: 1.07 - ETA: 38s - loss: 1.07 - ETA: 37s - loss: 1.07 - ETA: 35s - loss: 1.07 - ETA: 34s - loss: 1.07 - ETA: 33s - loss: 1.07 - ETA: 31s - loss: 1.07 - ETA: 30s - loss: 1.07 - ETA: 29s - loss: 1.07 - ETA: 27s - loss: 1.07 - ETA: 26s - loss: 1.07 - ETA: 25s - loss: 1.07 - ETA: 23s - loss: 1.07 - ETA: 22s - loss: 1.07 - ETA: 21s - loss: 1.07 - ETA: 19s - loss: 1.07 - ETA: 18s - loss: 1.07 - ETA: 17s - loss: 1.07 - ETA: 15s - loss: 1.07 - ETA: 14s - loss: 1.07 - ETA: 13s - loss: 1.07 - ETA: 11s - loss: 1.07 - ETA: 10s - loss: 1.07 - ETA: 9s - loss: 1.0719 - ETA: 7s - loss: 1.071 - ETA: 6s - loss: 1.070 - ETA: 5s - loss: 1.071 - ETA: 3s - loss: 1.070 - ETA: 2s - loss: 1.070 - ETA: 1s - loss: 1.069 - 125s 1s/step - loss: 1.0686 - val_loss: 1.0890\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.07715\n",
      "Epoch 5/20\n",
      "93/93 [==============================] - ETA: 1:58 - loss: 0.934 - ETA: 1:57 - loss: 0.981 - ETA: 1:56 - loss: 1.006 - ETA: 1:54 - loss: 1.018 - ETA: 1:53 - loss: 1.021 - ETA: 1:52 - loss: 1.016 - ETA: 1:50 - loss: 1.021 - ETA: 1:49 - loss: 1.027 - ETA: 1:48 - loss: 1.027 - ETA: 1:46 - loss: 1.029 - ETA: 1:45 - loss: 1.027 - ETA: 1:44 - loss: 1.031 - ETA: 1:43 - loss: 1.042 - ETA: 1:41 - loss: 1.039 - ETA: 1:40 - loss: 1.039 - ETA: 1:39 - loss: 1.035 - ETA: 1:38 - loss: 1.027 - ETA: 1:36 - loss: 1.030 - ETA: 1:35 - loss: 1.031 - ETA: 1:34 - loss: 1.029 - ETA: 1:33 - loss: 1.030 - ETA: 1:31 - loss: 1.033 - ETA: 1:30 - loss: 1.034 - ETA: 1:29 - loss: 1.032 - ETA: 1:27 - loss: 1.029 - ETA: 1:26 - loss: 1.030 - ETA: 1:25 - loss: 1.030 - ETA: 1:24 - loss: 1.037 - ETA: 1:22 - loss: 1.042 - ETA: 1:21 - loss: 1.041 - ETA: 1:20 - loss: 1.044 - ETA: 1:18 - loss: 1.043 - ETA: 1:17 - loss: 1.045 - ETA: 1:16 - loss: 1.043 - ETA: 1:15 - loss: 1.045 - ETA: 1:13 - loss: 1.047 - ETA: 1:12 - loss: 1.045 - ETA: 1:11 - loss: 1.042 - ETA: 1:09 - loss: 1.042 - ETA: 1:08 - loss: 1.042 - ETA: 1:07 - loss: 1.042 - ETA: 1:06 - loss: 1.043 - ETA: 1:04 - loss: 1.042 - ETA: 1:03 - loss: 1.043 - ETA: 1:02 - loss: 1.043 - ETA: 1:00 - loss: 1.047 - ETA: 59s - loss: 1.045 - ETA: 58s - loss: 1.04 - ETA: 57s - loss: 1.04 - ETA: 55s - loss: 1.04 - ETA: 54s - loss: 1.04 - ETA: 53s - loss: 1.04 - ETA: 51s - loss: 1.04 - ETA: 50s - loss: 1.04 - ETA: 49s - loss: 1.04 - ETA: 47s - loss: 1.04 - ETA: 46s - loss: 1.04 - ETA: 45s - loss: 1.04 - ETA: 44s - loss: 1.04 - ETA: 42s - loss: 1.04 - ETA: 41s - loss: 1.04 - ETA: 40s - loss: 1.04 - ETA: 38s - loss: 1.04 - ETA: 37s - loss: 1.04 - ETA: 36s - loss: 1.04 - ETA: 35s - loss: 1.04 - ETA: 33s - loss: 1.03 - ETA: 32s - loss: 1.03 - ETA: 31s - loss: 1.03 - ETA: 29s - loss: 1.03 - ETA: 28s - loss: 1.03 - ETA: 27s - loss: 1.03 - ETA: 25s - loss: 1.03 - ETA: 24s - loss: 1.03 - ETA: 23s - loss: 1.03 - ETA: 22s - loss: 1.03 - ETA: 20s - loss: 1.03 - ETA: 19s - loss: 1.03 - ETA: 18s - loss: 1.03 - ETA: 16s - loss: 1.03 - ETA: 15s - loss: 1.03 - ETA: 14s - loss: 1.03 - ETA: 12s - loss: 1.03 - ETA: 11s - loss: 1.03 - ETA: 10s - loss: 1.03 - ETA: 9s - loss: 1.0317 - ETA: 7s - loss: 1.031 - ETA: 6s - loss: 1.031 - ETA: 5s - loss: 1.031 - ETA: 3s - loss: 1.030 - ETA: 2s - loss: 1.030 - ETA: 1s - loss: 1.030 - 123s 1s/step - loss: 1.0297 - val_loss: 1.0404\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.07715 to 1.04040, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 6/20\n",
      "93/93 [==============================] - ETA: 1:59 - loss: 1.067 - ETA: 1:57 - loss: 1.063 - ETA: 1:56 - loss: 1.078 - ETA: 1:55 - loss: 1.093 - ETA: 1:54 - loss: 1.087 - ETA: 1:53 - loss: 1.071 - ETA: 1:52 - loss: 1.060 - ETA: 1:50 - loss: 1.049 - ETA: 1:49 - loss: 1.035 - ETA: 1:48 - loss: 1.042 - ETA: 1:46 - loss: 1.037 - ETA: 1:45 - loss: 1.033 - ETA: 1:44 - loss: 1.035 - ETA: 1:42 - loss: 1.031 - ETA: 1:41 - loss: 1.029 - ETA: 1:40 - loss: 1.027 - ETA: 1:39 - loss: 1.031 - ETA: 1:37 - loss: 1.027 - ETA: 1:36 - loss: 1.025 - ETA: 1:35 - loss: 1.025 - ETA: 1:33 - loss: 1.024 - ETA: 1:32 - loss: 1.021 - ETA: 1:31 - loss: 1.019 - ETA: 1:29 - loss: 1.022 - ETA: 1:28 - loss: 1.019 - ETA: 1:27 - loss: 1.022 - ETA: 1:25 - loss: 1.022 - ETA: 1:24 - loss: 1.022 - ETA: 1:23 - loss: 1.023 - ETA: 1:22 - loss: 1.020 - ETA: 1:20 - loss: 1.021 - ETA: 1:19 - loss: 1.023 - ETA: 1:18 - loss: 1.022 - ETA: 1:16 - loss: 1.020 - ETA: 1:15 - loss: 1.020 - ETA: 1:14 - loss: 1.019 - ETA: 1:12 - loss: 1.020 - ETA: 1:11 - loss: 1.022 - ETA: 1:10 - loss: 1.021 - ETA: 1:09 - loss: 1.021 - ETA: 1:07 - loss: 1.021 - ETA: 1:06 - loss: 1.020 - ETA: 1:05 - loss: 1.020 - ETA: 1:03 - loss: 1.020 - ETA: 1:02 - loss: 1.020 - ETA: 1:01 - loss: 1.020 - ETA: 59s - loss: 1.019 - ETA: 58s - loss: 1.02 - ETA: 57s - loss: 1.01 - ETA: 56s - loss: 1.02 - ETA: 54s - loss: 1.02 - ETA: 53s - loss: 1.02 - ETA: 52s - loss: 1.02 - ETA: 50s - loss: 1.02 - ETA: 49s - loss: 1.02 - ETA: 48s - loss: 1.02 - ETA: 46s - loss: 1.02 - ETA: 45s - loss: 1.02 - ETA: 44s - loss: 1.02 - ETA: 42s - loss: 1.01 - ETA: 41s - loss: 1.01 - ETA: 40s - loss: 1.01 - ETA: 39s - loss: 1.01 - ETA: 37s - loss: 1.01 - ETA: 36s - loss: 1.01 - ETA: 35s - loss: 1.01 - ETA: 33s - loss: 1.01 - ETA: 32s - loss: 1.01 - ETA: 31s - loss: 1.01 - ETA: 29s - loss: 1.01 - ETA: 28s - loss: 1.01 - ETA: 27s - loss: 1.01 - ETA: 26s - loss: 1.01 - ETA: 24s - loss: 1.01 - ETA: 23s - loss: 1.01 - ETA: 22s - loss: 1.01 - ETA: 20s - loss: 1.00 - ETA: 19s - loss: 1.00 - ETA: 18s - loss: 1.00 - ETA: 16s - loss: 1.00 - ETA: 15s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 13s - loss: 1.00 - ETA: 11s - loss: 1.00 - ETA: 10s - loss: 1.00 - ETA: 9s - loss: 1.0045 - ETA: 7s - loss: 1.004 - ETA: 6s - loss: 1.004 - ETA: 5s - loss: 1.003 - ETA: 3s - loss: 1.003 - ETA: 2s - loss: 1.003 - ETA: 1s - loss: 1.002 - 124s 1s/step - loss: 1.0027 - val_loss: 1.0620\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.04040\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 2:00 - loss: 1.005 - ETA: 1:58 - loss: 0.976 - ETA: 1:57 - loss: 0.975 - ETA: 1:56 - loss: 1.008 - ETA: 1:55 - loss: 1.007 - ETA: 1:54 - loss: 1.008 - ETA: 1:52 - loss: 1.009 - ETA: 1:51 - loss: 1.004 - ETA: 1:50 - loss: 0.994 - ETA: 1:48 - loss: 0.990 - ETA: 1:47 - loss: 1.002 - ETA: 1:46 - loss: 1.004 - ETA: 1:45 - loss: 1.007 - ETA: 1:43 - loss: 1.008 - ETA: 1:42 - loss: 1.007 - ETA: 1:41 - loss: 1.013 - ETA: 1:39 - loss: 1.008 - ETA: 1:38 - loss: 1.009 - ETA: 1:37 - loss: 1.009 - ETA: 1:36 - loss: 1.006 - ETA: 1:34 - loss: 1.003 - ETA: 1:33 - loss: 1.001 - ETA: 1:32 - loss: 1.002 - ETA: 1:31 - loss: 1.000 - ETA: 1:29 - loss: 0.997 - ETA: 1:28 - loss: 0.995 - ETA: 1:27 - loss: 0.995 - ETA: 1:25 - loss: 0.993 - ETA: 1:24 - loss: 0.995 - ETA: 1:23 - loss: 0.993 - ETA: 1:21 - loss: 0.997 - ETA: 1:20 - loss: 0.995 - ETA: 1:19 - loss: 0.995 - ETA: 1:17 - loss: 0.994 - ETA: 1:16 - loss: 0.992 - ETA: 1:15 - loss: 0.990 - ETA: 1:13 - loss: 0.991 - ETA: 1:12 - loss: 0.991 - ETA: 1:11 - loss: 0.989 - ETA: 1:10 - loss: 0.991 - ETA: 1:08 - loss: 0.992 - ETA: 1:07 - loss: 0.990 - ETA: 1:06 - loss: 0.990 - ETA: 1:04 - loss: 0.990 - ETA: 1:03 - loss: 0.989 - ETA: 1:02 - loss: 0.989 - ETA: 1:00 - loss: 0.988 - ETA: 59s - loss: 0.986 - ETA: 58s - loss: 0.98 - ETA: 56s - loss: 0.98 - ETA: 55s - loss: 0.98 - ETA: 54s - loss: 0.98 - ETA: 52s - loss: 0.98 - ETA: 51s - loss: 0.98 - ETA: 50s - loss: 0.98 - ETA: 48s - loss: 0.98 - ETA: 47s - loss: 0.98 - ETA: 46s - loss: 0.98 - ETA: 44s - loss: 0.98 - ETA: 43s - loss: 0.98 - ETA: 42s - loss: 0.98 - ETA: 40s - loss: 0.98 - ETA: 39s - loss: 0.98 - ETA: 38s - loss: 0.98 - ETA: 36s - loss: 0.98 - ETA: 35s - loss: 0.98 - ETA: 34s - loss: 0.98 - ETA: 32s - loss: 0.98 - ETA: 31s - loss: 0.98 - ETA: 30s - loss: 0.98 - ETA: 29s - loss: 0.98 - ETA: 27s - loss: 0.98 - ETA: 26s - loss: 0.98 - ETA: 25s - loss: 0.98 - ETA: 23s - loss: 0.98 - ETA: 22s - loss: 0.98 - ETA: 21s - loss: 0.98 - ETA: 19s - loss: 0.98 - ETA: 18s - loss: 0.98 - ETA: 17s - loss: 0.98 - ETA: 15s - loss: 0.98 - ETA: 14s - loss: 0.97 - ETA: 13s - loss: 0.97 - ETA: 11s - loss: 0.97 - ETA: 10s - loss: 0.97 - ETA: 9s - loss: 0.9791 - ETA: 7s - loss: 0.978 - ETA: 6s - loss: 0.978 - ETA: 5s - loss: 0.979 - ETA: 3s - loss: 0.980 - ETA: 2s - loss: 0.980 - ETA: 1s - loss: 0.981 - 125s 1s/step - loss: 0.9809 - val_loss: 1.1496\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.04040\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/20\n",
      "93/93 [==============================] - ETA: 1:58 - loss: 0.981 - ETA: 1:58 - loss: 1.003 - ETA: 1:57 - loss: 0.993 - ETA: 1:55 - loss: 0.981 - ETA: 1:53 - loss: 0.976 - ETA: 1:52 - loss: 0.958 - ETA: 1:51 - loss: 0.959 - ETA: 1:49 - loss: 0.946 - ETA: 1:48 - loss: 0.956 - ETA: 1:47 - loss: 0.952 - ETA: 1:46 - loss: 0.944 - ETA: 1:45 - loss: 0.953 - ETA: 1:44 - loss: 0.949 - ETA: 1:42 - loss: 0.954 - ETA: 1:41 - loss: 0.948 - ETA: 1:40 - loss: 0.947 - ETA: 1:38 - loss: 0.946 - ETA: 1:37 - loss: 0.943 - ETA: 1:36 - loss: 0.945 - ETA: 1:34 - loss: 0.947 - ETA: 1:33 - loss: 0.946 - ETA: 1:32 - loss: 0.946 - ETA: 1:31 - loss: 0.947 - ETA: 1:29 - loss: 0.947 - ETA: 1:28 - loss: 0.946 - ETA: 1:27 - loss: 0.943 - ETA: 1:25 - loss: 0.942 - ETA: 1:24 - loss: 0.943 - ETA: 1:23 - loss: 0.942 - ETA: 1:21 - loss: 0.942 - ETA: 1:20 - loss: 0.938 - ETA: 1:19 - loss: 0.939 - ETA: 1:17 - loss: 0.942 - ETA: 1:16 - loss: 0.941 - ETA: 1:15 - loss: 0.940 - ETA: 1:14 - loss: 0.941 - ETA: 1:12 - loss: 0.942 - ETA: 1:11 - loss: 0.941 - ETA: 1:10 - loss: 0.940 - ETA: 1:08 - loss: 0.941 - ETA: 1:07 - loss: 0.942 - ETA: 1:06 - loss: 0.941 - ETA: 1:04 - loss: 0.943 - ETA: 1:03 - loss: 0.943 - ETA: 1:02 - loss: 0.943 - ETA: 1:01 - loss: 0.941 - ETA: 59s - loss: 0.941 - ETA: 58s - loss: 0.94 - ETA: 57s - loss: 0.94 - ETA: 55s - loss: 0.94 - ETA: 54s - loss: 0.94 - ETA: 53s - loss: 0.93 - ETA: 52s - loss: 0.94 - ETA: 50s - loss: 0.94 - ETA: 49s - loss: 0.94 - ETA: 48s - loss: 0.94 - ETA: 46s - loss: 0.94 - ETA: 45s - loss: 0.94 - ETA: 44s - loss: 0.94 - ETA: 42s - loss: 0.94 - ETA: 41s - loss: 0.94 - ETA: 40s - loss: 0.94 - ETA: 39s - loss: 0.94 - ETA: 37s - loss: 0.94 - ETA: 36s - loss: 0.94 - ETA: 35s - loss: 0.94 - ETA: 33s - loss: 0.94 - ETA: 32s - loss: 0.94 - ETA: 31s - loss: 0.94 - ETA: 29s - loss: 0.94 - ETA: 28s - loss: 0.94 - ETA: 27s - loss: 0.94 - ETA: 26s - loss: 0.93 - ETA: 24s - loss: 0.93 - ETA: 23s - loss: 0.93 - ETA: 22s - loss: 0.93 - ETA: 20s - loss: 0.93 - ETA: 19s - loss: 0.93 - ETA: 18s - loss: 0.93 - ETA: 16s - loss: 0.93 - ETA: 15s - loss: 0.93 - ETA: 14s - loss: 0.93 - ETA: 13s - loss: 0.93 - ETA: 11s - loss: 0.93 - ETA: 10s - loss: 0.93 - ETA: 9s - loss: 0.9383 - ETA: 7s - loss: 0.937 - ETA: 6s - loss: 0.937 - ETA: 5s - loss: 0.938 - ETA: 3s - loss: 0.939 - ETA: 2s - loss: 0.940 - ETA: 1s - loss: 0.941 - 123s 1s/step - loss: 0.9413 - val_loss: 1.0616\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.04040\n",
      "Epoch 9/20\n",
      "93/93 [==============================] - ETA: 2:01 - loss: 0.920 - ETA: 1:59 - loss: 0.926 - ETA: 1:58 - loss: 0.944 - ETA: 1:57 - loss: 0.952 - ETA: 1:56 - loss: 0.949 - ETA: 1:55 - loss: 0.937 - ETA: 1:54 - loss: 0.919 - ETA: 1:52 - loss: 0.928 - ETA: 1:51 - loss: 0.920 - ETA: 1:49 - loss: 0.928 - ETA: 1:48 - loss: 0.934 - ETA: 1:46 - loss: 0.938 - ETA: 1:45 - loss: 0.940 - ETA: 1:44 - loss: 0.941 - ETA: 1:42 - loss: 0.939 - ETA: 1:41 - loss: 0.941 - ETA: 1:40 - loss: 0.936 - ETA: 1:38 - loss: 0.939 - ETA: 1:37 - loss: 0.939 - ETA: 1:36 - loss: 0.938 - ETA: 1:34 - loss: 0.935 - ETA: 1:33 - loss: 0.936 - ETA: 1:32 - loss: 0.935 - ETA: 1:30 - loss: 0.937 - ETA: 1:29 - loss: 0.940 - ETA: 1:28 - loss: 0.941 - ETA: 1:26 - loss: 0.940 - ETA: 1:25 - loss: 0.939 - ETA: 1:24 - loss: 0.939 - ETA: 1:22 - loss: 0.941 - ETA: 1:21 - loss: 0.940 - ETA: 1:20 - loss: 0.941 - ETA: 1:18 - loss: 0.939 - ETA: 1:17 - loss: 0.939 - ETA: 1:16 - loss: 0.940 - ETA: 1:14 - loss: 0.940 - ETA: 1:13 - loss: 0.940 - ETA: 1:12 - loss: 0.939 - ETA: 1:10 - loss: 0.939 - ETA: 1:09 - loss: 0.936 - ETA: 1:08 - loss: 0.937 - ETA: 1:06 - loss: 0.937 - ETA: 1:05 - loss: 0.936 - ETA: 1:04 - loss: 0.937 - ETA: 1:03 - loss: 0.937 - ETA: 1:01 - loss: 0.938 - ETA: 1:00 - loss: 0.937 - ETA: 59s - loss: 0.936 - ETA: 57s - loss: 0.93 - ETA: 56s - loss: 0.93 - ETA: 55s - loss: 0.93 - ETA: 53s - loss: 0.93 - ETA: 52s - loss: 0.93 - ETA: 51s - loss: 0.93 - ETA: 49s - loss: 0.93 - ETA: 48s - loss: 0.93 - ETA: 47s - loss: 0.93 - ETA: 45s - loss: 0.93 - ETA: 44s - loss: 0.93 - ETA: 43s - loss: 0.93 - ETA: 41s - loss: 0.93 - ETA: 40s - loss: 0.93 - ETA: 39s - loss: 0.93 - ETA: 38s - loss: 0.93 - ETA: 36s - loss: 0.93 - ETA: 35s - loss: 0.93 - ETA: 34s - loss: 0.93 - ETA: 32s - loss: 0.93 - ETA: 31s - loss: 0.93 - ETA: 30s - loss: 0.93 - ETA: 28s - loss: 0.93 - ETA: 27s - loss: 0.93 - ETA: 26s - loss: 0.93 - ETA: 24s - loss: 0.93 - ETA: 23s - loss: 0.93 - ETA: 22s - loss: 0.93 - ETA: 21s - loss: 0.93 - ETA: 19s - loss: 0.93 - ETA: 18s - loss: 0.93 - ETA: 17s - loss: 0.93 - ETA: 15s - loss: 0.93 - ETA: 14s - loss: 0.93 - ETA: 13s - loss: 0.93 - ETA: 11s - loss: 0.93 - ETA: 10s - loss: 0.93 - ETA: 9s - loss: 0.9329 - ETA: 7s - loss: 0.931 - ETA: 6s - loss: 0.930 - ETA: 5s - loss: 0.930 - ETA: 3s - loss: 0.930 - ETA: 2s - loss: 0.929 - ETA: 1s - loss: 0.930 - 124s 1s/step - loss: 0.9307 - val_loss: 1.0392\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.04040 to 1.03916, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 2:01 - loss: 0.920 - ETA: 1:59 - loss: 0.906 - ETA: 1:58 - loss: 0.885 - ETA: 1:56 - loss: 0.896 - ETA: 1:55 - loss: 0.897 - ETA: 1:54 - loss: 0.891 - ETA: 1:53 - loss: 0.898 - ETA: 1:51 - loss: 0.894 - ETA: 1:50 - loss: 0.893 - ETA: 1:49 - loss: 0.902 - ETA: 1:47 - loss: 0.906 - ETA: 1:46 - loss: 0.914 - ETA: 1:44 - loss: 0.910 - ETA: 1:43 - loss: 0.914 - ETA: 1:42 - loss: 0.913 - ETA: 1:41 - loss: 0.913 - ETA: 1:39 - loss: 0.909 - ETA: 1:38 - loss: 0.909 - ETA: 1:37 - loss: 0.911 - ETA: 1:35 - loss: 0.909 - ETA: 1:34 - loss: 0.908 - ETA: 1:33 - loss: 0.909 - ETA: 1:31 - loss: 0.911 - ETA: 1:30 - loss: 0.907 - ETA: 1:29 - loss: 0.909 - ETA: 1:28 - loss: 0.912 - ETA: 1:26 - loss: 0.915 - ETA: 1:25 - loss: 0.916 - ETA: 1:24 - loss: 0.916 - ETA: 1:22 - loss: 0.921 - ETA: 1:21 - loss: 0.922 - ETA: 1:20 - loss: 0.923 - ETA: 1:18 - loss: 0.923 - ETA: 1:17 - loss: 0.921 - ETA: 1:16 - loss: 0.921 - ETA: 1:14 - loss: 0.922 - ETA: 1:13 - loss: 0.922 - ETA: 1:12 - loss: 0.921 - ETA: 1:10 - loss: 0.922 - ETA: 1:09 - loss: 0.923 - ETA: 1:08 - loss: 0.925 - ETA: 1:06 - loss: 0.925 - ETA: 1:05 - loss: 0.927 - ETA: 1:04 - loss: 0.927 - ETA: 1:03 - loss: 0.926 - ETA: 1:01 - loss: 0.926 - ETA: 1:00 - loss: 0.924 - ETA: 59s - loss: 0.925 - ETA: 57s - loss: 0.92 - ETA: 56s - loss: 0.92 - ETA: 55s - loss: 0.92 - ETA: 53s - loss: 0.92 - ETA: 52s - loss: 0.92 - ETA: 51s - loss: 0.92 - ETA: 49s - loss: 0.92 - ETA: 48s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 45s - loss: 0.92 - ETA: 44s - loss: 0.92 - ETA: 43s - loss: 0.92 - ETA: 41s - loss: 0.92 - ETA: 40s - loss: 0.92 - ETA: 39s - loss: 0.92 - ETA: 37s - loss: 0.92 - ETA: 36s - loss: 0.92 - ETA: 35s - loss: 0.92 - ETA: 34s - loss: 0.92 - ETA: 32s - loss: 0.92 - ETA: 31s - loss: 0.92 - ETA: 30s - loss: 0.92 - ETA: 28s - loss: 0.92 - ETA: 27s - loss: 0.92 - ETA: 26s - loss: 0.92 - ETA: 24s - loss: 0.92 - ETA: 23s - loss: 0.92 - ETA: 22s - loss: 0.92 - ETA: 20s - loss: 0.92 - ETA: 19s - loss: 0.92 - ETA: 18s - loss: 0.92 - ETA: 17s - loss: 0.92 - ETA: 15s - loss: 0.92 - ETA: 14s - loss: 0.91 - ETA: 13s - loss: 0.91 - ETA: 11s - loss: 0.91 - ETA: 10s - loss: 0.91 - ETA: 9s - loss: 0.9190 - ETA: 7s - loss: 0.919 - ETA: 6s - loss: 0.920 - ETA: 5s - loss: 0.920 - ETA: 3s - loss: 0.920 - ETA: 2s - loss: 0.919 - ETA: 1s - loss: 0.919 - 124s 1s/step - loss: 0.9196 - val_loss: 1.0749\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.03916\n",
      "Epoch 11/20\n",
      "93/93 [==============================] - ETA: 2:03 - loss: 0.898 - ETA: 2:00 - loss: 0.979 - ETA: 1:59 - loss: 0.973 - ETA: 1:57 - loss: 0.960 - ETA: 1:55 - loss: 0.968 - ETA: 1:54 - loss: 0.964 - ETA: 1:52 - loss: 0.957 - ETA: 1:51 - loss: 0.954 - ETA: 1:49 - loss: 0.949 - ETA: 1:48 - loss: 0.947 - ETA: 1:47 - loss: 0.941 - ETA: 1:45 - loss: 0.938 - ETA: 1:44 - loss: 0.938 - ETA: 1:43 - loss: 0.937 - ETA: 1:42 - loss: 0.941 - ETA: 1:40 - loss: 0.940 - ETA: 1:39 - loss: 0.936 - ETA: 1:38 - loss: 0.932 - ETA: 1:36 - loss: 0.932 - ETA: 1:35 - loss: 0.929 - ETA: 1:34 - loss: 0.928 - ETA: 1:32 - loss: 0.928 - ETA: 1:31 - loss: 0.926 - ETA: 1:30 - loss: 0.924 - ETA: 1:28 - loss: 0.924 - ETA: 1:27 - loss: 0.921 - ETA: 1:26 - loss: 0.924 - ETA: 1:25 - loss: 0.923 - ETA: 1:23 - loss: 0.923 - ETA: 1:22 - loss: 0.922 - ETA: 1:21 - loss: 0.921 - ETA: 1:19 - loss: 0.921 - ETA: 1:18 - loss: 0.920 - ETA: 1:17 - loss: 0.920 - ETA: 1:15 - loss: 0.921 - ETA: 1:14 - loss: 0.921 - ETA: 1:13 - loss: 0.922 - ETA: 1:12 - loss: 0.921 - ETA: 1:10 - loss: 0.919 - ETA: 1:09 - loss: 0.920 - ETA: 1:08 - loss: 0.922 - ETA: 1:06 - loss: 0.921 - ETA: 1:05 - loss: 0.920 - ETA: 1:04 - loss: 0.922 - ETA: 1:02 - loss: 0.920 - ETA: 1:01 - loss: 0.922 - ETA: 1:00 - loss: 0.925 - ETA: 58s - loss: 0.924 - ETA: 57s - loss: 0.92 - ETA: 56s - loss: 0.92 - ETA: 55s - loss: 0.92 - ETA: 53s - loss: 0.92 - ETA: 52s - loss: 0.92 - ETA: 51s - loss: 0.92 - ETA: 49s - loss: 0.92 - ETA: 48s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 45s - loss: 0.92 - ETA: 44s - loss: 0.92 - ETA: 43s - loss: 0.92 - ETA: 41s - loss: 0.92 - ETA: 40s - loss: 0.92 - ETA: 39s - loss: 0.92 - ETA: 37s - loss: 0.92 - ETA: 36s - loss: 0.92 - ETA: 35s - loss: 0.92 - ETA: 34s - loss: 0.92 - ETA: 32s - loss: 0.92 - ETA: 31s - loss: 0.92 - ETA: 30s - loss: 0.92 - ETA: 28s - loss: 0.92 - ETA: 27s - loss: 0.92 - ETA: 26s - loss: 0.92 - ETA: 24s - loss: 0.92 - ETA: 23s - loss: 0.92 - ETA: 22s - loss: 0.92 - ETA: 20s - loss: 0.92 - ETA: 19s - loss: 0.92 - ETA: 18s - loss: 0.92 - ETA: 17s - loss: 0.92 - ETA: 15s - loss: 0.92 - ETA: 14s - loss: 0.92 - ETA: 13s - loss: 0.92 - ETA: 11s - loss: 0.92 - ETA: 10s - loss: 0.92 - ETA: 9s - loss: 0.9210 - ETA: 7s - loss: 0.920 - ETA: 6s - loss: 0.919 - ETA: 5s - loss: 0.919 - ETA: 3s - loss: 0.919 - ETA: 2s - loss: 0.919 - ETA: 1s - loss: 0.919 - 124s 1s/step - loss: 0.9192 - val_loss: 1.0814\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.03916\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/20\n",
      "93/93 [==============================] - ETA: 2:00 - loss: 0.804 - ETA: 1:59 - loss: 0.821 - ETA: 1:58 - loss: 0.857 - ETA: 1:56 - loss: 0.859 - ETA: 1:55 - loss: 0.876 - ETA: 1:53 - loss: 0.872 - ETA: 1:52 - loss: 0.886 - ETA: 1:51 - loss: 0.895 - ETA: 1:50 - loss: 0.892 - ETA: 1:48 - loss: 0.888 - ETA: 1:47 - loss: 0.897 - ETA: 1:46 - loss: 0.898 - ETA: 1:44 - loss: 0.898 - ETA: 1:43 - loss: 0.898 - ETA: 1:42 - loss: 0.897 - ETA: 1:40 - loss: 0.903 - ETA: 1:39 - loss: 0.898 - ETA: 1:38 - loss: 0.899 - ETA: 1:36 - loss: 0.897 - ETA: 1:35 - loss: 0.895 - ETA: 1:34 - loss: 0.893 - ETA: 1:33 - loss: 0.898 - ETA: 1:31 - loss: 0.900 - ETA: 1:30 - loss: 0.904 - ETA: 1:29 - loss: 0.905 - ETA: 1:27 - loss: 0.908 - ETA: 1:26 - loss: 0.908 - ETA: 1:25 - loss: 0.907 - ETA: 1:23 - loss: 0.904 - ETA: 1:22 - loss: 0.905 - ETA: 1:21 - loss: 0.905 - ETA: 1:19 - loss: 0.909 - ETA: 1:18 - loss: 0.910 - ETA: 1:17 - loss: 0.912 - ETA: 1:16 - loss: 0.910 - ETA: 1:14 - loss: 0.914 - ETA: 1:13 - loss: 0.915 - ETA: 1:12 - loss: 0.916 - ETA: 1:10 - loss: 0.917 - ETA: 1:09 - loss: 0.918 - ETA: 1:08 - loss: 0.916 - ETA: 1:06 - loss: 0.917 - ETA: 1:05 - loss: 0.918 - ETA: 1:04 - loss: 0.920 - ETA: 1:02 - loss: 0.921 - ETA: 1:01 - loss: 0.919 - ETA: 1:00 - loss: 0.919 - ETA: 59s - loss: 0.918 - ETA: 57s - loss: 0.91 - ETA: 56s - loss: 0.91 - ETA: 55s - loss: 0.91 - ETA: 53s - loss: 0.91 - ETA: 52s - loss: 0.91 - ETA: 51s - loss: 0.91 - ETA: 49s - loss: 0.91 - ETA: 48s - loss: 0.91 - ETA: 47s - loss: 0.91 - ETA: 45s - loss: 0.91 - ETA: 44s - loss: 0.91 - ETA: 43s - loss: 0.91 - ETA: 42s - loss: 0.91 - ETA: 40s - loss: 0.91 - ETA: 39s - loss: 0.91 - ETA: 38s - loss: 0.91 - ETA: 36s - loss: 0.91 - ETA: 35s - loss: 0.91 - ETA: 34s - loss: 0.91 - ETA: 32s - loss: 0.91 - ETA: 31s - loss: 0.91 - ETA: 30s - loss: 0.91 - ETA: 28s - loss: 0.91 - ETA: 27s - loss: 0.91 - ETA: 26s - loss: 0.91 - ETA: 24s - loss: 0.91 - ETA: 23s - loss: 0.91 - ETA: 22s - loss: 0.91 - ETA: 21s - loss: 0.91 - ETA: 19s - loss: 0.91 - ETA: 18s - loss: 0.91 - ETA: 17s - loss: 0.91 - ETA: 15s - loss: 0.91 - ETA: 14s - loss: 0.91 - ETA: 13s - loss: 0.91 - ETA: 11s - loss: 0.91 - ETA: 10s - loss: 0.91 - ETA: 9s - loss: 0.9114 - ETA: 7s - loss: 0.911 - ETA: 6s - loss: 0.911 - ETA: 5s - loss: 0.911 - ETA: 3s - loss: 0.910 - ETA: 2s - loss: 0.911 - ETA: 1s - loss: 0.910 - 125s 1s/step - loss: 0.9111 - val_loss: 1.0472\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.03916\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 1:59 - loss: 0.820 - ETA: 1:58 - loss: 0.819 - ETA: 1:57 - loss: 0.858 - ETA: 1:56 - loss: 0.893 - ETA: 1:55 - loss: 0.906 - ETA: 1:53 - loss: 0.906 - ETA: 1:52 - loss: 0.902 - ETA: 1:51 - loss: 0.904 - ETA: 1:49 - loss: 0.907 - ETA: 1:48 - loss: 0.904 - ETA: 1:47 - loss: 0.898 - ETA: 1:46 - loss: 0.891 - ETA: 1:44 - loss: 0.888 - ETA: 1:43 - loss: 0.894 - ETA: 1:42 - loss: 0.897 - ETA: 1:40 - loss: 0.895 - ETA: 1:39 - loss: 0.888 - ETA: 1:38 - loss: 0.888 - ETA: 1:37 - loss: 0.888 - ETA: 1:35 - loss: 0.888 - ETA: 1:34 - loss: 0.891 - ETA: 1:33 - loss: 0.893 - ETA: 1:31 - loss: 0.893 - ETA: 1:30 - loss: 0.890 - ETA: 1:29 - loss: 0.889 - ETA: 1:27 - loss: 0.890 - ETA: 1:26 - loss: 0.894 - ETA: 1:25 - loss: 0.899 - ETA: 1:24 - loss: 0.897 - ETA: 1:22 - loss: 0.897 - ETA: 1:21 - loss: 0.895 - ETA: 1:20 - loss: 0.895 - ETA: 1:18 - loss: 0.896 - ETA: 1:17 - loss: 0.896 - ETA: 1:16 - loss: 0.897 - ETA: 1:14 - loss: 0.897 - ETA: 1:13 - loss: 0.897 - ETA: 1:12 - loss: 0.896 - ETA: 1:10 - loss: 0.897 - ETA: 1:09 - loss: 0.897 - ETA: 1:08 - loss: 0.895 - ETA: 1:06 - loss: 0.897 - ETA: 1:05 - loss: 0.897 - ETA: 1:04 - loss: 0.897 - ETA: 1:02 - loss: 0.897 - ETA: 1:01 - loss: 0.896 - ETA: 1:00 - loss: 0.898 - ETA: 59s - loss: 0.896 - ETA: 57s - loss: 0.89 - ETA: 56s - loss: 0.89 - ETA: 55s - loss: 0.89 - ETA: 53s - loss: 0.89 - ETA: 52s - loss: 0.89 - ETA: 51s - loss: 0.89 - ETA: 49s - loss: 0.89 - ETA: 48s - loss: 0.89 - ETA: 47s - loss: 0.90 - ETA: 45s - loss: 0.89 - ETA: 44s - loss: 0.89 - ETA: 43s - loss: 0.89 - ETA: 41s - loss: 0.89 - ETA: 40s - loss: 0.89 - ETA: 39s - loss: 0.89 - ETA: 37s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 34s - loss: 0.89 - ETA: 32s - loss: 0.89 - ETA: 31s - loss: 0.89 - ETA: 30s - loss: 0.89 - ETA: 28s - loss: 0.89 - ETA: 27s - loss: 0.89 - ETA: 26s - loss: 0.89 - ETA: 24s - loss: 0.89 - ETA: 23s - loss: 0.89 - ETA: 22s - loss: 0.89 - ETA: 20s - loss: 0.89 - ETA: 19s - loss: 0.89 - ETA: 18s - loss: 0.89 - ETA: 17s - loss: 0.89 - ETA: 15s - loss: 0.89 - ETA: 14s - loss: 0.89 - ETA: 13s - loss: 0.89 - ETA: 11s - loss: 0.89 - ETA: 10s - loss: 0.89 - ETA: 9s - loss: 0.8980 - ETA: 7s - loss: 0.899 - ETA: 6s - loss: 0.899 - ETA: 5s - loss: 0.897 - ETA: 3s - loss: 0.898 - ETA: 2s - loss: 0.898 - ETA: 1s - loss: 0.898 - 124s 1s/step - loss: 0.8992 - val_loss: 1.0219\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.03916 to 1.02189, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 14/20\n",
      "93/93 [==============================] - ETA: 1:59 - loss: 0.871 - ETA: 1:59 - loss: 0.904 - ETA: 1:58 - loss: 0.899 - ETA: 1:56 - loss: 0.922 - ETA: 1:55 - loss: 0.931 - ETA: 1:53 - loss: 0.943 - ETA: 1:52 - loss: 0.950 - ETA: 1:50 - loss: 0.931 - ETA: 1:49 - loss: 0.935 - ETA: 1:48 - loss: 0.929 - ETA: 1:46 - loss: 0.930 - ETA: 1:45 - loss: 0.928 - ETA: 1:44 - loss: 0.919 - ETA: 1:42 - loss: 0.921 - ETA: 1:41 - loss: 0.921 - ETA: 1:40 - loss: 0.918 - ETA: 1:39 - loss: 0.912 - ETA: 1:37 - loss: 0.910 - ETA: 1:36 - loss: 0.906 - ETA: 1:35 - loss: 0.904 - ETA: 1:33 - loss: 0.907 - ETA: 1:32 - loss: 0.908 - ETA: 1:31 - loss: 0.910 - ETA: 1:29 - loss: 0.913 - ETA: 1:28 - loss: 0.910 - ETA: 1:27 - loss: 0.911 - ETA: 1:26 - loss: 0.913 - ETA: 1:24 - loss: 0.916 - ETA: 1:23 - loss: 0.914 - ETA: 1:22 - loss: 0.911 - ETA: 1:20 - loss: 0.909 - ETA: 1:19 - loss: 0.909 - ETA: 1:18 - loss: 0.906 - ETA: 1:16 - loss: 0.905 - ETA: 1:15 - loss: 0.905 - ETA: 1:14 - loss: 0.907 - ETA: 1:13 - loss: 0.907 - ETA: 1:11 - loss: 0.904 - ETA: 1:10 - loss: 0.905 - ETA: 1:09 - loss: 0.905 - ETA: 1:07 - loss: 0.904 - ETA: 1:06 - loss: 0.904 - ETA: 1:05 - loss: 0.904 - ETA: 1:03 - loss: 0.906 - ETA: 1:02 - loss: 0.903 - ETA: 1:01 - loss: 0.904 - ETA: 1:00 - loss: 0.903 - ETA: 58s - loss: 0.904 - ETA: 57s - loss: 0.90 - ETA: 56s - loss: 0.90 - ETA: 54s - loss: 0.90 - ETA: 53s - loss: 0.90 - ETA: 52s - loss: 0.90 - ETA: 50s - loss: 0.90 - ETA: 49s - loss: 0.90 - ETA: 48s - loss: 0.90 - ETA: 47s - loss: 0.90 - ETA: 45s - loss: 0.90 - ETA: 44s - loss: 0.90 - ETA: 43s - loss: 0.90 - ETA: 41s - loss: 0.90 - ETA: 40s - loss: 0.90 - ETA: 39s - loss: 0.89 - ETA: 37s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 34s - loss: 0.89 - ETA: 32s - loss: 0.89 - ETA: 31s - loss: 0.89 - ETA: 30s - loss: 0.89 - ETA: 28s - loss: 0.89 - ETA: 27s - loss: 0.89 - ETA: 26s - loss: 0.89 - ETA: 24s - loss: 0.89 - ETA: 23s - loss: 0.89 - ETA: 22s - loss: 0.89 - ETA: 20s - loss: 0.89 - ETA: 19s - loss: 0.89 - ETA: 18s - loss: 0.90 - ETA: 16s - loss: 0.90 - ETA: 15s - loss: 0.90 - ETA: 14s - loss: 0.90 - ETA: 13s - loss: 0.90 - ETA: 11s - loss: 0.90 - ETA: 10s - loss: 0.90 - ETA: 9s - loss: 0.9012 - ETA: 7s - loss: 0.900 - ETA: 6s - loss: 0.899 - ETA: 5s - loss: 0.899 - ETA: 3s - loss: 0.900 - ETA: 2s - loss: 0.900 - ETA: 1s - loss: 0.900 - 124s 1s/step - loss: 0.9012 - val_loss: 1.0026\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.02189 to 1.00262, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 15/20\n",
      "93/93 [==============================] - ETA: 2:00 - loss: 0.898 - ETA: 1:58 - loss: 0.889 - ETA: 1:57 - loss: 0.895 - ETA: 1:56 - loss: 0.886 - ETA: 1:55 - loss: 0.908 - ETA: 1:53 - loss: 0.906 - ETA: 1:52 - loss: 0.888 - ETA: 1:51 - loss: 0.884 - ETA: 1:49 - loss: 0.892 - ETA: 1:48 - loss: 0.893 - ETA: 1:47 - loss: 0.891 - ETA: 1:46 - loss: 0.891 - ETA: 1:44 - loss: 0.888 - ETA: 1:43 - loss: 0.889 - ETA: 1:42 - loss: 0.892 - ETA: 1:41 - loss: 0.889 - ETA: 1:39 - loss: 0.890 - ETA: 1:38 - loss: 0.892 - ETA: 1:37 - loss: 0.886 - ETA: 1:35 - loss: 0.883 - ETA: 1:34 - loss: 0.883 - ETA: 1:33 - loss: 0.880 - ETA: 1:31 - loss: 0.883 - ETA: 1:30 - loss: 0.885 - ETA: 1:29 - loss: 0.884 - ETA: 1:27 - loss: 0.882 - ETA: 1:26 - loss: 0.884 - ETA: 1:25 - loss: 0.883 - ETA: 1:23 - loss: 0.882 - ETA: 1:22 - loss: 0.886 - ETA: 1:21 - loss: 0.887 - ETA: 1:19 - loss: 0.887 - ETA: 1:18 - loss: 0.886 - ETA: 1:17 - loss: 0.885 - ETA: 1:16 - loss: 0.888 - ETA: 1:14 - loss: 0.890 - ETA: 1:13 - loss: 0.891 - ETA: 1:12 - loss: 0.892 - ETA: 1:10 - loss: 0.893 - ETA: 1:09 - loss: 0.892 - ETA: 1:08 - loss: 0.893 - ETA: 1:06 - loss: 0.893 - ETA: 1:05 - loss: 0.893 - ETA: 1:04 - loss: 0.894 - ETA: 1:02 - loss: 0.894 - ETA: 1:01 - loss: 0.894 - ETA: 1:00 - loss: 0.894 - ETA: 58s - loss: 0.894 - ETA: 57s - loss: 0.89 - ETA: 56s - loss: 0.89 - ETA: 55s - loss: 0.89 - ETA: 53s - loss: 0.89 - ETA: 52s - loss: 0.89 - ETA: 51s - loss: 0.89 - ETA: 49s - loss: 0.89 - ETA: 48s - loss: 0.89 - ETA: 47s - loss: 0.89 - ETA: 45s - loss: 0.89 - ETA: 44s - loss: 0.89 - ETA: 43s - loss: 0.89 - ETA: 41s - loss: 0.89 - ETA: 40s - loss: 0.89 - ETA: 39s - loss: 0.89 - ETA: 37s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 34s - loss: 0.89 - ETA: 32s - loss: 0.89 - ETA: 31s - loss: 0.89 - ETA: 30s - loss: 0.89 - ETA: 28s - loss: 0.89 - ETA: 27s - loss: 0.89 - ETA: 26s - loss: 0.89 - ETA: 24s - loss: 0.89 - ETA: 23s - loss: 0.89 - ETA: 22s - loss: 0.89 - ETA: 20s - loss: 0.89 - ETA: 19s - loss: 0.89 - ETA: 18s - loss: 0.88 - ETA: 17s - loss: 0.88 - ETA: 15s - loss: 0.88 - ETA: 14s - loss: 0.88 - ETA: 13s - loss: 0.88 - ETA: 11s - loss: 0.88 - ETA: 10s - loss: 0.88 - ETA: 9s - loss: 0.8878 - ETA: 7s - loss: 0.887 - ETA: 6s - loss: 0.887 - ETA: 5s - loss: 0.887 - ETA: 3s - loss: 0.887 - ETA: 2s - loss: 0.886 - ETA: 1s - loss: 0.887 - 124s 1s/step - loss: 0.8877 - val_loss: 1.0627\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.00262\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 2:00 - loss: 0.835 - ETA: 1:59 - loss: 0.813 - ETA: 1:57 - loss: 0.830 - ETA: 1:55 - loss: 0.851 - ETA: 1:54 - loss: 0.848 - ETA: 1:53 - loss: 0.844 - ETA: 1:52 - loss: 0.844 - ETA: 1:51 - loss: 0.848 - ETA: 1:50 - loss: 0.845 - ETA: 1:48 - loss: 0.851 - ETA: 1:47 - loss: 0.869 - ETA: 1:46 - loss: 0.876 - ETA: 1:44 - loss: 0.879 - ETA: 1:43 - loss: 0.878 - ETA: 1:42 - loss: 0.876 - ETA: 1:40 - loss: 0.884 - ETA: 1:39 - loss: 0.885 - ETA: 1:38 - loss: 0.881 - ETA: 1:36 - loss: 0.882 - ETA: 1:35 - loss: 0.887 - ETA: 1:34 - loss: 0.889 - ETA: 1:32 - loss: 0.886 - ETA: 1:31 - loss: 0.887 - ETA: 1:30 - loss: 0.886 - ETA: 1:28 - loss: 0.884 - ETA: 1:27 - loss: 0.882 - ETA: 1:26 - loss: 0.880 - ETA: 1:24 - loss: 0.878 - ETA: 1:23 - loss: 0.875 - ETA: 1:22 - loss: 0.875 - ETA: 1:20 - loss: 0.874 - ETA: 1:19 - loss: 0.873 - ETA: 1:18 - loss: 0.872 - ETA: 1:17 - loss: 0.871 - ETA: 1:15 - loss: 0.871 - ETA: 1:14 - loss: 0.873 - ETA: 1:13 - loss: 0.874 - ETA: 1:11 - loss: 0.875 - ETA: 1:10 - loss: 0.874 - ETA: 1:09 - loss: 0.874 - ETA: 1:07 - loss: 0.874 - ETA: 1:06 - loss: 0.875 - ETA: 1:05 - loss: 0.876 - ETA: 1:04 - loss: 0.876 - ETA: 1:02 - loss: 0.876 - ETA: 1:01 - loss: 0.875 - ETA: 1:00 - loss: 0.875 - ETA: 58s - loss: 0.876 - ETA: 57s - loss: 0.87 - ETA: 56s - loss: 0.87 - ETA: 54s - loss: 0.87 - ETA: 53s - loss: 0.87 - ETA: 52s - loss: 0.87 - ETA: 50s - loss: 0.87 - ETA: 49s - loss: 0.87 - ETA: 48s - loss: 0.87 - ETA: 47s - loss: 0.87 - ETA: 45s - loss: 0.87 - ETA: 44s - loss: 0.87 - ETA: 43s - loss: 0.87 - ETA: 41s - loss: 0.88 - ETA: 40s - loss: 0.88 - ETA: 39s - loss: 0.88 - ETA: 37s - loss: 0.88 - ETA: 36s - loss: 0.88 - ETA: 35s - loss: 0.88 - ETA: 33s - loss: 0.88 - ETA: 32s - loss: 0.88 - ETA: 31s - loss: 0.88 - ETA: 30s - loss: 0.88 - ETA: 28s - loss: 0.88 - ETA: 27s - loss: 0.88 - ETA: 26s - loss: 0.88 - ETA: 24s - loss: 0.88 - ETA: 23s - loss: 0.88 - ETA: 22s - loss: 0.88 - ETA: 20s - loss: 0.88 - ETA: 19s - loss: 0.88 - ETA: 18s - loss: 0.88 - ETA: 16s - loss: 0.88 - ETA: 15s - loss: 0.88 - ETA: 14s - loss: 0.88 - ETA: 13s - loss: 0.88 - ETA: 11s - loss: 0.88 - ETA: 10s - loss: 0.88 - ETA: 9s - loss: 0.8860 - ETA: 7s - loss: 0.886 - ETA: 6s - loss: 0.886 - ETA: 5s - loss: 0.886 - ETA: 3s - loss: 0.886 - ETA: 2s - loss: 0.886 - ETA: 1s - loss: 0.887 - 124s 1s/step - loss: 0.8878 - val_loss: 1.0966\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.00262\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 17/20\n",
      "93/93 [==============================] - ETA: 2:00 - loss: 0.923 - ETA: 1:58 - loss: 0.933 - ETA: 1:57 - loss: 0.917 - ETA: 1:56 - loss: 0.916 - ETA: 1:55 - loss: 0.912 - ETA: 1:53 - loss: 0.884 - ETA: 1:52 - loss: 0.880 - ETA: 1:51 - loss: 0.883 - ETA: 1:50 - loss: 0.877 - ETA: 1:48 - loss: 0.877 - ETA: 1:47 - loss: 0.877 - ETA: 1:46 - loss: 0.868 - ETA: 1:44 - loss: 0.871 - ETA: 1:43 - loss: 0.866 - ETA: 1:42 - loss: 0.867 - ETA: 1:40 - loss: 0.866 - ETA: 1:39 - loss: 0.871 - ETA: 1:38 - loss: 0.872 - ETA: 1:37 - loss: 0.870 - ETA: 1:35 - loss: 0.874 - ETA: 1:34 - loss: 0.878 - ETA: 1:33 - loss: 0.879 - ETA: 1:32 - loss: 0.879 - ETA: 1:30 - loss: 0.884 - ETA: 1:29 - loss: 0.887 - ETA: 1:28 - loss: 0.887 - ETA: 1:26 - loss: 0.886 - ETA: 1:25 - loss: 0.887 - ETA: 1:24 - loss: 0.886 - ETA: 1:22 - loss: 0.885 - ETA: 1:21 - loss: 0.884 - ETA: 1:20 - loss: 0.887 - ETA: 1:19 - loss: 0.887 - ETA: 1:17 - loss: 0.886 - ETA: 1:16 - loss: 0.884 - ETA: 1:15 - loss: 0.883 - ETA: 1:13 - loss: 0.884 - ETA: 1:12 - loss: 0.881 - ETA: 1:11 - loss: 0.882 - ETA: 1:09 - loss: 0.881 - ETA: 1:08 - loss: 0.882 - ETA: 1:07 - loss: 0.882 - ETA: 1:05 - loss: 0.882 - ETA: 1:04 - loss: 0.882 - ETA: 1:03 - loss: 0.883 - ETA: 1:01 - loss: 0.880 - ETA: 1:00 - loss: 0.880 - ETA: 59s - loss: 0.878 - ETA: 57s - loss: 0.87 - ETA: 56s - loss: 0.87 - ETA: 55s - loss: 0.87 - ETA: 53s - loss: 0.87 - ETA: 52s - loss: 0.87 - ETA: 51s - loss: 0.87 - ETA: 49s - loss: 0.87 - ETA: 48s - loss: 0.87 - ETA: 47s - loss: 0.87 - ETA: 45s - loss: 0.87 - ETA: 44s - loss: 0.87 - ETA: 43s - loss: 0.87 - ETA: 41s - loss: 0.88 - ETA: 40s - loss: 0.88 - ETA: 39s - loss: 0.88 - ETA: 38s - loss: 0.88 - ETA: 36s - loss: 0.88 - ETA: 35s - loss: 0.88 - ETA: 34s - loss: 0.88 - ETA: 32s - loss: 0.88 - ETA: 31s - loss: 0.88 - ETA: 30s - loss: 0.88 - ETA: 28s - loss: 0.87 - ETA: 27s - loss: 0.87 - ETA: 26s - loss: 0.87 - ETA: 24s - loss: 0.88 - ETA: 23s - loss: 0.87 - ETA: 22s - loss: 0.87 - ETA: 20s - loss: 0.87 - ETA: 19s - loss: 0.88 - ETA: 18s - loss: 0.88 - ETA: 17s - loss: 0.87 - ETA: 15s - loss: 0.88 - ETA: 14s - loss: 0.87 - ETA: 13s - loss: 0.88 - ETA: 11s - loss: 0.88 - ETA: 10s - loss: 0.88 - ETA: 9s - loss: 0.8808 - ETA: 7s - loss: 0.880 - ETA: 6s - loss: 0.879 - ETA: 5s - loss: 0.879 - ETA: 3s - loss: 0.879 - ETA: 2s - loss: 0.879 - ETA: 1s - loss: 0.879 - 124s 1s/step - loss: 0.8795 - val_loss: 0.9998\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.00262 to 0.99978, saving model to ./decoders/VGG16_GRU_flickr8k_2l_64b.hdf5\n",
      "Epoch 18/20\n",
      "93/93 [==============================] - ETA: 1:58 - loss: 0.879 - ETA: 1:58 - loss: 0.907 - ETA: 1:56 - loss: 0.896 - ETA: 1:55 - loss: 0.901 - ETA: 1:54 - loss: 0.893 - ETA: 1:53 - loss: 0.897 - ETA: 1:52 - loss: 0.898 - ETA: 1:50 - loss: 0.898 - ETA: 1:49 - loss: 0.893 - ETA: 1:48 - loss: 0.893 - ETA: 1:46 - loss: 0.888 - ETA: 1:45 - loss: 0.885 - ETA: 1:44 - loss: 0.887 - ETA: 1:43 - loss: 0.888 - ETA: 1:41 - loss: 0.890 - ETA: 1:40 - loss: 0.888 - ETA: 1:39 - loss: 0.883 - ETA: 1:37 - loss: 0.881 - ETA: 1:36 - loss: 0.883 - ETA: 1:35 - loss: 0.883 - ETA: 1:33 - loss: 0.884 - ETA: 1:32 - loss: 0.883 - ETA: 1:31 - loss: 0.880 - ETA: 1:29 - loss: 0.879 - ETA: 1:28 - loss: 0.882 - ETA: 1:27 - loss: 0.882 - ETA: 1:26 - loss: 0.883 - ETA: 1:24 - loss: 0.883 - ETA: 1:23 - loss: 0.881 - ETA: 1:22 - loss: 0.879 - ETA: 1:20 - loss: 0.876 - ETA: 1:19 - loss: 0.878 - ETA: 1:18 - loss: 0.879 - ETA: 1:16 - loss: 0.878 - ETA: 1:15 - loss: 0.880 - ETA: 1:14 - loss: 0.878 - ETA: 1:12 - loss: 0.877 - ETA: 1:11 - loss: 0.876 - ETA: 1:10 - loss: 0.877 - ETA: 1:09 - loss: 0.874 - ETA: 1:07 - loss: 0.875 - ETA: 1:06 - loss: 0.874 - ETA: 1:05 - loss: 0.875 - ETA: 1:03 - loss: 0.877 - ETA: 1:02 - loss: 0.878 - ETA: 1:01 - loss: 0.877 - ETA: 59s - loss: 0.878 - ETA: 58s - loss: 0.87 - ETA: 57s - loss: 0.87 - ETA: 56s - loss: 0.87 - ETA: 54s - loss: 0.87 - ETA: 53s - loss: 0.87 - ETA: 52s - loss: 0.87 - ETA: 50s - loss: 0.87 - ETA: 49s - loss: 0.87 - ETA: 48s - loss: 0.87 - ETA: 46s - loss: 0.87 - ETA: 45s - loss: 0.87 - ETA: 44s - loss: 0.87 - ETA: 43s - loss: 0.87 - ETA: 41s - loss: 0.87 - ETA: 40s - loss: 0.87 - ETA: 39s - loss: 0.87 - ETA: 37s - loss: 0.87 - ETA: 36s - loss: 0.87 - ETA: 35s - loss: 0.87 - ETA: 33s - loss: 0.87 - ETA: 32s - loss: 0.88 - ETA: 31s - loss: 0.87 - ETA: 29s - loss: 0.87 - ETA: 28s - loss: 0.87 - ETA: 27s - loss: 0.87 - ETA: 26s - loss: 0.87 - ETA: 24s - loss: 0.87 - ETA: 23s - loss: 0.87 - ETA: 22s - loss: 0.87 - ETA: 20s - loss: 0.88 - ETA: 19s - loss: 0.88 - ETA: 18s - loss: 0.88 - ETA: 16s - loss: 0.88 - ETA: 15s - loss: 0.88 - ETA: 14s - loss: 0.88 - ETA: 13s - loss: 0.88 - ETA: 11s - loss: 0.88 - ETA: 10s - loss: 0.87 - ETA: 9s - loss: 0.8798 - ETA: 7s - loss: 0.879 - ETA: 6s - loss: 0.879 - ETA: 5s - loss: 0.878 - ETA: 3s - loss: 0.878 - ETA: 2s - loss: 0.878 - ETA: 1s - loss: 0.878 - 124s 1s/step - loss: 0.8791 - val_loss: 1.0462\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.99978\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - ETA: 1:59 - loss: 0.890 - ETA: 1:59 - loss: 0.874 - ETA: 1:57 - loss: 0.862 - ETA: 1:55 - loss: 0.869 - ETA: 1:54 - loss: 0.858 - ETA: 1:53 - loss: 0.860 - ETA: 1:51 - loss: 0.854 - ETA: 1:50 - loss: 0.867 - ETA: 1:49 - loss: 0.873 - ETA: 1:48 - loss: 0.874 - ETA: 1:46 - loss: 0.880 - ETA: 1:45 - loss: 0.872 - ETA: 1:44 - loss: 0.870 - ETA: 1:42 - loss: 0.870 - ETA: 1:41 - loss: 0.864 - ETA: 1:40 - loss: 0.866 - ETA: 1:38 - loss: 0.864 - ETA: 1:37 - loss: 0.860 - ETA: 1:36 - loss: 0.858 - ETA: 1:34 - loss: 0.857 - ETA: 1:33 - loss: 0.859 - ETA: 1:32 - loss: 0.860 - ETA: 1:31 - loss: 0.859 - ETA: 1:29 - loss: 0.859 - ETA: 1:28 - loss: 0.858 - ETA: 1:27 - loss: 0.859 - ETA: 1:26 - loss: 0.860 - ETA: 1:24 - loss: 0.861 - ETA: 1:23 - loss: 0.861 - ETA: 1:22 - loss: 0.862 - ETA: 1:20 - loss: 0.861 - ETA: 1:19 - loss: 0.860 - ETA: 1:18 - loss: 0.859 - ETA: 1:16 - loss: 0.859 - ETA: 1:15 - loss: 0.859 - ETA: 1:14 - loss: 0.859 - ETA: 1:13 - loss: 0.859 - ETA: 1:11 - loss: 0.859 - ETA: 1:10 - loss: 0.861 - ETA: 1:09 - loss: 0.862 - ETA: 1:07 - loss: 0.862 - ETA: 1:06 - loss: 0.862 - ETA: 1:05 - loss: 0.861 - ETA: 1:03 - loss: 0.859 - ETA: 1:02 - loss: 0.860 - ETA: 1:01 - loss: 0.859 - ETA: 1:00 - loss: 0.860 - ETA: 58s - loss: 0.859 - ETA: 57s - loss: 0.85 - ETA: 56s - loss: 0.86 - ETA: 54s - loss: 0.86 - ETA: 53s - loss: 0.86 - ETA: 52s - loss: 0.86 - ETA: 50s - loss: 0.86 - ETA: 49s - loss: 0.86 - ETA: 48s - loss: 0.86 - ETA: 46s - loss: 0.86 - ETA: 45s - loss: 0.86 - ETA: 44s - loss: 0.86 - ETA: 43s - loss: 0.86 - ETA: 41s - loss: 0.86 - ETA: 40s - loss: 0.86 - ETA: 39s - loss: 0.86 - ETA: 37s - loss: 0.86 - ETA: 36s - loss: 0.86 - ETA: 35s - loss: 0.86 - ETA: 33s - loss: 0.86 - ETA: 32s - loss: 0.86 - ETA: 31s - loss: 0.86 - ETA: 29s - loss: 0.86 - ETA: 28s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 26s - loss: 0.86 - ETA: 24s - loss: 0.86 - ETA: 23s - loss: 0.86 - ETA: 22s - loss: 0.86 - ETA: 20s - loss: 0.86 - ETA: 19s - loss: 0.86 - ETA: 18s - loss: 0.86 - ETA: 16s - loss: 0.86 - ETA: 15s - loss: 0.86 - ETA: 14s - loss: 0.86 - ETA: 13s - loss: 0.86 - ETA: 11s - loss: 0.86 - ETA: 10s - loss: 0.86 - ETA: 9s - loss: 0.8694 - ETA: 7s - loss: 0.868 - ETA: 6s - loss: 0.869 - ETA: 5s - loss: 0.869 - ETA: 3s - loss: 0.869 - ETA: 2s - loss: 0.869 - ETA: 1s - loss: 0.869 - 124s 1s/step - loss: 0.8681 - val_loss: 1.0777\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.99978\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 20/20\n",
      "93/93 [==============================] - ETA: 2:00 - loss: 0.859 - ETA: 1:59 - loss: 0.843 - ETA: 1:57 - loss: 0.861 - ETA: 1:56 - loss: 0.878 - ETA: 1:54 - loss: 0.875 - ETA: 1:53 - loss: 0.876 - ETA: 1:52 - loss: 0.867 - ETA: 1:50 - loss: 0.868 - ETA: 1:49 - loss: 0.858 - ETA: 1:48 - loss: 0.859 - ETA: 1:46 - loss: 0.855 - ETA: 1:45 - loss: 0.857 - ETA: 1:44 - loss: 0.858 - ETA: 1:42 - loss: 0.858 - ETA: 1:41 - loss: 0.862 - ETA: 1:40 - loss: 0.864 - ETA: 1:39 - loss: 0.865 - ETA: 1:37 - loss: 0.864 - ETA: 1:36 - loss: 0.866 - ETA: 1:35 - loss: 0.867 - ETA: 1:34 - loss: 0.870 - ETA: 1:32 - loss: 0.872 - ETA: 1:31 - loss: 0.871 - ETA: 1:30 - loss: 0.874 - ETA: 1:28 - loss: 0.873 - ETA: 1:27 - loss: 0.874 - ETA: 1:26 - loss: 0.870 - ETA: 1:24 - loss: 0.868 - ETA: 1:23 - loss: 0.869 - ETA: 1:22 - loss: 0.867 - ETA: 1:20 - loss: 0.866 - ETA: 1:19 - loss: 0.867 - ETA: 1:18 - loss: 0.868 - ETA: 1:17 - loss: 0.867 - ETA: 1:15 - loss: 0.868 - ETA: 1:14 - loss: 0.868 - ETA: 1:13 - loss: 0.867 - ETA: 1:11 - loss: 0.863 - ETA: 1:10 - loss: 0.861 - ETA: 1:09 - loss: 0.860 - ETA: 1:07 - loss: 0.858 - ETA: 1:06 - loss: 0.857 - ETA: 1:05 - loss: 0.858 - ETA: 1:03 - loss: 0.857 - ETA: 1:02 - loss: 0.858 - ETA: 1:01 - loss: 0.859 - ETA: 1:00 - loss: 0.860 - ETA: 58s - loss: 0.861 - ETA: 57s - loss: 0.86 - ETA: 56s - loss: 0.86 - ETA: 54s - loss: 0.86 - ETA: 53s - loss: 0.86 - ETA: 52s - loss: 0.86 - ETA: 50s - loss: 0.86 - ETA: 49s - loss: 0.86 - ETA: 48s - loss: 0.86 - ETA: 46s - loss: 0.86 - ETA: 45s - loss: 0.86 - ETA: 44s - loss: 0.86 - ETA: 43s - loss: 0.86 - ETA: 41s - loss: 0.86 - ETA: 40s - loss: 0.86 - ETA: 39s - loss: 0.86 - ETA: 37s - loss: 0.86 - ETA: 36s - loss: 0.86 - ETA: 35s - loss: 0.86 - ETA: 33s - loss: 0.86 - ETA: 32s - loss: 0.86 - ETA: 31s - loss: 0.86 - ETA: 29s - loss: 0.86 - ETA: 28s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 26s - loss: 0.86 - ETA: 24s - loss: 0.87 - ETA: 23s - loss: 0.87 - ETA: 22s - loss: 0.87 - ETA: 20s - loss: 0.87 - ETA: 19s - loss: 0.87 - ETA: 18s - loss: 0.87 - ETA: 16s - loss: 0.87 - ETA: 15s - loss: 0.87 - ETA: 14s - loss: 0.87 - ETA: 13s - loss: 0.87 - ETA: 11s - loss: 0.87 - ETA: 10s - loss: 0.87 - ETA: 9s - loss: 0.8727 - ETA: 7s - loss: 0.872 - ETA: 6s - loss: 0.872 - ETA: 5s - loss: 0.872 - ETA: 3s - loss: 0.872 - ETA: 2s - loss: 0.872 - ETA: 1s - loss: 0.871 - 124s 1s/step - loss: 0.8722 - val_loss: 1.0082\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.99978\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = decoder_model.fit_generator(generator=generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=20,\n",
    "                            callbacks=[checkpoints, reduce_lr],\n",
    "                            validation_data=val_generator,\n",
    "                            validation_steps=5)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training: 2514.081176996231 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Time for training: {} seconds\".format(time_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'his' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5905b3b1b398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'his' is not defined"
     ]
    }
   ],
   "source": [
    "his"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
