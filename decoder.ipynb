{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datasets import flickr8k_parse\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, LSTM, add, Embedding, GRU, Dropout, Multiply, Dot, Lambda, BatchNormalization, \\\n",
    "    RepeatVector, concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import batch_generator\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import path_generation\n",
    "import tensorflow as tf\n",
    "import text_processing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 5614745094127694968, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 1462032793\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17093751452724026229\n",
       " physical_device_desc: \"device: 0, name: GeForce 840M, pci bus id: 0000:07:00.0, compute capability: 5.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building decoder, it is necessary to encode captions into one-hot vectors which further would be used in embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_path = 'D:/coco/annotations/'\n",
    "# images_path = 'D:/coco/images/'\n",
    "\n",
    "# # parse JSON file with captions to get paths to images with captions\n",
    "# val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=False)\n",
    "# val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "# train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=True)\n",
    "# train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)\n",
    "\n",
    "# ### Extract captions\n",
    "# train_captions = coco_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "# val_captions = coco_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'D:/Flickr8k/images/'\n",
    "annotations_path = 'D:/Flickr8k/annotations/'\n",
    "captions_file = 'D:/Flickr8k/annotations/Flickr8k.token.txt'\n",
    "train_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.trainImages.txt'\n",
    "dev_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.devImages.txt'\n",
    "test_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.testImages.txt'\n",
    "\n",
    "filenames_with_all_captions = flickr8k_parse.generate_filenames_with_all_captions(captions_file, images_path)\n",
    "\n",
    "train_filenames_with_all_captions = flickr8k_parse.generate_set(train_txt_path, filenames_with_all_captions, images_path)\n",
    "val_filenames_with_all_captions = flickr8k_parse.generate_set(dev_txt_path, filenames_with_all_captions, images_path)\n",
    "test_filenames_with_all_captions = flickr8k_parse.generate_set(test_txt_path, filenames_with_all_captions, images_path)\n",
    "\n",
    "train_captions = flickr8k_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "val_captions = flickr8k_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess captions\n",
    "text_processing.preprocess_captions(val_captions)\n",
    "text_processing.preprocess_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add markers of captions' starts and ends\n",
    "text_processing.add_start_and_end_to_captions(train_captions)\n",
    "text_processing.add_start_and_end_to_captions(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vocabulary from the training captions\n",
    "train_vocab = text_processing.Vocabulary()\n",
    "for caption_list in train_captions:\n",
    "    for caption in caption_list:\n",
    "        tmp_caption_list = caption.split()\n",
    "        for word in tmp_caption_list:\n",
    "            train_vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./vocabulary'):\n",
    "    os.mkdir('./vocabulary')\n",
    "train_vocab.save_vocabulary('word_to_id.pickle', 'id_to_word.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions_tokens = text_processing.tokenise_captions(train_captions, train_vocab)\n",
    "val_captions_tokens = text_processing.tokenise_captions(val_captions, train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 2, 8, 4, 9, 10, 11, 12],\n",
       " [1, 3, 4, 13, 14, 4, 15, 11, 12],\n",
       " [1, 16, 17, 18, 19, 20, 21, 10, 22, 23, 12],\n",
       " [1, 16, 17, 24, 25, 9, 10, 11, 12],\n",
       " [1, 16, 17, 6, 15, 2, 26, 27, 28, 29, 30, 12]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black dog is running after a white dog in the snow <eos>',\n",
       " '<sos> black dog chasing brown dog through snow <eos>',\n",
       " '<sos> two dogs chase each other across the snowy ground <eos>',\n",
       " '<sos> two dogs play together in the snow <eos>',\n",
       " '<sos> two dogs running through a low lying body of water <eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'flickr8k'\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "steps_per_epoch = int(len(train_captions) / batch_size)\n",
    "initial_state_size = 512\n",
    "embedding_out_size = 512\n",
    "number_of_layers = 2\n",
    "batch_norm = True\n",
    "dropout = True\n",
    "gru = False\n",
    "attn = True\n",
    "attn_type = 'bahdanau'\n",
    "max_len = 30\n",
    "path_gen = path_generation.PathGenerator(gru, dataset, number_of_layers, batch_size, batch_norm, dropout, attn, attn_type)\n",
    "path_checkpoint = path_gen.get_weights_path()\n",
    "model_path = path_gen.get_model_path()\n",
    "callbacks_path = path_gen.get_callbacks_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./callbacks/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.csv\n"
     ]
    }
   ],
   "source": [
    "print(callbacks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attn:\n",
    "    transfer_values = np.load('./cnn_features/vgg16_flickr8k_train_attn.npy')\n",
    "    val_transfer_values = np.load('./cnn_features/vgg16_flickr8k_val_attn.npy')\n",
    "else:\n",
    "    transfer_values = np.load('./cnn_features/vgg16_flickr8k_train.npy')\n",
    "    val_transfer_values = np.load('./cnn_features/vgg16_flickr8k_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 14, 14, 512)\n",
      "(6000, 196, 512)\n"
     ]
    }
   ],
   "source": [
    "if attn:\n",
    "    print(transfer_values.shape)\n",
    "    transfer_values = transfer_values.reshape(6000, -1, 512)\n",
    "    val_transfer_values = val_transfer_values.reshape(1000, -1, 512)\n",
    "    print(transfer_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the required layers as a global vatiables which further will be used in functions\n",
    "# Encoder input part\n",
    "encoder_input = Input(shape=(4096,), name='encoder_input')\n",
    "if attn:\n",
    "    encoder_input_attn = Input(shape=(transfer_values.shape[1],transfer_values.shape[2]), name='encoder_input')\n",
    "encoder_reduction = Dense(initial_state_size, activation='relu', name='encoder_reduction')\n",
    "bn1 = BatchNormalization()\n",
    "repeat = RepeatVector(max_len)\n",
    "\n",
    "### Decoder input and embedding\n",
    "decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "decoder_input_lstm = Input(shape=(max_len,), name='decoder_input')\n",
    "embedding = Embedding(input_dim=train_vocab.number_of_words, output_dim=embedding_out_size, mask_zero=True, name='embedding')\n",
    "drop1 = Dropout(0.5)\n",
    "### GRU1\n",
    "gru1 = GRU(initial_state_size, name='GRU1', return_sequences=True)\n",
    "s0 = Input(shape=(initial_state_size,), name='s0')\n",
    "c0 = Input(shape=(initial_state_size,), name='c0')\n",
    "lstm_att = LSTM(initial_state_size, return_state=True)\n",
    "lstm_att2 = LSTM(initial_state_size, return_sequences=True)\n",
    "lstm1 = LSTM(initial_state_size, name='LSTM1', return_sequences=True)\n",
    "bn2 = BatchNormalization()\n",
    "### GRU2    \n",
    "gru2 = GRU(initial_state_size, name='GRU2', return_sequences=True)\n",
    "lstm2 = LSTM(initial_state_size, name='LSTM2', return_sequences=True)\n",
    "bn3 = BatchNormalization()\n",
    "### GRU3        \n",
    "gru3 = GRU(initial_state_size, name='GRU3', return_sequences=True)\n",
    "lstm3 = LSTM(initial_state_size, name='LSTM3', return_sequences=True)\n",
    "bn4 = BatchNormalization()\n",
    "\n",
    "decoder_dense = Dense(train_vocab.number_of_words, activation='softmax', name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_transfer_values_gru(transfer_values):\n",
    "    \"\"\"\n",
    "    Connects extracted image features to sentences and passes to GRU.\n",
    "    Image features are the initial state of GRU while sentences are the first input words.\n",
    "    \"\"\"\n",
    "    ### process encoder values\n",
    "    initial_state = encoder_reduction(transfer_values)\n",
    "    if batch_norm:\n",
    "        initial_state = bn1(initial_state)\n",
    "    ### pass sentences to embedding\n",
    "    X = decoder_input\n",
    "    X = embedding(X)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "    ### RNN\n",
    "    X = gru1(X, initial_state=initial_state)\n",
    "    if batch_norm:\n",
    "        X = bn2(X)\n",
    "    if number_of_layers >= 2:\n",
    "        X = gru2(X, initial_state=initial_state)\n",
    "        if batch_norm:\n",
    "            X = bn3(X)\n",
    "    if number_of_layers == 3:\n",
    "        X = gru3(X, initial_state=initial_state)\n",
    "        if batch_norm:\n",
    "            X = bn4(X)\n",
    "    ### pass the outputs of RNNs to final dense layer which returns a one-hot vector for each word\n",
    "    decoder_output = decoder_dense(X)\n",
    "    return decoder_output\n",
    "\n",
    "def connect_transfer_values_lstm(transfer_values, max_len=40):\n",
    "    \"\"\"\n",
    "    Connects extracted image features to sentences and passes to LSTM.\n",
    "    Concatenated image features and sentences are LSTM inputs.\n",
    "    \"\"\"\n",
    "    features = encoder_reduction(transfer_values)\n",
    "    if batch_norm:\n",
    "        features = bn1(features)\n",
    "    features = repeat(features)\n",
    " \n",
    "    X = decoder_input_lstm\n",
    "    X = embedding(X)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "    \n",
    "    X = concatenate([features, X])\n",
    "    print(X.shape)\n",
    "    X = lstm1(X)\n",
    "    print(X.shape)\n",
    "\n",
    "    if batch_norm:\n",
    "        X = bn2(X)\n",
    "    if number_of_layers >= 2:\n",
    "        X = lstm2(X)\n",
    "        if batch_norm:\n",
    "            X = bn3(X)\n",
    "    if number_of_layers == 3:\n",
    "        X = lstm3(X)\n",
    "        if batch_norm:\n",
    "            X = bn4(X)\n",
    "\n",
    "    decoder_output = decoder_dense(X)\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "densor_s = Dense(initial_state_size)\n",
    "densor_feat = Dense(initial_state_size)\n",
    "gating_scalar_func = Dense(initial_state_size, activation='sigmoid')\n",
    "densor2 = Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bahdanau_attention(a, s_prev, i):\n",
    "    \"\"\"\n",
    "    Produces context vector for a given pair of image features and previous hidden state using Bahdanau additive attention\n",
    "    \"\"\"\n",
    "    print('------------------------')\n",
    "    print('Attention')\n",
    "    print('img features', a.shape)\n",
    "    print('prev state', s_prev.shape)\n",
    "    a_dense = densor_feat(a)\n",
    "    print('a_dense', a_dense.shape)\n",
    "    s_prev = Lambda(lambda x: K.expand_dims(x, 1))(s_prev)\n",
    "    s_dense = densor_s(s_prev)\n",
    "    print('s_dense', s_dense.shape)\n",
    "    sum_dense = add([a_dense, s_dense])\n",
    "    print('summary', sum_dense.shape)\n",
    "    concat = Lambda(lambda x: K.tanh(x))(sum_dense)\n",
    "    print('first_dense', concat.shape)\n",
    "    weights = densor2(concat)\n",
    "    weights = Lambda(lambda x: K.softmax(x, axis=1), name='weights_{}'.format(i))(weights)\n",
    "    print('weights', weights.shape)\n",
    "    context = Dot(axes=1)([weights, a])\n",
    "    gating_scalar = gating_scalar_func(s_prev)\n",
    "    context = Multiply()([context, gating_scalar])\n",
    "    print('context', context.shape)\n",
    "    print('------------------------')\n",
    "    return context\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(a, s_prev, i, initital_state_size):\n",
    "    \"\"\"\n",
    "    Produces context vector for a given pair of image features and previous hidden state using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    print('------------------------')\n",
    "    print('Attention')\n",
    "    print('img features', a.shape)\n",
    "    print('prev state', s_prev.shape)\n",
    "    s_prev = Lambda(lambda x: K.expand_dims(x, 1))(s_prev)\n",
    "    dot_prod = Dot(axes=2)([a, s_prev])\n",
    "    print('dot prod', dot_prod.shape)\n",
    "    scaled_dot_prod = Lambda(lambda x: x / np.sqrt(512))(dot_prod)\n",
    "    print('dot prod', dot_prod.shape)\n",
    "    weights = densor2(scaled_dot_prod)\n",
    "    weights = Lambda(lambda x: K.softmax(x, axis=1), name='weights_{}'.format(i))(weights)\n",
    "    print('weights', weights.shape)\n",
    "    context = Dot(axes=1)([weights, a])\n",
    "    print('context', context.shape)\n",
    "    print('------------------------')\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_transfer_values_lstm_attention(features, max_len=30, initial_state_size=512, attn='bahdanau'):\n",
    "    \"\"\"\n",
    "    Connects the transfer values to words and pass to LSTM with attention.\n",
    "    \"\"\"\n",
    "    print('Initial features shape', features.shape)\n",
    "    X = decoder_input_lstm\n",
    "    X = embedding(X)\n",
    "    print('word-embedding', X.shape)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "        \n",
    "    print('Initial states')\n",
    "    s0 = Lambda(lambda x: K.mean(x, axis=1))(features)\n",
    "    s0 = Dense(initial_state_size, activation='relu')(s0)\n",
    "    s0 = BatchNormalization()(s0)\n",
    "    s = s0\n",
    "    print('s initial', s.shape)\n",
    "    c0 = Lambda(lambda x: K.mean(x, axis=1))(features)\n",
    "    c0 = Dense(initial_state_size, activation='relu')(c0)\n",
    "    c0 = BatchNormalization()(c0)\n",
    "    c = c0\n",
    "    print('c initial', c.shape)\n",
    "    lstm_att_out = []\n",
    "    for i in range(max_len):\n",
    "        print('------------------------')\n",
    "        print('LSTM iteration {}'.format(i))\n",
    "        if attn == 'bahdanau':\n",
    "            context = bahdanau_attention(features, s, i)\n",
    "        elif attn == 'scaled_dot':\n",
    "            context = scaled_dot_product_attention(features, s, i, initial_state_size)\n",
    "        else:\n",
    "            raise ValueError('No such attention mechanism')\n",
    "        print('context', context.shape) \n",
    "        tmp_X = Lambda(lambda x, t: K.expand_dims(x[:, t], axis=1), arguments={'t': i}, output_shape=lambda s: (s[0], 1, s[2]))(X)\n",
    "        print('current word vector', tmp_X.shape)\n",
    "        concat = concatenate([context, tmp_X])\n",
    "        print('lstm input: context-word concat', concat.shape)\n",
    "        s, _, c = lstm_att(concat, initial_state=[s, c])\n",
    "        print('hidden state', s.shape)\n",
    "        lstm_att_out.append(s)\n",
    "    out = Lambda(lambda x: K.stack(x, axis=1))(lstm_att_out)\n",
    "    print('final lstm output shape', X.shape)\n",
    "    if batch_norm:\n",
    "        out = bn2(out)\n",
    "    \n",
    "    if number_of_layers == 2:\n",
    "        out = lstm_att2(out, initial_state=[s0, c0])\n",
    "    if batch_norm:\n",
    "        out = bn3(out)\n",
    "    decoder_output = decoder_dense(out)\n",
    "    print('output', decoder_output.shape)\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gru:\n",
    "    generator = batch_generator.generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size)\n",
    "    val_generator = batch_generator.generate_batch(val_transfer_values, val_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size)\n",
    "else:\n",
    "    generator = batch_generator.generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)\n",
    "    val_generator = batch_generator.generate_batch(val_transfer_values, val_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=1e-3, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features shape (?, 196, 512)\n",
      "word-embedding (?, 30, 512)\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Initial states\n",
      "s initial (?, 512)\n",
      "c initial (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 0\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 1\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 2\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 3\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 4\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 5\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 6\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 7\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 8\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 9\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 10\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 11\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 12\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 13\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 14\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 15\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 16\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 17\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 18\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 19\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 20\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 21\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 22\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 23\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 24\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 25\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 26\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 27\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 28\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "------------------------\n",
      "LSTM iteration 29\n",
      "------------------------\n",
      "Attention\n",
      "img features (?, 196, 512)\n",
      "prev state (?, 512)\n",
      "a_dense (?, 196, 512)\n",
      "s_dense (?, 1, 512)\n",
      "summary (?, 196, 512)\n",
      "first_dense (?, 196, 512)\n",
      "weights (?, 196, 1)\n",
      "context (?, 1, 512)\n",
      "------------------------\n",
      "context (?, 1, 512)\n",
      "current word vector (?, 1, 512)\n",
      "lstm input: context-word concat (?, 1, 1024)\n",
      "hidden state (?, 512)\n",
      "final lstm output shape (?, 30, 512)\n",
      "output (?, 30, 7373)\n"
     ]
    }
   ],
   "source": [
    "if gru:\n",
    "    decoder_output = connect_transfer_values_gru(encoder_input)\n",
    "else:\n",
    "    if attn:\n",
    "        decoder_output = connect_transfer_values_lstm_attention(encoder_input_attn, attn=attn_type)\n",
    "    else:\n",
    "        decoder_output = connect_transfer_values_lstm(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gru:\n",
    "    decoder_model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "else:\n",
    "    if attn:\n",
    "        decoder_model = Model(inputs=[encoder_input_attn, decoder_input_lstm], outputs=[decoder_output])\n",
    "    else:\n",
    "        decoder_model = Model(inputs=[encoder_input, decoder_input_lstm], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 196, 512)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512)          0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          262656      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 512)          2048        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 512)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 196, 512)     262656      encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 512)       262656      lambda_3[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 lambda_45[0][0]                  \n",
      "                                                                 lambda_48[0][0]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "                                                                 lambda_63[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "                                                                 lambda_69[0][0]                  \n",
      "                                                                 lambda_72[0][0]                  \n",
      "                                                                 lambda_75[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "                                                                 lambda_81[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "                                                                 lambda_87[0][0]                  \n",
      "                                                                 lambda_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 196, 512)     0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 196, 512)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 196, 1)       513         lambda_4[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "                                                                 lambda_40[0][0]                  \n",
      "                                                                 lambda_43[0][0]                  \n",
      "                                                                 lambda_46[0][0]                  \n",
      "                                                                 lambda_49[0][0]                  \n",
      "                                                                 lambda_52[0][0]                  \n",
      "                                                                 lambda_55[0][0]                  \n",
      "                                                                 lambda_58[0][0]                  \n",
      "                                                                 lambda_61[0][0]                  \n",
      "                                                                 lambda_64[0][0]                  \n",
      "                                                                 lambda_67[0][0]                  \n",
      "                                                                 lambda_70[0][0]                  \n",
      "                                                                 lambda_73[0][0]                  \n",
      "                                                                 lambda_76[0][0]                  \n",
      "                                                                 lambda_79[0][0]                  \n",
      "                                                                 lambda_82[0][0]                  \n",
      "                                                                 lambda_85[0][0]                  \n",
      "                                                                 lambda_88[0][0]                  \n",
      "                                                                 lambda_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "weights_0 (Lambda)              (None, 196, 1)       0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 512)      3774976     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 512)       0           weights_0[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 512)       262656      lambda_3[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 lambda_45[0][0]                  \n",
      "                                                                 lambda_48[0][0]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "                                                                 lambda_63[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "                                                                 lambda_69[0][0]                  \n",
      "                                                                 lambda_72[0][0]                  \n",
      "                                                                 lambda_75[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "                                                                 lambda_81[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "                                                                 lambda_87[0][0]                  \n",
      "                                                                 lambda_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 512)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 512)          0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1, 512)       0           dot_1[0][0]                      \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          262656      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1024)      0           multiply_1[0][0]                 \n",
      "                                                                 lambda_5[0][0]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 512)          2048        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 512), (None, 3147776     concatenate_1[0][0]              \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 concatenate_5[0][0]              \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 concatenate_7[0][0]              \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 concatenate_8[0][0]              \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 concatenate_11[0][0]             \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 concatenate_12[0][0]             \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 concatenate_13[0][0]             \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 concatenate_14[0][0]             \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 concatenate_15[0][0]             \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "                                                                 concatenate_16[0][0]             \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[14][2]                    \n",
      "                                                                 concatenate_17[0][0]             \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[15][2]                    \n",
      "                                                                 concatenate_18[0][0]             \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[16][2]                    \n",
      "                                                                 concatenate_19[0][0]             \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[17][2]                    \n",
      "                                                                 concatenate_20[0][0]             \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[18][2]                    \n",
      "                                                                 concatenate_21[0][0]             \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[19][2]                    \n",
      "                                                                 concatenate_22[0][0]             \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[20][2]                    \n",
      "                                                                 concatenate_23[0][0]             \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[21][2]                    \n",
      "                                                                 concatenate_24[0][0]             \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[22][2]                    \n",
      "                                                                 concatenate_25[0][0]             \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[23][2]                    \n",
      "                                                                 concatenate_26[0][0]             \n",
      "                                                                 lstm_1[24][0]                    \n",
      "                                                                 lstm_1[24][2]                    \n",
      "                                                                 concatenate_27[0][0]             \n",
      "                                                                 lstm_1[25][0]                    \n",
      "                                                                 lstm_1[25][2]                    \n",
      "                                                                 concatenate_28[0][0]             \n",
      "                                                                 lstm_1[26][0]                    \n",
      "                                                                 lstm_1[26][2]                    \n",
      "                                                                 concatenate_29[0][0]             \n",
      "                                                                 lstm_1[27][0]                    \n",
      "                                                                 lstm_1[27][2]                    \n",
      "                                                                 concatenate_30[0][0]             \n",
      "                                                                 lstm_1[28][0]                    \n",
      "                                                                 lstm_1[28][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1, 512)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 196, 512)     0           dense_2[1][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 196, 512)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_1 (Lambda)              (None, 196, 1)       0           dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 512)       0           weights_1[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1, 512)       0           dot_2[0][0]                      \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 1024)      0           multiply_2[0][0]                 \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1, 512)       0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 196, 512)     0           dense_2[2][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 196, 512)     0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_2 (Lambda)              (None, 196, 1)       0           dense_4[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 512)       0           weights_2[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 1, 512)       0           dot_3[0][0]                      \n",
      "                                                                 dense_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 1024)      0           multiply_3[0][0]                 \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 1, 512)       0           lstm_1[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 196, 512)     0           dense_2[3][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 196, 512)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_3 (Lambda)              (None, 196, 1)       0           dense_4[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1, 512)       0           weights_3[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 1, 512)       0           dot_4[0][0]                      \n",
      "                                                                 dense_3[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 1024)      0           multiply_4[0][0]                 \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 1, 512)       0           lstm_1[3][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 196, 512)     0           dense_2[4][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 196, 512)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_4 (Lambda)              (None, 196, 1)       0           dense_4[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1, 512)       0           weights_4[0][0]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 1, 512)       0           dot_5[0][0]                      \n",
      "                                                                 dense_3[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 1024)      0           multiply_5[0][0]                 \n",
      "                                                                 lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1, 512)       0           lstm_1[4][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 196, 512)     0           dense_2[5][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 196, 512)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_5 (Lambda)              (None, 196, 1)       0           dense_4[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1, 512)       0           weights_5[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 1, 512)       0           dot_6[0][0]                      \n",
      "                                                                 dense_3[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 1024)      0           multiply_6[0][0]                 \n",
      "                                                                 lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1, 512)       0           lstm_1[5][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 196, 512)     0           dense_2[6][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 196, 512)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_6 (Lambda)              (None, 196, 1)       0           dense_4[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_7 (Dot)                     (None, 1, 512)       0           weights_6[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 1, 512)       0           dot_7[0][0]                      \n",
      "                                                                 dense_3[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 1024)      0           multiply_7[0][0]                 \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1, 512)       0           lstm_1[6][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 196, 512)     0           dense_2[7][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 196, 512)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_7 (Lambda)              (None, 196, 1)       0           dense_4[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 1, 512)       0           weights_7[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 1, 512)       0           dot_8[0][0]                      \n",
      "                                                                 dense_3[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1, 1024)      0           multiply_8[0][0]                 \n",
      "                                                                 lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 1, 512)       0           lstm_1[7][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 196, 512)     0           dense_2[8][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 196, 512)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "weights_8 (Lambda)              (None, 196, 1)       0           dense_4[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 1, 512)       0           weights_8[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 1, 512)       0           dot_9[0][0]                      \n",
      "                                                                 dense_3[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1, 1024)      0           multiply_9[0][0]                 \n",
      "                                                                 lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 1, 512)       0           lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 196, 512)     0           dense_2[9][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 196, 512)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_9 (Lambda)              (None, 196, 1)       0           dense_4[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_10 (Dot)                    (None, 1, 512)       0           weights_9[0][0]                  \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 1, 512)       0           dot_10[0][0]                     \n",
      "                                                                 dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1, 1024)      0           multiply_10[0][0]                \n",
      "                                                                 lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 1, 512)       0           lstm_1[9][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 196, 512)     0           dense_2[10][0]                   \n",
      "                                                                 dense_1[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 196, 512)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_10 (Lambda)             (None, 196, 1)       0           dense_4[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_11 (Dot)                    (None, 1, 512)       0           weights_10[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 1, 512)       0           dot_11[0][0]                     \n",
      "                                                                 dense_3[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1, 1024)      0           multiply_11[0][0]                \n",
      "                                                                 lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 1, 512)       0           lstm_1[10][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 196, 512)     0           dense_2[11][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 196, 512)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_11 (Lambda)             (None, 196, 1)       0           dense_4[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_12 (Dot)                    (None, 1, 512)       0           weights_11[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 1, 512)       0           dot_12[0][0]                     \n",
      "                                                                 dense_3[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1, 1024)      0           multiply_12[0][0]                \n",
      "                                                                 lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 1, 512)       0           lstm_1[11][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 196, 512)     0           dense_2[12][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 196, 512)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_12 (Lambda)             (None, 196, 1)       0           dense_4[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_13 (Dot)                    (None, 1, 512)       0           weights_12[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 1, 512)       0           dot_13[0][0]                     \n",
      "                                                                 dense_3[12][0]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1, 1024)      0           multiply_13[0][0]                \n",
      "                                                                 lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 1, 512)       0           lstm_1[12][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 196, 512)     0           dense_2[13][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 196, 512)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_13 (Lambda)             (None, 196, 1)       0           dense_4[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_14 (Dot)                    (None, 1, 512)       0           weights_13[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 1, 512)       0           dot_14[0][0]                     \n",
      "                                                                 dense_3[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1, 1024)      0           multiply_14[0][0]                \n",
      "                                                                 lambda_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 1, 512)       0           lstm_1[13][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 196, 512)     0           dense_2[14][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 196, 512)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_14 (Lambda)             (None, 196, 1)       0           dense_4[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_15 (Dot)                    (None, 1, 512)       0           weights_14[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 1, 512)       0           dot_15[0][0]                     \n",
      "                                                                 dense_3[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1, 1024)      0           multiply_15[0][0]                \n",
      "                                                                 lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 1, 512)       0           lstm_1[14][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 196, 512)     0           dense_2[15][0]                   \n",
      "                                                                 dense_1[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 196, 512)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_15 (Lambda)             (None, 196, 1)       0           dense_4[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_16 (Dot)                    (None, 1, 512)       0           weights_15[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 1, 512)       0           dot_16[0][0]                     \n",
      "                                                                 dense_3[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1, 1024)      0           multiply_16[0][0]                \n",
      "                                                                 lambda_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 1, 512)       0           lstm_1[15][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 196, 512)     0           dense_2[16][0]                   \n",
      "                                                                 dense_1[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 196, 512)     0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_16 (Lambda)             (None, 196, 1)       0           dense_4[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_17 (Dot)                    (None, 1, 512)       0           weights_16[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 1, 512)       0           dot_17[0][0]                     \n",
      "                                                                 dense_3[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1, 1024)      0           multiply_17[0][0]                \n",
      "                                                                 lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 1, 512)       0           lstm_1[16][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 196, 512)     0           dense_2[17][0]                   \n",
      "                                                                 dense_1[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 196, 512)     0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_17 (Lambda)             (None, 196, 1)       0           dense_4[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_18 (Dot)                    (None, 1, 512)       0           weights_17[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 1, 512)       0           dot_18[0][0]                     \n",
      "                                                                 dense_3[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1, 1024)      0           multiply_18[0][0]                \n",
      "                                                                 lambda_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 1, 512)       0           lstm_1[17][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 196, 512)     0           dense_2[18][0]                   \n",
      "                                                                 dense_1[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 196, 512)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_18 (Lambda)             (None, 196, 1)       0           dense_4[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_19 (Dot)                    (None, 1, 512)       0           weights_18[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 1, 512)       0           dot_19[0][0]                     \n",
      "                                                                 dense_3[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 1, 1024)      0           multiply_19[0][0]                \n",
      "                                                                 lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 1, 512)       0           lstm_1[18][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 196, 512)     0           dense_2[19][0]                   \n",
      "                                                                 dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 196, 512)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_19 (Lambda)             (None, 196, 1)       0           dense_4[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_20 (Dot)                    (None, 1, 512)       0           weights_19[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 1, 512)       0           dot_20[0][0]                     \n",
      "                                                                 dense_3[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 1, 1024)      0           multiply_20[0][0]                \n",
      "                                                                 lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 1, 512)       0           lstm_1[19][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 196, 512)     0           dense_2[20][0]                   \n",
      "                                                                 dense_1[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 196, 512)     0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_20 (Lambda)             (None, 196, 1)       0           dense_4[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_21 (Dot)                    (None, 1, 512)       0           weights_20[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 1, 512)       0           dot_21[0][0]                     \n",
      "                                                                 dense_3[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 1, 1024)      0           multiply_21[0][0]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 1, 512)       0           lstm_1[20][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 196, 512)     0           dense_2[21][0]                   \n",
      "                                                                 dense_1[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 196, 512)     0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_21 (Lambda)             (None, 196, 1)       0           dense_4[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_22 (Dot)                    (None, 1, 512)       0           weights_21[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 1, 512)       0           dot_22[0][0]                     \n",
      "                                                                 dense_3[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 1, 1024)      0           multiply_22[0][0]                \n",
      "                                                                 lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1, 512)       0           lstm_1[21][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 196, 512)     0           dense_2[22][0]                   \n",
      "                                                                 dense_1[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 196, 512)     0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_22 (Lambda)             (None, 196, 1)       0           dense_4[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_23 (Dot)                    (None, 1, 512)       0           weights_22[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 1, 512)       0           dot_23[0][0]                     \n",
      "                                                                 dense_3[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 1, 1024)      0           multiply_23[0][0]                \n",
      "                                                                 lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 1, 512)       0           lstm_1[22][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 196, 512)     0           dense_2[23][0]                   \n",
      "                                                                 dense_1[23][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 196, 512)     0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_23 (Lambda)             (None, 196, 1)       0           dense_4[23][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_24 (Dot)                    (None, 1, 512)       0           weights_23[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 1, 512)       0           dot_24[0][0]                     \n",
      "                                                                 dense_3[23][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1, 1024)      0           multiply_24[0][0]                \n",
      "                                                                 lambda_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 1, 512)       0           lstm_1[23][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 196, 512)     0           dense_2[24][0]                   \n",
      "                                                                 dense_1[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 196, 512)     0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_24 (Lambda)             (None, 196, 1)       0           dense_4[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_25 (Dot)                    (None, 1, 512)       0           weights_24[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_25 (Multiply)          (None, 1, 512)       0           dot_25[0][0]                     \n",
      "                                                                 dense_3[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 1, 1024)      0           multiply_25[0][0]                \n",
      "                                                                 lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 1, 512)       0           lstm_1[24][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 196, 512)     0           dense_2[25][0]                   \n",
      "                                                                 dense_1[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 196, 512)     0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_25 (Lambda)             (None, 196, 1)       0           dense_4[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_26 (Dot)                    (None, 1, 512)       0           weights_25[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 1, 512)       0           dot_26[0][0]                     \n",
      "                                                                 dense_3[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 1, 1024)      0           multiply_26[0][0]                \n",
      "                                                                 lambda_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 1, 512)       0           lstm_1[25][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 196, 512)     0           dense_2[26][0]                   \n",
      "                                                                 dense_1[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 196, 512)     0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_26 (Lambda)             (None, 196, 1)       0           dense_4[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_27 (Dot)                    (None, 1, 512)       0           weights_26[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_27 (Multiply)          (None, 1, 512)       0           dot_27[0][0]                     \n",
      "                                                                 dense_3[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 1, 1024)      0           multiply_27[0][0]                \n",
      "                                                                 lambda_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 1, 512)       0           lstm_1[26][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 196, 512)     0           dense_2[27][0]                   \n",
      "                                                                 dense_1[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 196, 512)     0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_27 (Lambda)             (None, 196, 1)       0           dense_4[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_28 (Dot)                    (None, 1, 512)       0           weights_27[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_28 (Multiply)          (None, 1, 512)       0           dot_28[0][0]                     \n",
      "                                                                 dense_3[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 1, 1024)      0           multiply_28[0][0]                \n",
      "                                                                 lambda_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 1, 512)       0           lstm_1[27][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 196, 512)     0           dense_2[28][0]                   \n",
      "                                                                 dense_1[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 196, 512)     0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_28 (Lambda)             (None, 196, 1)       0           dense_4[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_29 (Dot)                    (None, 1, 512)       0           weights_28[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_29 (Multiply)          (None, 1, 512)       0           dot_29[0][0]                     \n",
      "                                                                 dense_3[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 1, 1024)      0           multiply_29[0][0]                \n",
      "                                                                 lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 1, 512)       0           lstm_1[28][0]                    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_30 (Add)                    (None, 196, 512)     0           dense_2[29][0]                   \n",
      "                                                                 dense_1[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 196, 512)     0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "weights_29 (Lambda)             (None, 196, 1)       0           dense_4[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_30 (Dot)                    (None, 1, 512)       0           weights_29[0][0]                 \n",
      "                                                                 encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_30 (Multiply)          (None, 1, 512)       0           dot_30[0][0]                     \n",
      "                                                                 dense_3[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 1, 512)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 1, 1024)      0           multiply_30[0][0]                \n",
      "                                                                 lambda_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 30, 512)      0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[24][0]                    \n",
      "                                                                 lstm_1[25][0]                    \n",
      "                                                                 lstm_1[26][0]                    \n",
      "                                                                 lstm_1[27][0]                    \n",
      "                                                                 lstm_1[28][0]                    \n",
      "                                                                 lstm_1[29][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30, 512)      2048        lambda_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 30, 512)      2099200     batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 30, 512)      2048        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, 30, 7373)     3782349     batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 14,126,286\n",
      "Trainable params: 14,122,190\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'batch_normalization_5/cond/Merge:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'batch_normalization_6/cond/Merge:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_1/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_1/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_2/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_2/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_3/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_3/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_4/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_4/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_5/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_5/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_6/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_6/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_7/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_7/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_8/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_8/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_9/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_9/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_10/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_10/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_11/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_11/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_12/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_12/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_13/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_13/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_14/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_14/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_15/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_15/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_16/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_16/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_17/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_17/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_18/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_18/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_19/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_19/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_20/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_20/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_21/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_21/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_22/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_22/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_23/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_23/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_24/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_24/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_25/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_25/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_26/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_26/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_27/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_27/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_28/TensorArrayReadV3:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1_28/while/Exit_4:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'batch_normalization_5/cond/Merge:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'batch_normalization_6/cond/Merge:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "model_json = decoder_model.to_json()\n",
    "try:\n",
    "    os.mkdir('./models')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json.dump(json.loads(model_json), json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "During the training process, it is a good idea to save the weights periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('./weights/')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "\n",
    "checkpoints = ModelCheckpoint(path_checkpoint, verbose=1, save_weights_only=True, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, verbose=1, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "187/187 [==============================] - ETA: 2:52:11 - loss: 8.99 - ETA: 1:28:05 - loss: 7.93 - ETA: 1:00:01 - loss: 7.10 - ETA: 45:58 - loss: 6.4499 - ETA: 37:32 - loss: 6.03 - ETA: 31:54 - loss: 5.82 - ETA: 27:53 - loss: 5.49 - ETA: 24:51 - loss: 5.21 - ETA: 22:29 - loss: 4.98 - ETA: 20:35 - loss: 4.79 - ETA: 19:02 - loss: 4.62 - ETA: 17:44 - loss: 4.45 - ETA: 16:38 - loss: 4.30 - ETA: 15:41 - loss: 4.17 - ETA: 14:51 - loss: 4.07 - ETA: 14:08 - loss: 3.95 - ETA: 13:29 - loss: 3.84 - ETA: 12:54 - loss: 3.75 - ETA: 12:23 - loss: 3.66 - ETA: 11:55 - loss: 3.59 - ETA: 11:29 - loss: 3.52 - ETA: 11:06 - loss: 3.45 - ETA: 10:45 - loss: 3.40 - ETA: 10:25 - loss: 3.33 - ETA: 10:06 - loss: 3.28 - ETA: 9:49 - loss: 3.2429 - ETA: 9:33 - loss: 3.201 - ETA: 9:18 - loss: 3.166 - ETA: 9:04 - loss: 3.128 - ETA: 8:51 - loss: 3.092 - ETA: 8:39 - loss: 3.058 - ETA: 8:27 - loss: 3.028 - ETA: 8:16 - loss: 3.007 - ETA: 8:05 - loss: 2.977 - ETA: 7:55 - loss: 2.954 - ETA: 7:46 - loss: 2.928 - ETA: 7:37 - loss: 2.906 - ETA: 7:28 - loss: 2.884 - ETA: 7:19 - loss: 2.863 - ETA: 7:11 - loss: 2.841 - ETA: 7:04 - loss: 2.817 - ETA: 6:56 - loss: 2.793 - ETA: 6:49 - loss: 2.774 - ETA: 6:42 - loss: 2.749 - ETA: 6:35 - loss: 2.729 - ETA: 6:29 - loss: 2.710 - ETA: 6:23 - loss: 2.692 - ETA: 6:17 - loss: 2.675 - ETA: 6:11 - loss: 2.657 - ETA: 6:05 - loss: 2.639 - ETA: 6:00 - loss: 2.622 - ETA: 5:54 - loss: 2.606 - ETA: 5:49 - loss: 2.589 - ETA: 5:44 - loss: 2.577 - ETA: 5:39 - loss: 2.564 - ETA: 5:34 - loss: 2.553 - ETA: 5:29 - loss: 2.542 - ETA: 5:25 - loss: 2.530 - ETA: 5:20 - loss: 2.516 - ETA: 5:15 - loss: 2.508 - ETA: 5:11 - loss: 2.496 - ETA: 5:07 - loss: 2.486 - ETA: 5:03 - loss: 2.475 - ETA: 4:59 - loss: 2.463 - ETA: 4:55 - loss: 2.455 - ETA: 4:51 - loss: 2.448 - ETA: 4:47 - loss: 2.438 - ETA: 4:43 - loss: 2.427 - ETA: 4:39 - loss: 2.417 - ETA: 4:36 - loss: 2.409 - ETA: 4:32 - loss: 2.401 - ETA: 4:28 - loss: 2.393 - ETA: 4:25 - loss: 2.382 - ETA: 4:21 - loss: 2.374 - ETA: 4:18 - loss: 2.364 - ETA: 4:15 - loss: 2.356 - ETA: 4:11 - loss: 2.345 - ETA: 4:08 - loss: 2.340 - ETA: 4:05 - loss: 2.332 - ETA: 4:02 - loss: 2.327 - ETA: 3:59 - loss: 2.319 - ETA: 3:55 - loss: 2.312 - ETA: 3:52 - loss: 2.302 - ETA: 3:49 - loss: 2.294 - ETA: 3:46 - loss: 2.288 - ETA: 3:43 - loss: 2.282 - ETA: 3:40 - loss: 2.276 - ETA: 3:37 - loss: 2.268 - ETA: 3:35 - loss: 2.262 - ETA: 3:32 - loss: 2.255 - ETA: 3:29 - loss: 2.250 - ETA: 3:26 - loss: 2.246 - ETA: 3:23 - loss: 2.239 - ETA: 3:21 - loss: 2.232 - ETA: 3:18 - loss: 2.228 - ETA: 3:15 - loss: 2.224 - ETA: 3:13 - loss: 2.219 - ETA: 3:10 - loss: 2.215 - ETA: 3:07 - loss: 2.209 - ETA: 3:05 - loss: 2.205 - ETA: 3:02 - loss: 2.201 - ETA: 3:00 - loss: 2.196 - ETA: 2:57 - loss: 2.191 - ETA: 2:54 - loss: 2.188 - ETA: 2:52 - loss: 2.184 - ETA: 2:49 - loss: 2.178 - ETA: 2:47 - loss: 2.173 - ETA: 2:45 - loss: 2.169 - ETA: 2:42 - loss: 2.166 - ETA: 2:40 - loss: 2.162 - ETA: 2:37 - loss: 2.158 - ETA: 2:35 - loss: 2.152 - ETA: 2:32 - loss: 2.149 - ETA: 2:30 - loss: 2.145 - ETA: 2:28 - loss: 2.141 - ETA: 2:25 - loss: 2.137 - ETA: 2:23 - loss: 2.133 - ETA: 2:21 - loss: 2.127 - ETA: 2:18 - loss: 2.121 - ETA: 2:16 - loss: 2.117 - ETA: 2:14 - loss: 2.112 - ETA: 2:12 - loss: 2.107 - ETA: 2:09 - loss: 2.103 - ETA: 2:07 - loss: 2.100 - ETA: 2:05 - loss: 2.096 - ETA: 2:03 - loss: 2.092 - ETA: 2:00 - loss: 2.089 - ETA: 1:58 - loss: 2.085 - ETA: 1:56 - loss: 2.083 - ETA: 1:54 - loss: 2.079 - ETA: 1:52 - loss: 2.075 - ETA: 1:49 - loss: 2.070 - ETA: 1:47 - loss: 2.065 - ETA: 1:45 - loss: 2.061 - ETA: 1:43 - loss: 2.058 - ETA: 1:41 - loss: 2.055 - ETA: 1:39 - loss: 2.052 - ETA: 1:37 - loss: 2.049 - ETA: 1:34 - loss: 2.049 - ETA: 1:32 - loss: 2.046 - ETA: 1:30 - loss: 2.042 - ETA: 1:28 - loss: 2.041 - ETA: 1:26 - loss: 2.039 - ETA: 1:24 - loss: 2.037 - ETA: 1:22 - loss: 2.032 - ETA: 1:20 - loss: 2.029 - ETA: 1:18 - loss: 2.028 - ETA: 1:16 - loss: 2.024 - ETA: 1:14 - loss: 2.021 - ETA: 1:12 - loss: 2.017 - ETA: 1:10 - loss: 2.014 - ETA: 1:08 - loss: 2.009 - ETA: 1:06 - loss: 2.007 - ETA: 1:04 - loss: 2.005 - ETA: 1:02 - loss: 2.003 - ETA: 59s - loss: 1.999 - ETA: 57s - loss: 1.99 - ETA: 55s - loss: 1.99 - ETA: 54s - loss: 1.99 - ETA: 52s - loss: 1.98 - ETA: 50s - loss: 1.98 - ETA: 48s - loss: 1.98 - ETA: 46s - loss: 1.98 - ETA: 44s - loss: 1.97 - ETA: 42s - loss: 1.97 - ETA: 40s - loss: 1.97 - ETA: 38s - loss: 1.97 - ETA: 36s - loss: 1.96 - ETA: 34s - loss: 1.96 - ETA: 32s - loss: 1.96 - ETA: 30s - loss: 1.96 - ETA: 28s - loss: 1.96 - ETA: 26s - loss: 1.95 - ETA: 24s - loss: 1.95 - ETA: 22s - loss: 1.95 - ETA: 20s - loss: 1.95 - ETA: 18s - loss: 1.94 - ETA: 17s - loss: 1.94 - ETA: 15s - loss: 1.94 - ETA: 13s - loss: 1.94 - ETA: 11s - loss: 1.93 - ETA: 9s - loss: 1.9361 - ETA: 7s - loss: 1.933 - ETA: 5s - loss: 1.931 - ETA: 3s - loss: 1.929 - ETA: 1s - loss: 1.927 - 356s 2s/step - loss: 1.9256 - val_loss: 11.5604\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 11.56038, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 2/20\n",
      "187/187 [==============================] - ETA: 4:56 - loss: 1.610 - ETA: 4:54 - loss: 1.553 - ETA: 4:53 - loss: 1.563 - ETA: 4:52 - loss: 1.566 - ETA: 4:50 - loss: 1.546 - ETA: 4:48 - loss: 1.537 - ETA: 4:46 - loss: 1.513 - ETA: 4:45 - loss: 1.514 - ETA: 4:43 - loss: 1.526 - ETA: 4:42 - loss: 1.529 - ETA: 4:40 - loss: 1.521 - ETA: 4:38 - loss: 1.534 - ETA: 4:37 - loss: 1.527 - ETA: 4:35 - loss: 1.515 - ETA: 4:34 - loss: 1.526 - ETA: 4:32 - loss: 1.527 - ETA: 4:30 - loss: 1.525 - ETA: 4:29 - loss: 1.525 - ETA: 4:27 - loss: 1.517 - ETA: 4:26 - loss: 1.514 - ETA: 4:24 - loss: 1.519 - ETA: 4:22 - loss: 1.517 - ETA: 4:21 - loss: 1.518 - ETA: 4:19 - loss: 1.513 - ETA: 4:18 - loss: 1.515 - ETA: 4:16 - loss: 1.510 - ETA: 4:14 - loss: 1.507 - ETA: 4:13 - loss: 1.510 - ETA: 4:11 - loss: 1.508 - ETA: 4:10 - loss: 1.512 - ETA: 4:08 - loss: 1.511 - ETA: 4:06 - loss: 1.512 - ETA: 4:05 - loss: 1.511 - ETA: 4:03 - loss: 1.514 - ETA: 4:02 - loss: 1.513 - ETA: 4:00 - loss: 1.513 - ETA: 3:58 - loss: 1.512 - ETA: 3:57 - loss: 1.511 - ETA: 3:55 - loss: 1.508 - ETA: 3:54 - loss: 1.507 - ETA: 3:52 - loss: 1.508 - ETA: 3:51 - loss: 1.506 - ETA: 3:49 - loss: 1.507 - ETA: 3:47 - loss: 1.506 - ETA: 3:46 - loss: 1.513 - ETA: 3:44 - loss: 1.513 - ETA: 3:43 - loss: 1.509 - ETA: 3:41 - loss: 1.506 - ETA: 3:39 - loss: 1.507 - ETA: 3:38 - loss: 1.506 - ETA: 3:36 - loss: 1.506 - ETA: 3:35 - loss: 1.506 - ETA: 3:33 - loss: 1.503 - ETA: 3:31 - loss: 1.503 - ETA: 3:30 - loss: 1.503 - ETA: 3:28 - loss: 1.501 - ETA: 3:27 - loss: 1.501 - ETA: 3:25 - loss: 1.500 - ETA: 3:23 - loss: 1.499 - ETA: 3:22 - loss: 1.494 - ETA: 3:20 - loss: 1.493 - ETA: 3:19 - loss: 1.489 - ETA: 3:17 - loss: 1.483 - ETA: 3:15 - loss: 1.481 - ETA: 3:14 - loss: 1.478 - ETA: 3:12 - loss: 1.479 - ETA: 3:11 - loss: 1.477 - ETA: 3:09 - loss: 1.476 - ETA: 3:08 - loss: 1.475 - ETA: 3:06 - loss: 1.476 - ETA: 3:04 - loss: 1.475 - ETA: 3:03 - loss: 1.476 - ETA: 3:01 - loss: 1.477 - ETA: 3:00 - loss: 1.477 - ETA: 2:58 - loss: 1.477 - ETA: 2:56 - loss: 1.477 - ETA: 2:55 - loss: 1.477 - ETA: 2:53 - loss: 1.476 - ETA: 2:52 - loss: 1.478 - ETA: 2:50 - loss: 1.478 - ETA: 2:48 - loss: 1.475 - ETA: 2:47 - loss: 1.474 - ETA: 2:45 - loss: 1.473 - ETA: 2:44 - loss: 1.473 - ETA: 2:42 - loss: 1.472 - ETA: 2:41 - loss: 1.474 - ETA: 2:39 - loss: 1.473 - ETA: 2:37 - loss: 1.474 - ETA: 2:36 - loss: 1.474 - ETA: 2:34 - loss: 1.477 - ETA: 2:33 - loss: 1.477 - ETA: 2:31 - loss: 1.477 - ETA: 2:29 - loss: 1.474 - ETA: 2:28 - loss: 1.471 - ETA: 2:26 - loss: 1.471 - ETA: 2:25 - loss: 1.469 - ETA: 2:23 - loss: 1.468 - ETA: 2:21 - loss: 1.467 - ETA: 2:20 - loss: 1.469 - ETA: 2:18 - loss: 1.469 - ETA: 2:17 - loss: 1.471 - ETA: 2:15 - loss: 1.470 - ETA: 2:13 - loss: 1.469 - ETA: 2:12 - loss: 1.469 - ETA: 2:10 - loss: 1.467 - ETA: 2:09 - loss: 1.468 - ETA: 2:07 - loss: 1.468 - ETA: 2:05 - loss: 1.467 - ETA: 2:04 - loss: 1.467 - ETA: 2:02 - loss: 1.467 - ETA: 2:01 - loss: 1.468 - ETA: 1:59 - loss: 1.468 - ETA: 1:57 - loss: 1.469 - ETA: 1:56 - loss: 1.468 - ETA: 1:54 - loss: 1.468 - ETA: 1:53 - loss: 1.468 - ETA: 1:51 - loss: 1.467 - ETA: 1:49 - loss: 1.467 - ETA: 1:48 - loss: 1.468 - ETA: 1:46 - loss: 1.468 - ETA: 1:45 - loss: 1.469 - ETA: 1:43 - loss: 1.468 - ETA: 1:42 - loss: 1.467 - ETA: 1:40 - loss: 1.464 - ETA: 1:38 - loss: 1.464 - ETA: 1:37 - loss: 1.463 - ETA: 1:35 - loss: 1.464 - ETA: 1:34 - loss: 1.463 - ETA: 1:32 - loss: 1.463 - ETA: 1:30 - loss: 1.463 - ETA: 1:29 - loss: 1.461 - ETA: 1:27 - loss: 1.459 - ETA: 1:26 - loss: 1.459 - ETA: 1:24 - loss: 1.458 - ETA: 1:22 - loss: 1.457 - ETA: 1:21 - loss: 1.456 - ETA: 1:19 - loss: 1.456 - ETA: 1:18 - loss: 1.456 - ETA: 1:16 - loss: 1.456 - ETA: 1:14 - loss: 1.456 - ETA: 1:13 - loss: 1.456 - ETA: 1:11 - loss: 1.454 - ETA: 1:10 - loss: 1.455 - ETA: 1:08 - loss: 1.454 - ETA: 1:06 - loss: 1.453 - ETA: 1:05 - loss: 1.453 - ETA: 1:03 - loss: 1.453 - ETA: 1:02 - loss: 1.450 - ETA: 1:00 - loss: 1.450 - ETA: 58s - loss: 1.449 - ETA: 57s - loss: 1.45 - ETA: 55s - loss: 1.45 - ETA: 54s - loss: 1.44 - ETA: 52s - loss: 1.44 - ETA: 51s - loss: 1.44 - ETA: 49s - loss: 1.44 - ETA: 47s - loss: 1.44 - ETA: 46s - loss: 1.44 - ETA: 44s - loss: 1.44 - ETA: 43s - loss: 1.44 - ETA: 41s - loss: 1.44 - ETA: 39s - loss: 1.44 - ETA: 38s - loss: 1.44 - ETA: 36s - loss: 1.44 - ETA: 35s - loss: 1.44 - ETA: 33s - loss: 1.44 - ETA: 31s - loss: 1.44 - ETA: 30s - loss: 1.44 - ETA: 28s - loss: 1.44 - ETA: 27s - loss: 1.44 - ETA: 25s - loss: 1.44 - ETA: 23s - loss: 1.44 - ETA: 22s - loss: 1.44 - ETA: 20s - loss: 1.44 - ETA: 19s - loss: 1.44 - ETA: 17s - loss: 1.44 - ETA: 15s - loss: 1.44 - ETA: 14s - loss: 1.44 - ETA: 12s - loss: 1.44 - ETA: 11s - loss: 1.44 - ETA: 9s - loss: 1.4436 - ETA: 7s - loss: 1.443 - ETA: 6s - loss: 1.443 - ETA: 4s - loss: 1.443 - ETA: 3s - loss: 1.443 - ETA: 1s - loss: 1.442 - 300s 2s/step - loss: 1.4410 - val_loss: 10.8031\n",
      "\n",
      "Epoch 00002: val_loss improved from 11.56038 to 10.80313, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:56 - loss: 1.342 - ETA: 4:54 - loss: 1.389 - ETA: 4:52 - loss: 1.329 - ETA: 4:51 - loss: 1.313 - ETA: 4:50 - loss: 1.282 - ETA: 4:48 - loss: 1.315 - ETA: 4:47 - loss: 1.313 - ETA: 4:45 - loss: 1.319 - ETA: 4:43 - loss: 1.313 - ETA: 4:41 - loss: 1.337 - ETA: 4:40 - loss: 1.341 - ETA: 4:38 - loss: 1.345 - ETA: 4:37 - loss: 1.348 - ETA: 4:35 - loss: 1.339 - ETA: 4:33 - loss: 1.346 - ETA: 4:32 - loss: 1.352 - ETA: 4:30 - loss: 1.352 - ETA: 4:29 - loss: 1.342 - ETA: 4:27 - loss: 1.352 - ETA: 4:25 - loss: 1.348 - ETA: 4:24 - loss: 1.354 - ETA: 4:22 - loss: 1.359 - ETA: 4:20 - loss: 1.374 - ETA: 4:19 - loss: 1.373 - ETA: 4:17 - loss: 1.370 - ETA: 4:16 - loss: 1.369 - ETA: 4:14 - loss: 1.364 - ETA: 4:13 - loss: 1.369 - ETA: 4:11 - loss: 1.367 - ETA: 4:10 - loss: 1.373 - ETA: 4:08 - loss: 1.370 - ETA: 4:06 - loss: 1.376 - ETA: 4:05 - loss: 1.374 - ETA: 4:03 - loss: 1.368 - ETA: 4:02 - loss: 1.370 - ETA: 4:00 - loss: 1.368 - ETA: 3:58 - loss: 1.364 - ETA: 3:57 - loss: 1.366 - ETA: 3:55 - loss: 1.364 - ETA: 3:54 - loss: 1.361 - ETA: 3:52 - loss: 1.360 - ETA: 3:50 - loss: 1.358 - ETA: 3:49 - loss: 1.365 - ETA: 3:47 - loss: 1.367 - ETA: 3:46 - loss: 1.370 - ETA: 3:44 - loss: 1.370 - ETA: 3:42 - loss: 1.371 - ETA: 3:41 - loss: 1.370 - ETA: 3:39 - loss: 1.368 - ETA: 3:38 - loss: 1.369 - ETA: 3:36 - loss: 1.368 - ETA: 3:35 - loss: 1.368 - ETA: 3:33 - loss: 1.367 - ETA: 3:31 - loss: 1.368 - ETA: 3:30 - loss: 1.366 - ETA: 3:28 - loss: 1.362 - ETA: 3:27 - loss: 1.366 - ETA: 3:25 - loss: 1.367 - ETA: 3:23 - loss: 1.365 - ETA: 3:22 - loss: 1.368 - ETA: 3:20 - loss: 1.371 - ETA: 3:19 - loss: 1.372 - ETA: 3:17 - loss: 1.370 - ETA: 3:15 - loss: 1.370 - ETA: 3:14 - loss: 1.370 - ETA: 3:12 - loss: 1.369 - ETA: 3:11 - loss: 1.368 - ETA: 3:09 - loss: 1.366 - ETA: 3:08 - loss: 1.364 - ETA: 3:06 - loss: 1.362 - ETA: 3:04 - loss: 1.363 - ETA: 3:03 - loss: 1.360 - ETA: 3:01 - loss: 1.359 - ETA: 3:00 - loss: 1.358 - ETA: 2:58 - loss: 1.358 - ETA: 2:56 - loss: 1.358 - ETA: 2:55 - loss: 1.357 - ETA: 2:53 - loss: 1.358 - ETA: 2:52 - loss: 1.356 - ETA: 2:50 - loss: 1.355 - ETA: 2:48 - loss: 1.353 - ETA: 2:47 - loss: 1.355 - ETA: 2:45 - loss: 1.357 - ETA: 2:44 - loss: 1.359 - ETA: 2:42 - loss: 1.359 - ETA: 2:41 - loss: 1.360 - ETA: 2:39 - loss: 1.360 - ETA: 2:37 - loss: 1.362 - ETA: 2:36 - loss: 1.362 - ETA: 2:34 - loss: 1.361 - ETA: 2:33 - loss: 1.360 - ETA: 2:31 - loss: 1.362 - ETA: 2:29 - loss: 1.363 - ETA: 2:28 - loss: 1.363 - ETA: 2:26 - loss: 1.362 - ETA: 2:25 - loss: 1.362 - ETA: 2:23 - loss: 1.362 - ETA: 2:21 - loss: 1.361 - ETA: 2:20 - loss: 1.360 - ETA: 2:18 - loss: 1.359 - ETA: 2:17 - loss: 1.358 - ETA: 2:15 - loss: 1.357 - ETA: 2:13 - loss: 1.357 - ETA: 2:12 - loss: 1.355 - ETA: 2:10 - loss: 1.355 - ETA: 2:09 - loss: 1.356 - ETA: 2:07 - loss: 1.357 - ETA: 2:05 - loss: 1.357 - ETA: 2:04 - loss: 1.357 - ETA: 2:02 - loss: 1.357 - ETA: 2:01 - loss: 1.356 - ETA: 1:59 - loss: 1.356 - ETA: 1:57 - loss: 1.355 - ETA: 1:56 - loss: 1.356 - ETA: 1:54 - loss: 1.356 - ETA: 1:53 - loss: 1.357 - ETA: 1:51 - loss: 1.357 - ETA: 1:49 - loss: 1.358 - ETA: 1:48 - loss: 1.357 - ETA: 1:46 - loss: 1.358 - ETA: 1:45 - loss: 1.361 - ETA: 1:43 - loss: 1.359 - ETA: 1:42 - loss: 1.359 - ETA: 1:40 - loss: 1.360 - ETA: 1:38 - loss: 1.360 - ETA: 1:37 - loss: 1.359 - ETA: 1:35 - loss: 1.359 - ETA: 1:34 - loss: 1.361 - ETA: 1:32 - loss: 1.359 - ETA: 1:30 - loss: 1.359 - ETA: 1:29 - loss: 1.360 - ETA: 1:27 - loss: 1.360 - ETA: 1:26 - loss: 1.361 - ETA: 1:24 - loss: 1.360 - ETA: 1:22 - loss: 1.361 - ETA: 1:21 - loss: 1.360 - ETA: 1:19 - loss: 1.361 - ETA: 1:18 - loss: 1.361 - ETA: 1:16 - loss: 1.362 - ETA: 1:14 - loss: 1.362 - ETA: 1:13 - loss: 1.362 - ETA: 1:11 - loss: 1.363 - ETA: 1:10 - loss: 1.362 - ETA: 1:08 - loss: 1.363 - ETA: 1:06 - loss: 1.364 - ETA: 1:05 - loss: 1.363 - ETA: 1:03 - loss: 1.363 - ETA: 1:02 - loss: 1.364 - ETA: 1:00 - loss: 1.365 - ETA: 58s - loss: 1.365 - ETA: 57s - loss: 1.36 - ETA: 55s - loss: 1.36 - ETA: 54s - loss: 1.36 - ETA: 52s - loss: 1.36 - ETA: 50s - loss: 1.36 - ETA: 49s - loss: 1.36 - ETA: 47s - loss: 1.36 - ETA: 46s - loss: 1.36 - ETA: 44s - loss: 1.36 - ETA: 43s - loss: 1.36 - ETA: 41s - loss: 1.36 - ETA: 39s - loss: 1.36 - ETA: 38s - loss: 1.36 - ETA: 36s - loss: 1.36 - ETA: 35s - loss: 1.36 - ETA: 33s - loss: 1.36 - ETA: 31s - loss: 1.36 - ETA: 30s - loss: 1.36 - ETA: 28s - loss: 1.36 - ETA: 27s - loss: 1.36 - ETA: 25s - loss: 1.37 - ETA: 23s - loss: 1.37 - ETA: 22s - loss: 1.37 - ETA: 20s - loss: 1.37 - ETA: 19s - loss: 1.37 - ETA: 17s - loss: 1.37 - ETA: 15s - loss: 1.37 - ETA: 14s - loss: 1.37 - ETA: 12s - loss: 1.37 - ETA: 11s - loss: 1.37 - ETA: 9s - loss: 1.3714 - ETA: 8s - loss: 1.371 - ETA: 6s - loss: 1.370 - ETA: 4s - loss: 1.369 - ETA: 3s - loss: 1.369 - ETA: 1s - loss: 1.368 - 302s 2s/step - loss: 1.3679 - val_loss: 1.4419\n",
      "\n",
      "Epoch 00003: val_loss improved from 10.80313 to 1.44194, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 4/20\n",
      "187/187 [==============================] - ETA: 5:01 - loss: 1.370 - ETA: 4:59 - loss: 1.416 - ETA: 4:57 - loss: 1.366 - ETA: 4:55 - loss: 1.374 - ETA: 4:54 - loss: 1.334 - ETA: 4:52 - loss: 1.335 - ETA: 4:51 - loss: 1.334 - ETA: 4:49 - loss: 1.342 - ETA: 4:47 - loss: 1.336 - ETA: 4:45 - loss: 1.337 - ETA: 4:43 - loss: 1.336 - ETA: 4:41 - loss: 1.337 - ETA: 4:39 - loss: 1.353 - ETA: 4:37 - loss: 1.347 - ETA: 4:35 - loss: 1.343 - ETA: 4:34 - loss: 1.334 - ETA: 4:32 - loss: 1.327 - ETA: 4:30 - loss: 1.334 - ETA: 4:28 - loss: 1.322 - ETA: 4:27 - loss: 1.327 - ETA: 4:25 - loss: 1.322 - ETA: 4:23 - loss: 1.323 - ETA: 4:22 - loss: 1.324 - ETA: 4:20 - loss: 1.328 - ETA: 4:18 - loss: 1.326 - ETA: 4:17 - loss: 1.326 - ETA: 4:15 - loss: 1.326 - ETA: 4:14 - loss: 1.319 - ETA: 4:12 - loss: 1.325 - ETA: 4:10 - loss: 1.323 - ETA: 4:09 - loss: 1.323 - ETA: 4:07 - loss: 1.327 - ETA: 4:05 - loss: 1.323 - ETA: 4:04 - loss: 1.327 - ETA: 4:02 - loss: 1.326 - ETA: 4:00 - loss: 1.322 - ETA: 3:59 - loss: 1.316 - ETA: 3:57 - loss: 1.313 - ETA: 3:56 - loss: 1.311 - ETA: 3:54 - loss: 1.313 - ETA: 3:52 - loss: 1.311 - ETA: 3:51 - loss: 1.310 - ETA: 3:49 - loss: 1.310 - ETA: 3:48 - loss: 1.306 - ETA: 3:46 - loss: 1.307 - ETA: 3:44 - loss: 1.307 - ETA: 3:43 - loss: 1.303 - ETA: 3:41 - loss: 1.301 - ETA: 3:40 - loss: 1.302 - ETA: 3:38 - loss: 1.302 - ETA: 3:36 - loss: 1.299 - ETA: 3:35 - loss: 1.300 - ETA: 3:33 - loss: 1.304 - ETA: 3:31 - loss: 1.303 - ETA: 3:30 - loss: 1.305 - ETA: 3:28 - loss: 1.306 - ETA: 3:27 - loss: 1.308 - ETA: 3:25 - loss: 1.305 - ETA: 3:23 - loss: 1.307 - ETA: 3:22 - loss: 1.308 - ETA: 3:20 - loss: 1.308 - ETA: 3:19 - loss: 1.310 - ETA: 3:17 - loss: 1.309 - ETA: 3:15 - loss: 1.307 - ETA: 3:14 - loss: 1.309 - ETA: 3:12 - loss: 1.310 - ETA: 3:11 - loss: 1.310 - ETA: 3:09 - loss: 1.309 - ETA: 3:07 - loss: 1.309 - ETA: 3:06 - loss: 1.311 - ETA: 3:04 - loss: 1.312 - ETA: 3:03 - loss: 1.312 - ETA: 3:01 - loss: 1.315 - ETA: 2:59 - loss: 1.314 - ETA: 2:58 - loss: 1.315 - ETA: 2:56 - loss: 1.315 - ETA: 2:55 - loss: 1.314 - ETA: 2:53 - loss: 1.316 - ETA: 2:51 - loss: 1.317 - ETA: 2:50 - loss: 1.315 - ETA: 2:48 - loss: 1.317 - ETA: 2:47 - loss: 1.316 - ETA: 2:45 - loss: 1.314 - ETA: 2:44 - loss: 1.316 - ETA: 2:42 - loss: 1.317 - ETA: 2:40 - loss: 1.317 - ETA: 2:39 - loss: 1.316 - ETA: 2:37 - loss: 1.317 - ETA: 2:36 - loss: 1.314 - ETA: 2:34 - loss: 1.315 - ETA: 2:32 - loss: 1.316 - ETA: 2:31 - loss: 1.317 - ETA: 2:29 - loss: 1.316 - ETA: 2:28 - loss: 1.315 - ETA: 2:26 - loss: 1.316 - ETA: 2:24 - loss: 1.315 - ETA: 2:23 - loss: 1.316 - ETA: 2:21 - loss: 1.317 - ETA: 2:20 - loss: 1.317 - ETA: 2:18 - loss: 1.316 - ETA: 2:16 - loss: 1.317 - ETA: 2:15 - loss: 1.319 - ETA: 2:13 - loss: 1.317 - ETA: 2:12 - loss: 1.316 - ETA: 2:10 - loss: 1.315 - ETA: 2:08 - loss: 1.317 - ETA: 2:07 - loss: 1.317 - ETA: 2:05 - loss: 1.316 - ETA: 2:04 - loss: 1.313 - ETA: 2:02 - loss: 1.313 - ETA: 2:01 - loss: 1.314 - ETA: 1:59 - loss: 1.314 - ETA: 1:57 - loss: 1.315 - ETA: 1:56 - loss: 1.314 - ETA: 1:54 - loss: 1.313 - ETA: 1:53 - loss: 1.312 - ETA: 1:51 - loss: 1.311 - ETA: 1:49 - loss: 1.312 - ETA: 1:48 - loss: 1.311 - ETA: 1:46 - loss: 1.312 - ETA: 1:45 - loss: 1.311 - ETA: 1:43 - loss: 1.310 - ETA: 1:41 - loss: 1.312 - ETA: 1:40 - loss: 1.310 - ETA: 1:38 - loss: 1.311 - ETA: 1:37 - loss: 1.310 - ETA: 1:35 - loss: 1.309 - ETA: 1:33 - loss: 1.309 - ETA: 1:32 - loss: 1.310 - ETA: 1:30 - loss: 1.310 - ETA: 1:29 - loss: 1.309 - ETA: 1:27 - loss: 1.309 - ETA: 1:25 - loss: 1.309 - ETA: 1:24 - loss: 1.308 - ETA: 1:22 - loss: 1.307 - ETA: 1:21 - loss: 1.306 - ETA: 1:19 - loss: 1.305 - ETA: 1:17 - loss: 1.305 - ETA: 1:16 - loss: 1.306 - ETA: 1:14 - loss: 1.307 - ETA: 1:13 - loss: 1.307 - ETA: 1:11 - loss: 1.307 - ETA: 1:10 - loss: 1.307 - ETA: 1:08 - loss: 1.308 - ETA: 1:06 - loss: 1.307 - ETA: 1:05 - loss: 1.306 - ETA: 1:03 - loss: 1.307 - ETA: 1:02 - loss: 1.307 - ETA: 1:00 - loss: 1.306 - ETA: 58s - loss: 1.307 - ETA: 57s - loss: 1.30 - ETA: 55s - loss: 1.30 - ETA: 54s - loss: 1.30 - ETA: 52s - loss: 1.30 - ETA: 50s - loss: 1.30 - ETA: 49s - loss: 1.30 - ETA: 47s - loss: 1.30 - ETA: 46s - loss: 1.30 - ETA: 44s - loss: 1.30 - ETA: 42s - loss: 1.30 - ETA: 41s - loss: 1.30 - ETA: 39s - loss: 1.30 - ETA: 38s - loss: 1.30 - ETA: 36s - loss: 1.30 - ETA: 35s - loss: 1.30 - ETA: 33s - loss: 1.30 - ETA: 31s - loss: 1.30 - ETA: 30s - loss: 1.30 - ETA: 28s - loss: 1.30 - ETA: 27s - loss: 1.30 - ETA: 25s - loss: 1.30 - ETA: 23s - loss: 1.30 - ETA: 22s - loss: 1.30 - ETA: 20s - loss: 1.30 - ETA: 19s - loss: 1.30 - ETA: 17s - loss: 1.30 - ETA: 15s - loss: 1.30 - ETA: 14s - loss: 1.30 - ETA: 12s - loss: 1.30 - ETA: 11s - loss: 1.30 - ETA: 9s - loss: 1.3059 - ETA: 7s - loss: 1.305 - ETA: 6s - loss: 1.305 - ETA: 4s - loss: 1.305 - ETA: 3s - loss: 1.305 - ETA: 1s - loss: 1.305 - 300s 2s/step - loss: 1.3051 - val_loss: 1.5421\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.44194\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:54 - loss: 1.335 - ETA: 4:53 - loss: 1.237 - ETA: 4:51 - loss: 1.232 - ETA: 4:50 - loss: 1.252 - ETA: 4:48 - loss: 1.257 - ETA: 4:46 - loss: 1.234 - ETA: 4:45 - loss: 1.237 - ETA: 4:44 - loss: 1.241 - ETA: 4:42 - loss: 1.253 - ETA: 4:40 - loss: 1.241 - ETA: 4:39 - loss: 1.249 - ETA: 4:37 - loss: 1.255 - ETA: 4:36 - loss: 1.247 - ETA: 4:34 - loss: 1.253 - ETA: 4:32 - loss: 1.258 - ETA: 4:31 - loss: 1.253 - ETA: 4:30 - loss: 1.251 - ETA: 4:28 - loss: 1.255 - ETA: 4:26 - loss: 1.260 - ETA: 4:25 - loss: 1.268 - ETA: 4:23 - loss: 1.271 - ETA: 4:22 - loss: 1.270 - ETA: 4:20 - loss: 1.269 - ETA: 4:18 - loss: 1.266 - ETA: 4:17 - loss: 1.267 - ETA: 4:15 - loss: 1.272 - ETA: 4:14 - loss: 1.275 - ETA: 4:12 - loss: 1.274 - ETA: 4:11 - loss: 1.277 - ETA: 4:09 - loss: 1.274 - ETA: 4:07 - loss: 1.275 - ETA: 4:06 - loss: 1.271 - ETA: 4:04 - loss: 1.269 - ETA: 4:03 - loss: 1.272 - ETA: 4:01 - loss: 1.274 - ETA: 3:59 - loss: 1.271 - ETA: 3:58 - loss: 1.271 - ETA: 3:56 - loss: 1.271 - ETA: 3:55 - loss: 1.272 - ETA: 3:53 - loss: 1.275 - ETA: 3:51 - loss: 1.272 - ETA: 3:50 - loss: 1.271 - ETA: 3:48 - loss: 1.271 - ETA: 3:47 - loss: 1.272 - ETA: 3:45 - loss: 1.272 - ETA: 3:44 - loss: 1.273 - ETA: 3:42 - loss: 1.270 - ETA: 3:40 - loss: 1.279 - ETA: 3:39 - loss: 1.282 - ETA: 3:37 - loss: 1.281 - ETA: 3:35 - loss: 1.278 - ETA: 3:34 - loss: 1.280 - ETA: 3:32 - loss: 1.277 - ETA: 3:31 - loss: 1.275 - ETA: 3:29 - loss: 1.274 - ETA: 3:28 - loss: 1.274 - ETA: 3:26 - loss: 1.275 - ETA: 3:24 - loss: 1.276 - ETA: 3:23 - loss: 1.280 - ETA: 3:21 - loss: 1.281 - ETA: 3:20 - loss: 1.276 - ETA: 3:18 - loss: 1.277 - ETA: 3:17 - loss: 1.277 - ETA: 3:15 - loss: 1.279 - ETA: 3:13 - loss: 1.277 - ETA: 3:12 - loss: 1.281 - ETA: 3:10 - loss: 1.281 - ETA: 3:09 - loss: 1.280 - ETA: 3:07 - loss: 1.282 - ETA: 3:05 - loss: 1.281 - ETA: 3:04 - loss: 1.281 - ETA: 3:02 - loss: 1.282 - ETA: 3:01 - loss: 1.282 - ETA: 2:59 - loss: 1.283 - ETA: 2:57 - loss: 1.282 - ETA: 2:56 - loss: 1.281 - ETA: 2:54 - loss: 1.283 - ETA: 2:53 - loss: 1.284 - ETA: 2:51 - loss: 1.288 - ETA: 2:50 - loss: 1.287 - ETA: 2:48 - loss: 1.288 - ETA: 2:46 - loss: 1.287 - ETA: 2:45 - loss: 1.287 - ETA: 2:43 - loss: 1.286 - ETA: 2:42 - loss: 1.285 - ETA: 2:40 - loss: 1.286 - ETA: 2:38 - loss: 1.288 - ETA: 2:37 - loss: 1.289 - ETA: 2:35 - loss: 1.289 - ETA: 2:34 - loss: 1.292 - ETA: 2:32 - loss: 1.291 - ETA: 2:30 - loss: 1.292 - ETA: 2:29 - loss: 1.290 - ETA: 2:27 - loss: 1.290 - ETA: 2:26 - loss: 1.290 - ETA: 2:24 - loss: 1.289 - ETA: 2:23 - loss: 1.290 - ETA: 2:21 - loss: 1.289 - ETA: 2:19 - loss: 1.289 - ETA: 2:18 - loss: 1.289 - ETA: 2:16 - loss: 1.290 - ETA: 2:15 - loss: 1.292 - ETA: 2:13 - loss: 1.293 - ETA: 2:11 - loss: 1.291 - ETA: 2:10 - loss: 1.289 - ETA: 2:08 - loss: 1.288 - ETA: 2:07 - loss: 1.288 - ETA: 2:05 - loss: 1.289 - ETA: 2:03 - loss: 1.288 - ETA: 2:02 - loss: 1.289 - ETA: 2:00 - loss: 1.289 - ETA: 1:59 - loss: 1.287 - ETA: 1:57 - loss: 1.285 - ETA: 1:56 - loss: 1.284 - ETA: 1:54 - loss: 1.284 - ETA: 1:52 - loss: 1.284 - ETA: 1:51 - loss: 1.282 - ETA: 1:49 - loss: 1.284 - ETA: 1:48 - loss: 1.284 - ETA: 1:46 - loss: 1.285 - ETA: 1:44 - loss: 1.283 - ETA: 1:43 - loss: 1.282 - ETA: 1:41 - loss: 1.282 - ETA: 1:40 - loss: 1.281 - ETA: 1:38 - loss: 1.280 - ETA: 1:36 - loss: 1.280 - ETA: 1:35 - loss: 1.280 - ETA: 1:33 - loss: 1.280 - ETA: 1:32 - loss: 1.281 - ETA: 1:30 - loss: 1.280 - ETA: 1:28 - loss: 1.279 - ETA: 1:27 - loss: 1.280 - ETA: 1:25 - loss: 1.279 - ETA: 1:24 - loss: 1.280 - ETA: 1:22 - loss: 1.279 - ETA: 1:21 - loss: 1.279 - ETA: 1:19 - loss: 1.279 - ETA: 1:17 - loss: 1.280 - ETA: 1:16 - loss: 1.280 - ETA: 1:14 - loss: 1.281 - ETA: 1:13 - loss: 1.281 - ETA: 1:11 - loss: 1.282 - ETA: 1:09 - loss: 1.282 - ETA: 1:08 - loss: 1.281 - ETA: 1:06 - loss: 1.282 - ETA: 1:05 - loss: 1.282 - ETA: 1:03 - loss: 1.282 - ETA: 1:01 - loss: 1.283 - ETA: 1:00 - loss: 1.284 - ETA: 58s - loss: 1.282 - ETA: 57s - loss: 1.28 - ETA: 55s - loss: 1.28 - ETA: 54s - loss: 1.28 - ETA: 52s - loss: 1.28 - ETA: 50s - loss: 1.28 - ETA: 49s - loss: 1.27 - ETA: 47s - loss: 1.27 - ETA: 46s - loss: 1.27 - ETA: 44s - loss: 1.27 - ETA: 42s - loss: 1.27 - ETA: 41s - loss: 1.27 - ETA: 39s - loss: 1.27 - ETA: 38s - loss: 1.28 - ETA: 36s - loss: 1.27 - ETA: 34s - loss: 1.27 - ETA: 33s - loss: 1.27 - ETA: 31s - loss: 1.27 - ETA: 30s - loss: 1.27 - ETA: 28s - loss: 1.27 - ETA: 27s - loss: 1.27 - ETA: 25s - loss: 1.27 - ETA: 23s - loss: 1.27 - ETA: 22s - loss: 1.27 - ETA: 20s - loss: 1.27 - ETA: 19s - loss: 1.27 - ETA: 17s - loss: 1.27 - ETA: 15s - loss: 1.27 - ETA: 14s - loss: 1.27 - ETA: 12s - loss: 1.27 - ETA: 11s - loss: 1.27 - ETA: 9s - loss: 1.2773 - ETA: 7s - loss: 1.277 - ETA: 6s - loss: 1.277 - ETA: 4s - loss: 1.277 - ETA: 3s - loss: 1.277 - ETA: 1s - loss: 1.276 - 299s 2s/step - loss: 1.2757 - val_loss: 3.0958\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.44194\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "187/187 [==============================] - ETA: 4:54 - loss: 1.211 - ETA: 4:53 - loss: 1.232 - ETA: 4:51 - loss: 1.257 - ETA: 4:50 - loss: 1.310 - ETA: 4:49 - loss: 1.308 - ETA: 4:47 - loss: 1.285 - ETA: 4:45 - loss: 1.280 - ETA: 4:44 - loss: 1.285 - ETA: 4:42 - loss: 1.278 - ETA: 4:41 - loss: 1.276 - ETA: 4:39 - loss: 1.261 - ETA: 4:38 - loss: 1.252 - ETA: 4:36 - loss: 1.251 - ETA: 4:35 - loss: 1.249 - ETA: 4:33 - loss: 1.258 - ETA: 4:31 - loss: 1.260 - ETA: 4:30 - loss: 1.250 - ETA: 4:28 - loss: 1.250 - ETA: 4:27 - loss: 1.244 - ETA: 4:25 - loss: 1.242 - ETA: 4:24 - loss: 1.242 - ETA: 4:22 - loss: 1.247 - ETA: 4:20 - loss: 1.246 - ETA: 4:19 - loss: 1.249 - ETA: 4:17 - loss: 1.248 - ETA: 4:16 - loss: 1.252 - ETA: 4:14 - loss: 1.244 - ETA: 4:12 - loss: 1.246 - ETA: 4:11 - loss: 1.246 - ETA: 4:09 - loss: 1.237 - ETA: 4:08 - loss: 1.240 - ETA: 4:06 - loss: 1.245 - ETA: 4:04 - loss: 1.241 - ETA: 4:03 - loss: 1.241 - ETA: 4:01 - loss: 1.244 - ETA: 4:00 - loss: 1.246 - ETA: 3:58 - loss: 1.243 - ETA: 3:56 - loss: 1.240 - ETA: 3:55 - loss: 1.241 - ETA: 3:53 - loss: 1.241 - ETA: 3:52 - loss: 1.240 - ETA: 3:50 - loss: 1.240 - ETA: 3:48 - loss: 1.239 - ETA: 3:47 - loss: 1.238 - ETA: 3:45 - loss: 1.238 - ETA: 3:44 - loss: 1.232 - ETA: 3:42 - loss: 1.235 - ETA: 3:40 - loss: 1.231 - ETA: 3:39 - loss: 1.232 - ETA: 3:37 - loss: 1.231 - ETA: 3:36 - loss: 1.229 - ETA: 3:34 - loss: 1.228 - ETA: 3:32 - loss: 1.232 - ETA: 3:31 - loss: 1.230 - ETA: 3:29 - loss: 1.228 - ETA: 3:28 - loss: 1.228 - ETA: 3:26 - loss: 1.227 - ETA: 3:25 - loss: 1.227 - ETA: 3:23 - loss: 1.227 - ETA: 3:21 - loss: 1.225 - ETA: 3:20 - loss: 1.224 - ETA: 3:18 - loss: 1.223 - ETA: 3:17 - loss: 1.219 - ETA: 3:15 - loss: 1.219 - ETA: 3:13 - loss: 1.218 - ETA: 3:12 - loss: 1.220 - ETA: 3:10 - loss: 1.220 - ETA: 3:09 - loss: 1.220 - ETA: 3:07 - loss: 1.218 - ETA: 3:05 - loss: 1.220 - ETA: 3:04 - loss: 1.221 - ETA: 3:02 - loss: 1.221 - ETA: 3:01 - loss: 1.221 - ETA: 2:59 - loss: 1.220 - ETA: 2:57 - loss: 1.221 - ETA: 2:56 - loss: 1.220 - ETA: 2:54 - loss: 1.220 - ETA: 2:53 - loss: 1.220 - ETA: 2:51 - loss: 1.220 - ETA: 2:49 - loss: 1.219 - ETA: 2:48 - loss: 1.218 - ETA: 2:46 - loss: 1.220 - ETA: 2:45 - loss: 1.221 - ETA: 2:43 - loss: 1.220 - ETA: 2:42 - loss: 1.220 - ETA: 2:40 - loss: 1.222 - ETA: 2:38 - loss: 1.223 - ETA: 2:37 - loss: 1.222 - ETA: 2:35 - loss: 1.222 - ETA: 2:34 - loss: 1.224 - ETA: 2:32 - loss: 1.223 - ETA: 2:30 - loss: 1.221 - ETA: 2:29 - loss: 1.221 - ETA: 2:27 - loss: 1.223 - ETA: 2:26 - loss: 1.223 - ETA: 2:24 - loss: 1.223 - ETA: 2:23 - loss: 1.222 - ETA: 2:21 - loss: 1.222 - ETA: 2:19 - loss: 1.223 - ETA: 2:18 - loss: 1.224 - ETA: 2:16 - loss: 1.224 - ETA: 2:15 - loss: 1.223 - ETA: 2:13 - loss: 1.225 - ETA: 2:11 - loss: 1.225 - ETA: 2:10 - loss: 1.224 - ETA: 2:08 - loss: 1.224 - ETA: 2:07 - loss: 1.225 - ETA: 2:05 - loss: 1.226 - ETA: 2:03 - loss: 1.226 - ETA: 2:02 - loss: 1.225 - ETA: 2:00 - loss: 1.223 - ETA: 1:59 - loss: 1.224 - ETA: 1:57 - loss: 1.224 - ETA: 1:56 - loss: 1.222 - ETA: 1:54 - loss: 1.221 - ETA: 1:52 - loss: 1.221 - ETA: 1:51 - loss: 1.223 - ETA: 1:49 - loss: 1.223 - ETA: 1:48 - loss: 1.222 - ETA: 1:46 - loss: 1.223 - ETA: 1:44 - loss: 1.225 - ETA: 1:43 - loss: 1.224 - ETA: 1:41 - loss: 1.223 - ETA: 1:40 - loss: 1.222 - ETA: 1:38 - loss: 1.223 - ETA: 1:36 - loss: 1.223 - ETA: 1:35 - loss: 1.223 - ETA: 1:33 - loss: 1.223 - ETA: 1:32 - loss: 1.223 - ETA: 1:30 - loss: 1.223 - ETA: 1:28 - loss: 1.223 - ETA: 1:27 - loss: 1.223 - ETA: 1:25 - loss: 1.223 - ETA: 1:24 - loss: 1.223 - ETA: 1:22 - loss: 1.224 - ETA: 1:21 - loss: 1.223 - ETA: 1:19 - loss: 1.221 - ETA: 1:17 - loss: 1.221 - ETA: 1:16 - loss: 1.220 - ETA: 1:14 - loss: 1.219 - ETA: 1:13 - loss: 1.219 - ETA: 1:11 - loss: 1.218 - ETA: 1:09 - loss: 1.219 - ETA: 1:08 - loss: 1.219 - ETA: 1:06 - loss: 1.218 - ETA: 1:05 - loss: 1.218 - ETA: 1:03 - loss: 1.220 - ETA: 1:01 - loss: 1.220 - ETA: 1:00 - loss: 1.221 - ETA: 58s - loss: 1.221 - ETA: 57s - loss: 1.22 - ETA: 55s - loss: 1.22 - ETA: 54s - loss: 1.21 - ETA: 52s - loss: 1.21 - ETA: 50s - loss: 1.21 - ETA: 49s - loss: 1.21 - ETA: 47s - loss: 1.21 - ETA: 46s - loss: 1.21 - ETA: 44s - loss: 1.21 - ETA: 42s - loss: 1.21 - ETA: 41s - loss: 1.21 - ETA: 39s - loss: 1.21 - ETA: 38s - loss: 1.21 - ETA: 36s - loss: 1.21 - ETA: 34s - loss: 1.22 - ETA: 33s - loss: 1.22 - ETA: 31s - loss: 1.22 - ETA: 30s - loss: 1.22 - ETA: 28s - loss: 1.21 - ETA: 27s - loss: 1.21 - ETA: 25s - loss: 1.21 - ETA: 23s - loss: 1.21 - ETA: 22s - loss: 1.21 - ETA: 20s - loss: 1.21 - ETA: 19s - loss: 1.21 - ETA: 17s - loss: 1.21 - ETA: 15s - loss: 1.21 - ETA: 14s - loss: 1.21 - ETA: 12s - loss: 1.21 - ETA: 11s - loss: 1.21 - ETA: 9s - loss: 1.2161 - ETA: 7s - loss: 1.216 - ETA: 6s - loss: 1.215 - ETA: 4s - loss: 1.215 - ETA: 3s - loss: 1.214 - ETA: 1s - loss: 1.214 - 299s 2s/step - loss: 1.2144 - val_loss: 1.3481\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.44194 to 1.34808, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:53 - loss: 1.215 - ETA: 4:52 - loss: 1.076 - ETA: 4:50 - loss: 1.144 - ETA: 4:49 - loss: 1.160 - ETA: 4:48 - loss: 1.161 - ETA: 4:47 - loss: 1.205 - ETA: 4:45 - loss: 1.204 - ETA: 4:44 - loss: 1.231 - ETA: 4:42 - loss: 1.236 - ETA: 4:41 - loss: 1.234 - ETA: 4:39 - loss: 1.230 - ETA: 4:38 - loss: 1.226 - ETA: 4:36 - loss: 1.225 - ETA: 4:34 - loss: 1.226 - ETA: 4:33 - loss: 1.230 - ETA: 4:31 - loss: 1.227 - ETA: 4:30 - loss: 1.238 - ETA: 4:28 - loss: 1.234 - ETA: 4:26 - loss: 1.226 - ETA: 4:25 - loss: 1.223 - ETA: 4:23 - loss: 1.222 - ETA: 4:22 - loss: 1.216 - ETA: 4:20 - loss: 1.211 - ETA: 4:18 - loss: 1.207 - ETA: 4:17 - loss: 1.205 - ETA: 4:15 - loss: 1.202 - ETA: 4:14 - loss: 1.195 - ETA: 4:12 - loss: 1.197 - ETA: 4:10 - loss: 1.196 - ETA: 4:09 - loss: 1.200 - ETA: 4:07 - loss: 1.204 - ETA: 4:06 - loss: 1.202 - ETA: 4:04 - loss: 1.201 - ETA: 4:02 - loss: 1.199 - ETA: 4:01 - loss: 1.199 - ETA: 3:59 - loss: 1.199 - ETA: 3:58 - loss: 1.200 - ETA: 3:56 - loss: 1.201 - ETA: 3:54 - loss: 1.197 - ETA: 3:53 - loss: 1.197 - ETA: 3:51 - loss: 1.194 - ETA: 3:50 - loss: 1.196 - ETA: 3:48 - loss: 1.196 - ETA: 3:47 - loss: 1.195 - ETA: 3:45 - loss: 1.192 - ETA: 3:43 - loss: 1.194 - ETA: 3:42 - loss: 1.195 - ETA: 3:40 - loss: 1.197 - ETA: 3:39 - loss: 1.198 - ETA: 3:37 - loss: 1.201 - ETA: 3:36 - loss: 1.200 - ETA: 3:34 - loss: 1.199 - ETA: 3:32 - loss: 1.199 - ETA: 3:31 - loss: 1.198 - ETA: 3:29 - loss: 1.197 - ETA: 3:28 - loss: 1.199 - ETA: 3:26 - loss: 1.196 - ETA: 3:24 - loss: 1.196 - ETA: 3:23 - loss: 1.198 - ETA: 3:21 - loss: 1.200 - ETA: 3:20 - loss: 1.199 - ETA: 3:18 - loss: 1.199 - ETA: 3:17 - loss: 1.203 - ETA: 3:15 - loss: 1.202 - ETA: 3:13 - loss: 1.200 - ETA: 3:12 - loss: 1.200 - ETA: 3:10 - loss: 1.200 - ETA: 3:09 - loss: 1.200 - ETA: 3:07 - loss: 1.200 - ETA: 3:05 - loss: 1.199 - ETA: 3:04 - loss: 1.198 - ETA: 3:02 - loss: 1.198 - ETA: 3:01 - loss: 1.199 - ETA: 2:59 - loss: 1.198 - ETA: 2:57 - loss: 1.197 - ETA: 2:56 - loss: 1.194 - ETA: 2:54 - loss: 1.195 - ETA: 2:53 - loss: 1.193 - ETA: 2:51 - loss: 1.190 - ETA: 2:49 - loss: 1.189 - ETA: 2:48 - loss: 1.189 - ETA: 2:46 - loss: 1.189 - ETA: 2:45 - loss: 1.190 - ETA: 2:43 - loss: 1.190 - ETA: 2:42 - loss: 1.190 - ETA: 2:40 - loss: 1.189 - ETA: 2:38 - loss: 1.188 - ETA: 2:37 - loss: 1.188 - ETA: 2:35 - loss: 1.189 - ETA: 2:34 - loss: 1.189 - ETA: 2:32 - loss: 1.188 - ETA: 2:30 - loss: 1.189 - ETA: 2:29 - loss: 1.190 - ETA: 2:27 - loss: 1.189 - ETA: 2:26 - loss: 1.190 - ETA: 2:24 - loss: 1.189 - ETA: 2:22 - loss: 1.190 - ETA: 2:21 - loss: 1.189 - ETA: 2:19 - loss: 1.192 - ETA: 2:18 - loss: 1.191 - ETA: 2:16 - loss: 1.191 - ETA: 2:15 - loss: 1.192 - ETA: 2:13 - loss: 1.191 - ETA: 2:11 - loss: 1.191 - ETA: 2:10 - loss: 1.193 - ETA: 2:08 - loss: 1.195 - ETA: 2:07 - loss: 1.195 - ETA: 2:05 - loss: 1.195 - ETA: 2:03 - loss: 1.194 - ETA: 2:02 - loss: 1.195 - ETA: 2:00 - loss: 1.195 - ETA: 1:59 - loss: 1.196 - ETA: 1:57 - loss: 1.195 - ETA: 1:55 - loss: 1.194 - ETA: 1:54 - loss: 1.195 - ETA: 1:52 - loss: 1.194 - ETA: 1:51 - loss: 1.194 - ETA: 1:49 - loss: 1.196 - ETA: 1:48 - loss: 1.195 - ETA: 1:46 - loss: 1.195 - ETA: 1:44 - loss: 1.194 - ETA: 1:43 - loss: 1.194 - ETA: 1:41 - loss: 1.196 - ETA: 1:40 - loss: 1.194 - ETA: 1:38 - loss: 1.194 - ETA: 1:36 - loss: 1.194 - ETA: 1:35 - loss: 1.195 - ETA: 1:33 - loss: 1.194 - ETA: 1:32 - loss: 1.194 - ETA: 1:30 - loss: 1.193 - ETA: 1:28 - loss: 1.194 - ETA: 1:27 - loss: 1.194 - ETA: 1:25 - loss: 1.193 - ETA: 1:24 - loss: 1.193 - ETA: 1:22 - loss: 1.192 - ETA: 1:21 - loss: 1.192 - ETA: 1:19 - loss: 1.192 - ETA: 1:17 - loss: 1.191 - ETA: 1:16 - loss: 1.191 - ETA: 1:14 - loss: 1.191 - ETA: 1:13 - loss: 1.191 - ETA: 1:11 - loss: 1.191 - ETA: 1:09 - loss: 1.191 - ETA: 1:08 - loss: 1.191 - ETA: 1:06 - loss: 1.190 - ETA: 1:05 - loss: 1.190 - ETA: 1:03 - loss: 1.190 - ETA: 1:01 - loss: 1.191 - ETA: 1:00 - loss: 1.189 - ETA: 58s - loss: 1.188 - ETA: 57s - loss: 1.18 - ETA: 55s - loss: 1.18 - ETA: 54s - loss: 1.18 - ETA: 52s - loss: 1.18 - ETA: 50s - loss: 1.18 - ETA: 49s - loss: 1.18 - ETA: 47s - loss: 1.18 - ETA: 46s - loss: 1.18 - ETA: 44s - loss: 1.18 - ETA: 42s - loss: 1.19 - ETA: 41s - loss: 1.18 - ETA: 39s - loss: 1.18 - ETA: 38s - loss: 1.18 - ETA: 36s - loss: 1.18 - ETA: 34s - loss: 1.18 - ETA: 33s - loss: 1.18 - ETA: 31s - loss: 1.18 - ETA: 30s - loss: 1.18 - ETA: 28s - loss: 1.18 - ETA: 27s - loss: 1.18 - ETA: 25s - loss: 1.18 - ETA: 23s - loss: 1.18 - ETA: 22s - loss: 1.18 - ETA: 20s - loss: 1.18 - ETA: 19s - loss: 1.18 - ETA: 17s - loss: 1.18 - ETA: 15s - loss: 1.18 - ETA: 14s - loss: 1.18 - ETA: 12s - loss: 1.18 - ETA: 11s - loss: 1.18 - ETA: 9s - loss: 1.1847 - ETA: 7s - loss: 1.183 - ETA: 6s - loss: 1.183 - ETA: 4s - loss: 1.183 - ETA: 3s - loss: 1.184 - ETA: 1s - loss: 1.184 - 299s 2s/step - loss: 1.1840 - val_loss: 1.3343\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.34808 to 1.33426, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 8/20\n",
      "187/187 [==============================] - ETA: 4:54 - loss: 1.364 - ETA: 4:52 - loss: 1.252 - ETA: 4:50 - loss: 1.207 - ETA: 4:49 - loss: 1.156 - ETA: 4:48 - loss: 1.172 - ETA: 4:46 - loss: 1.172 - ETA: 4:45 - loss: 1.165 - ETA: 4:43 - loss: 1.188 - ETA: 4:42 - loss: 1.214 - ETA: 4:40 - loss: 1.209 - ETA: 4:39 - loss: 1.232 - ETA: 4:37 - loss: 1.227 - ETA: 4:36 - loss: 1.224 - ETA: 4:34 - loss: 1.235 - ETA: 4:33 - loss: 1.233 - ETA: 4:31 - loss: 1.223 - ETA: 4:29 - loss: 1.218 - ETA: 4:28 - loss: 1.208 - ETA: 4:26 - loss: 1.201 - ETA: 4:25 - loss: 1.197 - ETA: 4:23 - loss: 1.199 - ETA: 4:21 - loss: 1.200 - ETA: 4:20 - loss: 1.196 - ETA: 4:18 - loss: 1.190 - ETA: 4:17 - loss: 1.194 - ETA: 4:15 - loss: 1.189 - ETA: 4:14 - loss: 1.187 - ETA: 4:12 - loss: 1.182 - ETA: 4:10 - loss: 1.182 - ETA: 4:09 - loss: 1.182 - ETA: 4:07 - loss: 1.181 - ETA: 4:06 - loss: 1.175 - ETA: 4:04 - loss: 1.174 - ETA: 4:02 - loss: 1.170 - ETA: 4:01 - loss: 1.174 - ETA: 3:59 - loss: 1.170 - ETA: 3:58 - loss: 1.171 - ETA: 3:56 - loss: 1.173 - ETA: 3:55 - loss: 1.175 - ETA: 3:53 - loss: 1.180 - ETA: 3:51 - loss: 1.178 - ETA: 3:50 - loss: 1.178 - ETA: 3:48 - loss: 1.178 - ETA: 3:47 - loss: 1.180 - ETA: 3:45 - loss: 1.177 - ETA: 3:43 - loss: 1.180 - ETA: 3:42 - loss: 1.180 - ETA: 3:40 - loss: 1.180 - ETA: 3:39 - loss: 1.177 - ETA: 3:37 - loss: 1.178 - ETA: 3:36 - loss: 1.181 - ETA: 3:34 - loss: 1.181 - ETA: 3:32 - loss: 1.180 - ETA: 3:31 - loss: 1.180 - ETA: 3:29 - loss: 1.181 - ETA: 3:28 - loss: 1.179 - ETA: 3:26 - loss: 1.177 - ETA: 3:24 - loss: 1.174 - ETA: 3:23 - loss: 1.174 - ETA: 3:21 - loss: 1.173 - ETA: 3:20 - loss: 1.172 - ETA: 3:18 - loss: 1.172 - ETA: 3:16 - loss: 1.176 - ETA: 3:15 - loss: 1.177 - ETA: 3:13 - loss: 1.179 - ETA: 3:12 - loss: 1.180 - ETA: 3:10 - loss: 1.182 - ETA: 3:09 - loss: 1.184 - ETA: 3:07 - loss: 1.184 - ETA: 3:05 - loss: 1.186 - ETA: 3:04 - loss: 1.185 - ETA: 3:02 - loss: 1.185 - ETA: 3:01 - loss: 1.185 - ETA: 2:59 - loss: 1.189 - ETA: 2:57 - loss: 1.189 - ETA: 2:56 - loss: 1.187 - ETA: 2:54 - loss: 1.187 - ETA: 2:53 - loss: 1.185 - ETA: 2:51 - loss: 1.183 - ETA: 2:50 - loss: 1.183 - ETA: 2:48 - loss: 1.183 - ETA: 2:46 - loss: 1.184 - ETA: 2:45 - loss: 1.182 - ETA: 2:43 - loss: 1.181 - ETA: 2:42 - loss: 1.182 - ETA: 2:40 - loss: 1.181 - ETA: 2:38 - loss: 1.181 - ETA: 2:37 - loss: 1.181 - ETA: 2:35 - loss: 1.180 - ETA: 2:34 - loss: 1.181 - ETA: 2:32 - loss: 1.181 - ETA: 2:30 - loss: 1.182 - ETA: 2:29 - loss: 1.180 - ETA: 2:27 - loss: 1.181 - ETA: 2:26 - loss: 1.180 - ETA: 2:24 - loss: 1.179 - ETA: 2:23 - loss: 1.180 - ETA: 2:21 - loss: 1.180 - ETA: 2:19 - loss: 1.180 - ETA: 2:18 - loss: 1.179 - ETA: 2:16 - loss: 1.179 - ETA: 2:15 - loss: 1.180 - ETA: 2:13 - loss: 1.180 - ETA: 2:11 - loss: 1.181 - ETA: 2:10 - loss: 1.179 - ETA: 2:08 - loss: 1.179 - ETA: 2:07 - loss: 1.179 - ETA: 2:05 - loss: 1.178 - ETA: 2:03 - loss: 1.178 - ETA: 2:02 - loss: 1.177 - ETA: 2:00 - loss: 1.176 - ETA: 1:59 - loss: 1.176 - ETA: 1:57 - loss: 1.175 - ETA: 1:56 - loss: 1.174 - ETA: 1:54 - loss: 1.174 - ETA: 1:52 - loss: 1.175 - ETA: 1:51 - loss: 1.174 - ETA: 1:49 - loss: 1.174 - ETA: 1:48 - loss: 1.174 - ETA: 1:46 - loss: 1.174 - ETA: 1:44 - loss: 1.174 - ETA: 1:43 - loss: 1.175 - ETA: 1:41 - loss: 1.175 - ETA: 1:40 - loss: 1.176 - ETA: 1:38 - loss: 1.175 - ETA: 1:36 - loss: 1.175 - ETA: 1:35 - loss: 1.176 - ETA: 1:33 - loss: 1.176 - ETA: 1:32 - loss: 1.176 - ETA: 1:30 - loss: 1.175 - ETA: 1:29 - loss: 1.176 - ETA: 1:27 - loss: 1.175 - ETA: 1:25 - loss: 1.175 - ETA: 1:24 - loss: 1.176 - ETA: 1:22 - loss: 1.176 - ETA: 1:21 - loss: 1.175 - ETA: 1:19 - loss: 1.174 - ETA: 1:17 - loss: 1.173 - ETA: 1:16 - loss: 1.174 - ETA: 1:14 - loss: 1.173 - ETA: 1:13 - loss: 1.172 - ETA: 1:11 - loss: 1.172 - ETA: 1:09 - loss: 1.171 - ETA: 1:08 - loss: 1.172 - ETA: 1:06 - loss: 1.172 - ETA: 1:05 - loss: 1.172 - ETA: 1:03 - loss: 1.172 - ETA: 1:01 - loss: 1.171 - ETA: 1:00 - loss: 1.172 - ETA: 58s - loss: 1.173 - ETA: 57s - loss: 1.17 - ETA: 55s - loss: 1.17 - ETA: 54s - loss: 1.17 - ETA: 52s - loss: 1.17 - ETA: 50s - loss: 1.17 - ETA: 49s - loss: 1.17 - ETA: 47s - loss: 1.17 - ETA: 46s - loss: 1.17 - ETA: 44s - loss: 1.17 - ETA: 42s - loss: 1.17 - ETA: 41s - loss: 1.17 - ETA: 39s - loss: 1.17 - ETA: 38s - loss: 1.17 - ETA: 36s - loss: 1.17 - ETA: 34s - loss: 1.17 - ETA: 33s - loss: 1.17 - ETA: 31s - loss: 1.17 - ETA: 30s - loss: 1.17 - ETA: 28s - loss: 1.17 - ETA: 27s - loss: 1.17 - ETA: 25s - loss: 1.17 - ETA: 23s - loss: 1.17 - ETA: 22s - loss: 1.17 - ETA: 20s - loss: 1.17 - ETA: 19s - loss: 1.17 - ETA: 17s - loss: 1.17 - ETA: 15s - loss: 1.17 - ETA: 14s - loss: 1.17 - ETA: 12s - loss: 1.17 - ETA: 11s - loss: 1.17 - ETA: 9s - loss: 1.1738 - ETA: 7s - loss: 1.173 - ETA: 6s - loss: 1.173 - ETA: 4s - loss: 1.173 - ETA: 3s - loss: 1.174 - ETA: 1s - loss: 1.174 - 299s 2s/step - loss: 1.1730 - val_loss: 1.4655\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.33426\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:55 - loss: 1.128 - ETA: 4:54 - loss: 1.200 - ETA: 4:52 - loss: 1.225 - ETA: 4:50 - loss: 1.181 - ETA: 4:49 - loss: 1.163 - ETA: 4:48 - loss: 1.146 - ETA: 4:46 - loss: 1.193 - ETA: 4:44 - loss: 1.195 - ETA: 4:42 - loss: 1.171 - ETA: 4:41 - loss: 1.183 - ETA: 4:39 - loss: 1.172 - ETA: 4:38 - loss: 1.178 - ETA: 4:36 - loss: 1.167 - ETA: 4:35 - loss: 1.169 - ETA: 4:33 - loss: 1.181 - ETA: 4:31 - loss: 1.175 - ETA: 4:30 - loss: 1.174 - ETA: 4:28 - loss: 1.172 - ETA: 4:27 - loss: 1.170 - ETA: 4:25 - loss: 1.165 - ETA: 4:24 - loss: 1.179 - ETA: 4:22 - loss: 1.174 - ETA: 4:20 - loss: 1.176 - ETA: 4:19 - loss: 1.180 - ETA: 4:17 - loss: 1.174 - ETA: 4:15 - loss: 1.172 - ETA: 4:14 - loss: 1.169 - ETA: 4:12 - loss: 1.172 - ETA: 4:11 - loss: 1.171 - ETA: 4:09 - loss: 1.175 - ETA: 4:08 - loss: 1.180 - ETA: 4:06 - loss: 1.176 - ETA: 4:04 - loss: 1.172 - ETA: 4:03 - loss: 1.175 - ETA: 4:01 - loss: 1.172 - ETA: 4:00 - loss: 1.169 - ETA: 3:58 - loss: 1.163 - ETA: 3:56 - loss: 1.162 - ETA: 3:55 - loss: 1.160 - ETA: 3:53 - loss: 1.159 - ETA: 3:52 - loss: 1.157 - ETA: 3:50 - loss: 1.158 - ETA: 3:48 - loss: 1.158 - ETA: 3:47 - loss: 1.164 - ETA: 3:45 - loss: 1.166 - ETA: 3:44 - loss: 1.165 - ETA: 3:42 - loss: 1.163 - ETA: 3:40 - loss: 1.161 - ETA: 3:39 - loss: 1.162 - ETA: 3:37 - loss: 1.161 - ETA: 3:36 - loss: 1.159 - ETA: 3:34 - loss: 1.158 - ETA: 3:33 - loss: 1.160 - ETA: 3:31 - loss: 1.160 - ETA: 3:29 - loss: 1.160 - ETA: 3:28 - loss: 1.159 - ETA: 3:26 - loss: 1.157 - ETA: 3:25 - loss: 1.157 - ETA: 3:23 - loss: 1.155 - ETA: 3:21 - loss: 1.156 - ETA: 3:20 - loss: 1.155 - ETA: 3:18 - loss: 1.157 - ETA: 3:17 - loss: 1.156 - ETA: 3:15 - loss: 1.153 - ETA: 3:13 - loss: 1.152 - ETA: 3:12 - loss: 1.150 - ETA: 3:10 - loss: 1.152 - ETA: 3:09 - loss: 1.152 - ETA: 3:07 - loss: 1.150 - ETA: 3:05 - loss: 1.150 - ETA: 3:04 - loss: 1.150 - ETA: 3:02 - loss: 1.149 - ETA: 3:01 - loss: 1.150 - ETA: 2:59 - loss: 1.152 - ETA: 2:58 - loss: 1.153 - ETA: 2:56 - loss: 1.153 - ETA: 2:54 - loss: 1.157 - ETA: 2:53 - loss: 1.156 - ETA: 2:51 - loss: 1.156 - ETA: 2:50 - loss: 1.157 - ETA: 2:48 - loss: 1.155 - ETA: 2:46 - loss: 1.155 - ETA: 2:45 - loss: 1.154 - ETA: 2:43 - loss: 1.156 - ETA: 2:42 - loss: 1.155 - ETA: 2:40 - loss: 1.154 - ETA: 2:38 - loss: 1.155 - ETA: 2:37 - loss: 1.157 - ETA: 2:35 - loss: 1.157 - ETA: 2:34 - loss: 1.159 - ETA: 2:32 - loss: 1.161 - ETA: 2:31 - loss: 1.161 - ETA: 2:29 - loss: 1.162 - ETA: 2:27 - loss: 1.163 - ETA: 2:26 - loss: 1.162 - ETA: 2:24 - loss: 1.161 - ETA: 2:23 - loss: 1.162 - ETA: 2:21 - loss: 1.161 - ETA: 2:19 - loss: 1.160 - ETA: 2:18 - loss: 1.158 - ETA: 2:16 - loss: 1.159 - ETA: 2:15 - loss: 1.159 - ETA: 2:13 - loss: 1.158 - ETA: 2:11 - loss: 1.156 - ETA: 2:10 - loss: 1.154 - ETA: 2:08 - loss: 1.155 - ETA: 2:07 - loss: 1.153 - ETA: 2:05 - loss: 1.153 - ETA: 2:03 - loss: 1.153 - ETA: 2:02 - loss: 1.151 - ETA: 2:00 - loss: 1.152 - ETA: 1:59 - loss: 1.151 - ETA: 1:57 - loss: 1.151 - ETA: 1:56 - loss: 1.151 - ETA: 1:54 - loss: 1.151 - ETA: 1:52 - loss: 1.150 - ETA: 1:51 - loss: 1.151 - ETA: 1:49 - loss: 1.151 - ETA: 1:48 - loss: 1.150 - ETA: 1:46 - loss: 1.150 - ETA: 1:44 - loss: 1.149 - ETA: 1:43 - loss: 1.148 - ETA: 1:41 - loss: 1.149 - ETA: 1:40 - loss: 1.150 - ETA: 1:38 - loss: 1.150 - ETA: 1:36 - loss: 1.149 - ETA: 1:35 - loss: 1.151 - ETA: 1:33 - loss: 1.152 - ETA: 1:32 - loss: 1.152 - ETA: 1:30 - loss: 1.152 - ETA: 1:29 - loss: 1.152 - ETA: 1:27 - loss: 1.151 - ETA: 1:25 - loss: 1.151 - ETA: 1:24 - loss: 1.152 - ETA: 1:22 - loss: 1.152 - ETA: 1:21 - loss: 1.154 - ETA: 1:19 - loss: 1.153 - ETA: 1:17 - loss: 1.153 - ETA: 1:16 - loss: 1.153 - ETA: 1:14 - loss: 1.153 - ETA: 1:13 - loss: 1.153 - ETA: 1:11 - loss: 1.153 - ETA: 1:09 - loss: 1.153 - ETA: 1:08 - loss: 1.153 - ETA: 1:06 - loss: 1.152 - ETA: 1:05 - loss: 1.151 - ETA: 1:03 - loss: 1.151 - ETA: 1:01 - loss: 1.150 - ETA: 1:00 - loss: 1.150 - ETA: 58s - loss: 1.151 - ETA: 57s - loss: 1.15 - ETA: 55s - loss: 1.15 - ETA: 54s - loss: 1.15 - ETA: 52s - loss: 1.15 - ETA: 50s - loss: 1.15 - ETA: 49s - loss: 1.15 - ETA: 47s - loss: 1.15 - ETA: 46s - loss: 1.15 - ETA: 44s - loss: 1.15 - ETA: 42s - loss: 1.15 - ETA: 41s - loss: 1.15 - ETA: 39s - loss: 1.15 - ETA: 38s - loss: 1.15 - ETA: 36s - loss: 1.15 - ETA: 34s - loss: 1.15 - ETA: 33s - loss: 1.15 - ETA: 31s - loss: 1.15 - ETA: 30s - loss: 1.15 - ETA: 28s - loss: 1.15 - ETA: 27s - loss: 1.15 - ETA: 25s - loss: 1.15 - ETA: 23s - loss: 1.15 - ETA: 22s - loss: 1.15 - ETA: 20s - loss: 1.15 - ETA: 19s - loss: 1.15 - ETA: 17s - loss: 1.15 - ETA: 15s - loss: 1.15 - ETA: 14s - loss: 1.15 - ETA: 12s - loss: 1.15 - ETA: 11s - loss: 1.15 - ETA: 9s - loss: 1.1523 - ETA: 7s - loss: 1.151 - ETA: 6s - loss: 1.151 - ETA: 4s - loss: 1.152 - ETA: 3s - loss: 1.151 - ETA: 1s - loss: 1.152 - 299s 2s/step - loss: 1.1507 - val_loss: 1.4074\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.33426\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/20\n",
      "187/187 [==============================] - ETA: 4:56 - loss: 1.123 - ETA: 4:53 - loss: 1.096 - ETA: 4:51 - loss: 1.104 - ETA: 4:50 - loss: 1.082 - ETA: 4:48 - loss: 1.096 - ETA: 4:47 - loss: 1.104 - ETA: 4:45 - loss: 1.107 - ETA: 4:44 - loss: 1.107 - ETA: 4:42 - loss: 1.097 - ETA: 4:41 - loss: 1.098 - ETA: 4:39 - loss: 1.104 - ETA: 4:38 - loss: 1.108 - ETA: 4:36 - loss: 1.114 - ETA: 4:35 - loss: 1.100 - ETA: 4:33 - loss: 1.112 - ETA: 4:31 - loss: 1.103 - ETA: 4:30 - loss: 1.097 - ETA: 4:28 - loss: 1.100 - ETA: 4:26 - loss: 1.107 - ETA: 4:25 - loss: 1.107 - ETA: 4:23 - loss: 1.109 - ETA: 4:22 - loss: 1.114 - ETA: 4:20 - loss: 1.106 - ETA: 4:18 - loss: 1.104 - ETA: 4:17 - loss: 1.103 - ETA: 4:15 - loss: 1.105 - ETA: 4:14 - loss: 1.107 - ETA: 4:12 - loss: 1.109 - ETA: 4:11 - loss: 1.114 - ETA: 4:09 - loss: 1.115 - ETA: 4:07 - loss: 1.111 - ETA: 4:06 - loss: 1.110 - ETA: 4:04 - loss: 1.117 - ETA: 4:03 - loss: 1.117 - ETA: 4:01 - loss: 1.113 - ETA: 3:59 - loss: 1.117 - ETA: 3:58 - loss: 1.117 - ETA: 3:56 - loss: 1.114 - ETA: 3:55 - loss: 1.114 - ETA: 3:53 - loss: 1.114 - ETA: 3:52 - loss: 1.112 - ETA: 3:50 - loss: 1.119 - ETA: 3:48 - loss: 1.120 - ETA: 3:47 - loss: 1.121 - ETA: 3:45 - loss: 1.122 - ETA: 3:44 - loss: 1.125 - ETA: 3:42 - loss: 1.127 - ETA: 3:40 - loss: 1.121 - ETA: 3:39 - loss: 1.120 - ETA: 3:37 - loss: 1.118 - ETA: 3:36 - loss: 1.117 - ETA: 3:34 - loss: 1.115 - ETA: 3:32 - loss: 1.117 - ETA: 3:31 - loss: 1.115 - ETA: 3:29 - loss: 1.117 - ETA: 3:28 - loss: 1.116 - ETA: 3:26 - loss: 1.119 - ETA: 3:25 - loss: 1.117 - ETA: 3:23 - loss: 1.118 - ETA: 3:21 - loss: 1.118 - ETA: 3:20 - loss: 1.120 - ETA: 3:18 - loss: 1.119 - ETA: 3:17 - loss: 1.121 - ETA: 3:15 - loss: 1.123 - ETA: 3:13 - loss: 1.122 - ETA: 3:12 - loss: 1.122 - ETA: 3:10 - loss: 1.123 - ETA: 3:09 - loss: 1.124 - ETA: 3:07 - loss: 1.125 - ETA: 3:06 - loss: 1.124 - ETA: 3:04 - loss: 1.124 - ETA: 3:02 - loss: 1.123 - ETA: 3:01 - loss: 1.123 - ETA: 2:59 - loss: 1.123 - ETA: 2:58 - loss: 1.122 - ETA: 2:56 - loss: 1.125 - ETA: 2:54 - loss: 1.126 - ETA: 2:53 - loss: 1.128 - ETA: 2:51 - loss: 1.129 - ETA: 2:50 - loss: 1.128 - ETA: 2:48 - loss: 1.128 - ETA: 2:46 - loss: 1.129 - ETA: 2:45 - loss: 1.128 - ETA: 2:43 - loss: 1.127 - ETA: 2:42 - loss: 1.128 - ETA: 2:40 - loss: 1.127 - ETA: 2:38 - loss: 1.128 - ETA: 2:37 - loss: 1.128 - ETA: 2:35 - loss: 1.132 - ETA: 2:34 - loss: 1.130 - ETA: 2:32 - loss: 1.130 - ETA: 2:31 - loss: 1.130 - ETA: 2:29 - loss: 1.129 - ETA: 2:27 - loss: 1.129 - ETA: 2:26 - loss: 1.128 - ETA: 2:24 - loss: 1.128 - ETA: 2:23 - loss: 1.129 - ETA: 2:21 - loss: 1.129 - ETA: 2:19 - loss: 1.129 - ETA: 2:18 - loss: 1.128 - ETA: 2:16 - loss: 1.127 - ETA: 2:15 - loss: 1.125 - ETA: 2:13 - loss: 1.125 - ETA: 2:11 - loss: 1.124 - ETA: 2:10 - loss: 1.125 - ETA: 2:08 - loss: 1.124 - ETA: 2:07 - loss: 1.124 - ETA: 2:05 - loss: 1.123 - ETA: 2:04 - loss: 1.122 - ETA: 2:02 - loss: 1.122 - ETA: 2:00 - loss: 1.122 - ETA: 1:59 - loss: 1.122 - ETA: 1:57 - loss: 1.121 - ETA: 1:56 - loss: 1.121 - ETA: 1:54 - loss: 1.121 - ETA: 1:52 - loss: 1.122 - ETA: 1:51 - loss: 1.123 - ETA: 1:49 - loss: 1.123 - ETA: 1:48 - loss: 1.122 - ETA: 1:46 - loss: 1.122 - ETA: 1:44 - loss: 1.121 - ETA: 1:43 - loss: 1.122 - ETA: 1:41 - loss: 1.121 - ETA: 1:40 - loss: 1.122 - ETA: 1:38 - loss: 1.122 - ETA: 1:36 - loss: 1.122 - ETA: 1:35 - loss: 1.122 - ETA: 1:33 - loss: 1.122 - ETA: 1:32 - loss: 1.120 - ETA: 1:30 - loss: 1.121 - ETA: 1:29 - loss: 1.121 - ETA: 1:27 - loss: 1.121 - ETA: 1:25 - loss: 1.122 - ETA: 1:24 - loss: 1.122 - ETA: 1:22 - loss: 1.121 - ETA: 1:21 - loss: 1.122 - ETA: 1:19 - loss: 1.121 - ETA: 1:17 - loss: 1.122 - ETA: 1:16 - loss: 1.122 - ETA: 1:14 - loss: 1.121 - ETA: 1:13 - loss: 1.121 - ETA: 1:11 - loss: 1.121 - ETA: 1:09 - loss: 1.121 - ETA: 1:08 - loss: 1.121 - ETA: 1:06 - loss: 1.121 - ETA: 1:05 - loss: 1.121 - ETA: 1:03 - loss: 1.122 - ETA: 1:02 - loss: 1.123 - ETA: 1:00 - loss: 1.124 - ETA: 58s - loss: 1.123 - ETA: 57s - loss: 1.12 - ETA: 55s - loss: 1.12 - ETA: 54s - loss: 1.12 - ETA: 52s - loss: 1.12 - ETA: 50s - loss: 1.12 - ETA: 49s - loss: 1.12 - ETA: 47s - loss: 1.12 - ETA: 46s - loss: 1.12 - ETA: 44s - loss: 1.12 - ETA: 42s - loss: 1.12 - ETA: 41s - loss: 1.12 - ETA: 39s - loss: 1.12 - ETA: 38s - loss: 1.12 - ETA: 36s - loss: 1.11 - ETA: 34s - loss: 1.11 - ETA: 33s - loss: 1.12 - ETA: 31s - loss: 1.12 - ETA: 30s - loss: 1.12 - ETA: 28s - loss: 1.12 - ETA: 27s - loss: 1.11 - ETA: 25s - loss: 1.12 - ETA: 23s - loss: 1.12 - ETA: 22s - loss: 1.12 - ETA: 20s - loss: 1.12 - ETA: 19s - loss: 1.12 - ETA: 17s - loss: 1.11 - ETA: 15s - loss: 1.11 - ETA: 14s - loss: 1.11 - ETA: 12s - loss: 1.11 - ETA: 11s - loss: 1.11 - ETA: 9s - loss: 1.1202 - ETA: 7s - loss: 1.120 - ETA: 6s - loss: 1.120 - ETA: 4s - loss: 1.120 - ETA: 3s - loss: 1.120 - ETA: 1s - loss: 1.120 - 299s 2s/step - loss: 1.1208 - val_loss: 1.3267\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.33426 to 1.32666, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:54 - loss: 1.116 - ETA: 4:53 - loss: 1.124 - ETA: 4:51 - loss: 1.137 - ETA: 4:49 - loss: 1.135 - ETA: 4:48 - loss: 1.121 - ETA: 4:47 - loss: 1.147 - ETA: 4:45 - loss: 1.145 - ETA: 4:44 - loss: 1.143 - ETA: 4:42 - loss: 1.135 - ETA: 4:41 - loss: 1.132 - ETA: 4:39 - loss: 1.132 - ETA: 4:37 - loss: 1.124 - ETA: 4:36 - loss: 1.123 - ETA: 4:34 - loss: 1.120 - ETA: 4:32 - loss: 1.117 - ETA: 4:31 - loss: 1.123 - ETA: 4:29 - loss: 1.121 - ETA: 4:28 - loss: 1.125 - ETA: 4:26 - loss: 1.125 - ETA: 4:25 - loss: 1.120 - ETA: 4:23 - loss: 1.121 - ETA: 4:21 - loss: 1.133 - ETA: 4:20 - loss: 1.134 - ETA: 4:18 - loss: 1.132 - ETA: 4:17 - loss: 1.127 - ETA: 4:15 - loss: 1.127 - ETA: 4:14 - loss: 1.129 - ETA: 4:12 - loss: 1.126 - ETA: 4:10 - loss: 1.128 - ETA: 4:09 - loss: 1.127 - ETA: 4:07 - loss: 1.128 - ETA: 4:06 - loss: 1.126 - ETA: 4:04 - loss: 1.119 - ETA: 4:03 - loss: 1.117 - ETA: 4:01 - loss: 1.116 - ETA: 3:59 - loss: 1.112 - ETA: 3:58 - loss: 1.114 - ETA: 3:56 - loss: 1.115 - ETA: 3:55 - loss: 1.116 - ETA: 3:53 - loss: 1.111 - ETA: 3:51 - loss: 1.110 - ETA: 3:50 - loss: 1.110 - ETA: 3:48 - loss: 1.108 - ETA: 3:47 - loss: 1.108 - ETA: 3:45 - loss: 1.105 - ETA: 3:43 - loss: 1.102 - ETA: 3:42 - loss: 1.099 - ETA: 3:40 - loss: 1.098 - ETA: 3:39 - loss: 1.096 - ETA: 3:37 - loss: 1.098 - ETA: 3:36 - loss: 1.099 - ETA: 3:34 - loss: 1.097 - ETA: 3:32 - loss: 1.095 - ETA: 3:31 - loss: 1.094 - ETA: 3:29 - loss: 1.093 - ETA: 3:28 - loss: 1.091 - ETA: 3:26 - loss: 1.093 - ETA: 3:24 - loss: 1.091 - ETA: 3:23 - loss: 1.091 - ETA: 3:21 - loss: 1.093 - ETA: 3:20 - loss: 1.095 - ETA: 3:18 - loss: 1.096 - ETA: 3:16 - loss: 1.098 - ETA: 3:15 - loss: 1.097 - ETA: 3:13 - loss: 1.098 - ETA: 3:12 - loss: 1.100 - ETA: 3:10 - loss: 1.103 - ETA: 3:09 - loss: 1.102 - ETA: 3:07 - loss: 1.100 - ETA: 3:05 - loss: 1.099 - ETA: 3:04 - loss: 1.098 - ETA: 3:02 - loss: 1.100 - ETA: 3:01 - loss: 1.100 - ETA: 2:59 - loss: 1.099 - ETA: 2:57 - loss: 1.097 - ETA: 2:56 - loss: 1.096 - ETA: 2:54 - loss: 1.099 - ETA: 2:53 - loss: 1.100 - ETA: 2:51 - loss: 1.099 - ETA: 2:49 - loss: 1.097 - ETA: 2:48 - loss: 1.097 - ETA: 2:46 - loss: 1.097 - ETA: 2:45 - loss: 1.096 - ETA: 2:43 - loss: 1.096 - ETA: 2:42 - loss: 1.097 - ETA: 2:40 - loss: 1.098 - ETA: 2:38 - loss: 1.098 - ETA: 2:37 - loss: 1.098 - ETA: 2:35 - loss: 1.099 - ETA: 2:34 - loss: 1.099 - ETA: 2:32 - loss: 1.097 - ETA: 2:30 - loss: 1.097 - ETA: 2:29 - loss: 1.096 - ETA: 2:27 - loss: 1.096 - ETA: 2:26 - loss: 1.098 - ETA: 2:24 - loss: 1.098 - ETA: 2:23 - loss: 1.097 - ETA: 2:21 - loss: 1.097 - ETA: 2:19 - loss: 1.098 - ETA: 2:18 - loss: 1.098 - ETA: 2:16 - loss: 1.098 - ETA: 2:15 - loss: 1.097 - ETA: 2:13 - loss: 1.097 - ETA: 2:11 - loss: 1.096 - ETA: 2:10 - loss: 1.096 - ETA: 2:08 - loss: 1.094 - ETA: 2:07 - loss: 1.094 - ETA: 2:05 - loss: 1.093 - ETA: 2:03 - loss: 1.094 - ETA: 2:02 - loss: 1.094 - ETA: 2:00 - loss: 1.094 - ETA: 1:59 - loss: 1.094 - ETA: 1:57 - loss: 1.094 - ETA: 1:55 - loss: 1.093 - ETA: 1:54 - loss: 1.095 - ETA: 1:52 - loss: 1.097 - ETA: 1:51 - loss: 1.096 - ETA: 1:49 - loss: 1.096 - ETA: 1:48 - loss: 1.097 - ETA: 1:46 - loss: 1.097 - ETA: 1:44 - loss: 1.097 - ETA: 1:43 - loss: 1.099 - ETA: 1:41 - loss: 1.099 - ETA: 1:40 - loss: 1.098 - ETA: 1:38 - loss: 1.098 - ETA: 1:36 - loss: 1.097 - ETA: 1:35 - loss: 1.098 - ETA: 1:33 - loss: 1.099 - ETA: 1:32 - loss: 1.100 - ETA: 1:30 - loss: 1.100 - ETA: 1:28 - loss: 1.100 - ETA: 1:27 - loss: 1.099 - ETA: 1:25 - loss: 1.100 - ETA: 1:24 - loss: 1.100 - ETA: 1:22 - loss: 1.099 - ETA: 1:21 - loss: 1.098 - ETA: 1:19 - loss: 1.097 - ETA: 1:17 - loss: 1.097 - ETA: 1:16 - loss: 1.097 - ETA: 1:14 - loss: 1.097 - ETA: 1:13 - loss: 1.099 - ETA: 1:11 - loss: 1.098 - ETA: 1:09 - loss: 1.099 - ETA: 1:08 - loss: 1.099 - ETA: 1:06 - loss: 1.098 - ETA: 1:05 - loss: 1.098 - ETA: 1:03 - loss: 1.098 - ETA: 1:01 - loss: 1.098 - ETA: 1:00 - loss: 1.097 - ETA: 58s - loss: 1.097 - ETA: 57s - loss: 1.09 - ETA: 55s - loss: 1.09 - ETA: 54s - loss: 1.09 - ETA: 52s - loss: 1.09 - ETA: 50s - loss: 1.09 - ETA: 49s - loss: 1.09 - ETA: 47s - loss: 1.09 - ETA: 46s - loss: 1.09 - ETA: 44s - loss: 1.09 - ETA: 42s - loss: 1.09 - ETA: 41s - loss: 1.09 - ETA: 39s - loss: 1.09 - ETA: 38s - loss: 1.09 - ETA: 36s - loss: 1.09 - ETA: 34s - loss: 1.09 - ETA: 33s - loss: 1.09 - ETA: 31s - loss: 1.09 - ETA: 30s - loss: 1.09 - ETA: 28s - loss: 1.10 - ETA: 27s - loss: 1.10 - ETA: 25s - loss: 1.09 - ETA: 23s - loss: 1.09 - ETA: 22s - loss: 1.09 - ETA: 20s - loss: 1.09 - ETA: 19s - loss: 1.09 - ETA: 17s - loss: 1.09 - ETA: 15s - loss: 1.09 - ETA: 14s - loss: 1.09 - ETA: 12s - loss: 1.09 - ETA: 11s - loss: 1.09 - ETA: 9s - loss: 1.0980 - ETA: 7s - loss: 1.098 - ETA: 6s - loss: 1.098 - ETA: 4s - loss: 1.098 - ETA: 3s - loss: 1.097 - ETA: 1s - loss: 1.096 - 299s 2s/step - loss: 1.0968 - val_loss: 1.3324\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.32666\n",
      "Epoch 12/20\n",
      "187/187 [==============================] - ETA: 4:56 - loss: 1.134 - ETA: 4:54 - loss: 1.101 - ETA: 4:53 - loss: 1.092 - ETA: 4:51 - loss: 1.107 - ETA: 4:49 - loss: 1.100 - ETA: 4:48 - loss: 1.122 - ETA: 4:46 - loss: 1.134 - ETA: 4:44 - loss: 1.118 - ETA: 4:42 - loss: 1.117 - ETA: 4:41 - loss: 1.123 - ETA: 4:39 - loss: 1.125 - ETA: 4:38 - loss: 1.124 - ETA: 4:36 - loss: 1.119 - ETA: 4:35 - loss: 1.134 - ETA: 4:33 - loss: 1.131 - ETA: 4:31 - loss: 1.132 - ETA: 4:30 - loss: 1.131 - ETA: 4:28 - loss: 1.130 - ETA: 4:26 - loss: 1.118 - ETA: 4:25 - loss: 1.108 - ETA: 4:23 - loss: 1.115 - ETA: 4:22 - loss: 1.118 - ETA: 4:20 - loss: 1.115 - ETA: 4:18 - loss: 1.113 - ETA: 4:17 - loss: 1.117 - ETA: 4:15 - loss: 1.118 - ETA: 4:14 - loss: 1.116 - ETA: 4:12 - loss: 1.116 - ETA: 4:10 - loss: 1.117 - ETA: 4:09 - loss: 1.118 - ETA: 4:07 - loss: 1.114 - ETA: 4:06 - loss: 1.113 - ETA: 4:04 - loss: 1.109 - ETA: 4:02 - loss: 1.107 - ETA: 4:01 - loss: 1.110 - ETA: 3:59 - loss: 1.105 - ETA: 3:58 - loss: 1.108 - ETA: 3:56 - loss: 1.107 - ETA: 3:54 - loss: 1.111 - ETA: 3:53 - loss: 1.112 - ETA: 3:51 - loss: 1.115 - ETA: 3:50 - loss: 1.114 - ETA: 3:48 - loss: 1.112 - ETA: 3:47 - loss: 1.113 - ETA: 3:45 - loss: 1.113 - ETA: 3:43 - loss: 1.114 - ETA: 3:42 - loss: 1.114 - ETA: 3:40 - loss: 1.115 - ETA: 3:39 - loss: 1.114 - ETA: 3:37 - loss: 1.116 - ETA: 3:36 - loss: 1.116 - ETA: 3:34 - loss: 1.114 - ETA: 3:32 - loss: 1.113 - ETA: 3:31 - loss: 1.113 - ETA: 3:29 - loss: 1.113 - ETA: 3:28 - loss: 1.113 - ETA: 3:26 - loss: 1.113 - ETA: 3:24 - loss: 1.113 - ETA: 3:23 - loss: 1.114 - ETA: 3:21 - loss: 1.116 - ETA: 3:20 - loss: 1.117 - ETA: 3:18 - loss: 1.118 - ETA: 3:16 - loss: 1.120 - ETA: 3:15 - loss: 1.119 - ETA: 3:13 - loss: 1.120 - ETA: 3:12 - loss: 1.117 - ETA: 3:10 - loss: 1.117 - ETA: 3:08 - loss: 1.116 - ETA: 3:07 - loss: 1.118 - ETA: 3:05 - loss: 1.116 - ETA: 3:04 - loss: 1.116 - ETA: 3:02 - loss: 1.116 - ETA: 3:01 - loss: 1.117 - ETA: 2:59 - loss: 1.116 - ETA: 2:57 - loss: 1.113 - ETA: 2:56 - loss: 1.115 - ETA: 2:54 - loss: 1.115 - ETA: 2:53 - loss: 1.116 - ETA: 2:51 - loss: 1.114 - ETA: 2:49 - loss: 1.113 - ETA: 2:48 - loss: 1.113 - ETA: 2:46 - loss: 1.113 - ETA: 2:45 - loss: 1.114 - ETA: 2:43 - loss: 1.112 - ETA: 2:41 - loss: 1.113 - ETA: 2:40 - loss: 1.114 - ETA: 2:38 - loss: 1.112 - ETA: 2:37 - loss: 1.115 - ETA: 2:35 - loss: 1.113 - ETA: 2:34 - loss: 1.110 - ETA: 2:32 - loss: 1.110 - ETA: 2:30 - loss: 1.110 - ETA: 2:29 - loss: 1.110 - ETA: 2:27 - loss: 1.110 - ETA: 2:26 - loss: 1.112 - ETA: 2:24 - loss: 1.112 - ETA: 2:22 - loss: 1.114 - ETA: 2:21 - loss: 1.114 - ETA: 2:19 - loss: 1.114 - ETA: 2:18 - loss: 1.112 - ETA: 2:16 - loss: 1.114 - ETA: 2:14 - loss: 1.114 - ETA: 2:13 - loss: 1.113 - ETA: 2:11 - loss: 1.113 - ETA: 2:10 - loss: 1.114 - ETA: 2:08 - loss: 1.116 - ETA: 2:07 - loss: 1.117 - ETA: 2:05 - loss: 1.118 - ETA: 2:03 - loss: 1.119 - ETA: 2:02 - loss: 1.119 - ETA: 2:00 - loss: 1.121 - ETA: 1:59 - loss: 1.119 - ETA: 1:57 - loss: 1.118 - ETA: 1:55 - loss: 1.119 - ETA: 1:54 - loss: 1.119 - ETA: 1:52 - loss: 1.119 - ETA: 1:51 - loss: 1.120 - ETA: 1:49 - loss: 1.119 - ETA: 1:47 - loss: 1.118 - ETA: 1:46 - loss: 1.119 - ETA: 1:44 - loss: 1.119 - ETA: 1:43 - loss: 1.118 - ETA: 1:41 - loss: 1.117 - ETA: 1:40 - loss: 1.115 - ETA: 1:38 - loss: 1.115 - ETA: 1:36 - loss: 1.116 - ETA: 1:35 - loss: 1.117 - ETA: 1:33 - loss: 1.116 - ETA: 1:32 - loss: 1.115 - ETA: 1:30 - loss: 1.114 - ETA: 1:28 - loss: 1.114 - ETA: 1:27 - loss: 1.115 - ETA: 1:25 - loss: 1.114 - ETA: 1:24 - loss: 1.114 - ETA: 1:22 - loss: 1.113 - ETA: 1:21 - loss: 1.113 - ETA: 1:19 - loss: 1.114 - ETA: 1:17 - loss: 1.113 - ETA: 1:16 - loss: 1.114 - ETA: 1:14 - loss: 1.113 - ETA: 1:13 - loss: 1.113 - ETA: 1:11 - loss: 1.114 - ETA: 1:09 - loss: 1.114 - ETA: 1:08 - loss: 1.114 - ETA: 1:06 - loss: 1.113 - ETA: 1:05 - loss: 1.114 - ETA: 1:03 - loss: 1.113 - ETA: 1:01 - loss: 1.113 - ETA: 1:00 - loss: 1.112 - ETA: 58s - loss: 1.113 - ETA: 57s - loss: 1.11 - ETA: 55s - loss: 1.11 - ETA: 54s - loss: 1.11 - ETA: 52s - loss: 1.11 - ETA: 50s - loss: 1.11 - ETA: 49s - loss: 1.11 - ETA: 47s - loss: 1.11 - ETA: 46s - loss: 1.11 - ETA: 44s - loss: 1.11 - ETA: 42s - loss: 1.11 - ETA: 41s - loss: 1.11 - ETA: 39s - loss: 1.11 - ETA: 38s - loss: 1.11 - ETA: 36s - loss: 1.11 - ETA: 34s - loss: 1.11 - ETA: 33s - loss: 1.11 - ETA: 31s - loss: 1.11 - ETA: 30s - loss: 1.11 - ETA: 28s - loss: 1.11 - ETA: 27s - loss: 1.10 - ETA: 25s - loss: 1.10 - ETA: 23s - loss: 1.10 - ETA: 22s - loss: 1.10 - ETA: 20s - loss: 1.10 - ETA: 19s - loss: 1.10 - ETA: 17s - loss: 1.10 - ETA: 15s - loss: 1.10 - ETA: 14s - loss: 1.10 - ETA: 12s - loss: 1.10 - ETA: 11s - loss: 1.10 - ETA: 9s - loss: 1.1073 - ETA: 7s - loss: 1.106 - ETA: 6s - loss: 1.105 - ETA: 4s - loss: 1.106 - ETA: 3s - loss: 1.107 - ETA: 1s - loss: 1.107 - 299s 2s/step - loss: 1.1074 - val_loss: 1.4425\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.32666\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:53 - loss: 1.130 - ETA: 4:52 - loss: 1.120 - ETA: 4:51 - loss: 1.107 - ETA: 4:50 - loss: 1.100 - ETA: 4:48 - loss: 1.076 - ETA: 4:47 - loss: 1.082 - ETA: 4:45 - loss: 1.069 - ETA: 4:44 - loss: 1.080 - ETA: 4:42 - loss: 1.080 - ETA: 4:41 - loss: 1.076 - ETA: 4:39 - loss: 1.075 - ETA: 4:38 - loss: 1.062 - ETA: 4:36 - loss: 1.062 - ETA: 4:34 - loss: 1.068 - ETA: 4:33 - loss: 1.072 - ETA: 4:31 - loss: 1.074 - ETA: 4:30 - loss: 1.071 - ETA: 4:28 - loss: 1.071 - ETA: 4:26 - loss: 1.076 - ETA: 4:25 - loss: 1.081 - ETA: 4:23 - loss: 1.090 - ETA: 4:22 - loss: 1.091 - ETA: 4:20 - loss: 1.088 - ETA: 4:19 - loss: 1.084 - ETA: 4:17 - loss: 1.074 - ETA: 4:15 - loss: 1.076 - ETA: 4:14 - loss: 1.070 - ETA: 4:12 - loss: 1.067 - ETA: 4:11 - loss: 1.064 - ETA: 4:09 - loss: 1.062 - ETA: 4:07 - loss: 1.065 - ETA: 4:06 - loss: 1.064 - ETA: 4:04 - loss: 1.064 - ETA: 4:02 - loss: 1.066 - ETA: 4:01 - loss: 1.064 - ETA: 3:59 - loss: 1.064 - ETA: 3:58 - loss: 1.064 - ETA: 3:56 - loss: 1.065 - ETA: 3:55 - loss: 1.065 - ETA: 3:53 - loss: 1.063 - ETA: 3:51 - loss: 1.062 - ETA: 3:50 - loss: 1.060 - ETA: 3:48 - loss: 1.059 - ETA: 3:47 - loss: 1.060 - ETA: 3:45 - loss: 1.061 - ETA: 3:43 - loss: 1.062 - ETA: 3:42 - loss: 1.064 - ETA: 3:40 - loss: 1.064 - ETA: 3:39 - loss: 1.067 - ETA: 3:37 - loss: 1.069 - ETA: 3:35 - loss: 1.073 - ETA: 3:34 - loss: 1.071 - ETA: 3:32 - loss: 1.070 - ETA: 3:31 - loss: 1.075 - ETA: 3:29 - loss: 1.074 - ETA: 3:28 - loss: 1.072 - ETA: 3:26 - loss: 1.071 - ETA: 3:24 - loss: 1.070 - ETA: 3:23 - loss: 1.071 - ETA: 3:21 - loss: 1.071 - ETA: 3:20 - loss: 1.071 - ETA: 3:18 - loss: 1.070 - ETA: 3:16 - loss: 1.071 - ETA: 3:15 - loss: 1.072 - ETA: 3:13 - loss: 1.072 - ETA: 3:12 - loss: 1.071 - ETA: 3:10 - loss: 1.071 - ETA: 3:08 - loss: 1.070 - ETA: 3:07 - loss: 1.067 - ETA: 3:05 - loss: 1.070 - ETA: 3:04 - loss: 1.069 - ETA: 3:02 - loss: 1.070 - ETA: 3:01 - loss: 1.070 - ETA: 2:59 - loss: 1.072 - ETA: 2:57 - loss: 1.072 - ETA: 2:56 - loss: 1.074 - ETA: 2:54 - loss: 1.074 - ETA: 2:53 - loss: 1.074 - ETA: 2:51 - loss: 1.074 - ETA: 2:49 - loss: 1.073 - ETA: 2:48 - loss: 1.075 - ETA: 2:46 - loss: 1.076 - ETA: 2:45 - loss: 1.077 - ETA: 2:43 - loss: 1.077 - ETA: 2:41 - loss: 1.080 - ETA: 2:40 - loss: 1.080 - ETA: 2:38 - loss: 1.082 - ETA: 2:37 - loss: 1.082 - ETA: 2:35 - loss: 1.082 - ETA: 2:34 - loss: 1.080 - ETA: 2:32 - loss: 1.080 - ETA: 2:30 - loss: 1.082 - ETA: 2:29 - loss: 1.081 - ETA: 2:27 - loss: 1.081 - ETA: 2:26 - loss: 1.082 - ETA: 2:24 - loss: 1.081 - ETA: 2:22 - loss: 1.082 - ETA: 2:21 - loss: 1.083 - ETA: 2:19 - loss: 1.082 - ETA: 2:18 - loss: 1.082 - ETA: 2:16 - loss: 1.080 - ETA: 2:14 - loss: 1.080 - ETA: 2:13 - loss: 1.080 - ETA: 2:11 - loss: 1.080 - ETA: 2:10 - loss: 1.080 - ETA: 2:08 - loss: 1.079 - ETA: 2:07 - loss: 1.079 - ETA: 2:05 - loss: 1.079 - ETA: 2:03 - loss: 1.078 - ETA: 2:02 - loss: 1.077 - ETA: 2:00 - loss: 1.078 - ETA: 1:59 - loss: 1.078 - ETA: 1:57 - loss: 1.078 - ETA: 1:55 - loss: 1.078 - ETA: 1:54 - loss: 1.078 - ETA: 1:52 - loss: 1.077 - ETA: 1:51 - loss: 1.076 - ETA: 1:49 - loss: 1.077 - ETA: 1:47 - loss: 1.077 - ETA: 1:46 - loss: 1.076 - ETA: 1:44 - loss: 1.075 - ETA: 1:43 - loss: 1.076 - ETA: 1:41 - loss: 1.075 - ETA: 1:40 - loss: 1.076 - ETA: 1:38 - loss: 1.076 - ETA: 1:36 - loss: 1.076 - ETA: 1:35 - loss: 1.076 - ETA: 1:33 - loss: 1.076 - ETA: 1:32 - loss: 1.075 - ETA: 1:30 - loss: 1.075 - ETA: 1:28 - loss: 1.074 - ETA: 1:27 - loss: 1.075 - ETA: 1:25 - loss: 1.075 - ETA: 1:24 - loss: 1.074 - ETA: 1:22 - loss: 1.075 - ETA: 1:20 - loss: 1.075 - ETA: 1:19 - loss: 1.074 - ETA: 1:17 - loss: 1.073 - ETA: 1:16 - loss: 1.072 - ETA: 1:14 - loss: 1.071 - ETA: 1:13 - loss: 1.072 - ETA: 1:11 - loss: 1.073 - ETA: 1:09 - loss: 1.073 - ETA: 1:08 - loss: 1.071 - ETA: 1:06 - loss: 1.072 - ETA: 1:05 - loss: 1.072 - ETA: 1:03 - loss: 1.073 - ETA: 1:01 - loss: 1.072 - ETA: 1:00 - loss: 1.071 - ETA: 58s - loss: 1.071 - ETA: 57s - loss: 1.07 - ETA: 55s - loss: 1.07 - ETA: 53s - loss: 1.07 - ETA: 52s - loss: 1.07 - ETA: 50s - loss: 1.07 - ETA: 49s - loss: 1.07 - ETA: 47s - loss: 1.07 - ETA: 46s - loss: 1.07 - ETA: 44s - loss: 1.07 - ETA: 42s - loss: 1.07 - ETA: 41s - loss: 1.07 - ETA: 39s - loss: 1.07 - ETA: 38s - loss: 1.07 - ETA: 36s - loss: 1.07 - ETA: 34s - loss: 1.07 - ETA: 33s - loss: 1.07 - ETA: 31s - loss: 1.07 - ETA: 30s - loss: 1.07 - ETA: 28s - loss: 1.07 - ETA: 26s - loss: 1.07 - ETA: 25s - loss: 1.07 - ETA: 23s - loss: 1.07 - ETA: 22s - loss: 1.07 - ETA: 20s - loss: 1.07 - ETA: 19s - loss: 1.07 - ETA: 17s - loss: 1.07 - ETA: 15s - loss: 1.07 - ETA: 14s - loss: 1.07 - ETA: 12s - loss: 1.07 - ETA: 11s - loss: 1.07 - ETA: 9s - loss: 1.0707 - ETA: 7s - loss: 1.070 - ETA: 6s - loss: 1.070 - ETA: 4s - loss: 1.071 - ETA: 3s - loss: 1.070 - ETA: 1s - loss: 1.071 - 299s 2s/step - loss: 1.0699 - val_loss: 1.3679\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.32666\n",
      "Epoch 14/20\n",
      "187/187 [==============================] - ETA: 4:55 - loss: 1.016 - ETA: 4:53 - loss: 1.017 - ETA: 4:52 - loss: 1.044 - ETA: 4:50 - loss: 1.031 - ETA: 4:48 - loss: 1.028 - ETA: 4:46 - loss: 1.050 - ETA: 4:45 - loss: 1.029 - ETA: 4:44 - loss: 1.039 - ETA: 4:42 - loss: 1.052 - ETA: 4:41 - loss: 1.037 - ETA: 4:39 - loss: 1.050 - ETA: 4:38 - loss: 1.060 - ETA: 4:36 - loss: 1.055 - ETA: 4:34 - loss: 1.050 - ETA: 4:33 - loss: 1.049 - ETA: 4:31 - loss: 1.058 - ETA: 4:30 - loss: 1.058 - ETA: 4:28 - loss: 1.058 - ETA: 4:27 - loss: 1.070 - ETA: 4:25 - loss: 1.077 - ETA: 4:23 - loss: 1.077 - ETA: 4:22 - loss: 1.079 - ETA: 4:20 - loss: 1.077 - ETA: 4:18 - loss: 1.073 - ETA: 4:17 - loss: 1.070 - ETA: 4:15 - loss: 1.069 - ETA: 4:14 - loss: 1.073 - ETA: 4:12 - loss: 1.070 - ETA: 4:11 - loss: 1.071 - ETA: 4:09 - loss: 1.072 - ETA: 4:07 - loss: 1.067 - ETA: 4:06 - loss: 1.062 - ETA: 4:04 - loss: 1.059 - ETA: 4:03 - loss: 1.059 - ETA: 4:01 - loss: 1.056 - ETA: 4:00 - loss: 1.055 - ETA: 3:58 - loss: 1.051 - ETA: 3:56 - loss: 1.053 - ETA: 3:55 - loss: 1.051 - ETA: 3:53 - loss: 1.047 - ETA: 3:52 - loss: 1.049 - ETA: 3:50 - loss: 1.050 - ETA: 3:48 - loss: 1.055 - ETA: 3:47 - loss: 1.057 - ETA: 3:45 - loss: 1.055 - ETA: 3:44 - loss: 1.054 - ETA: 3:42 - loss: 1.051 - ETA: 3:41 - loss: 1.046 - ETA: 3:39 - loss: 1.046 - ETA: 3:37 - loss: 1.046 - ETA: 3:36 - loss: 1.047 - ETA: 3:34 - loss: 1.047 - ETA: 3:33 - loss: 1.048 - ETA: 3:31 - loss: 1.050 - ETA: 3:29 - loss: 1.050 - ETA: 3:28 - loss: 1.053 - ETA: 3:26 - loss: 1.052 - ETA: 3:25 - loss: 1.052 - ETA: 3:23 - loss: 1.050 - ETA: 3:21 - loss: 1.049 - ETA: 3:20 - loss: 1.050 - ETA: 3:18 - loss: 1.052 - ETA: 3:17 - loss: 1.054 - ETA: 3:15 - loss: 1.055 - ETA: 3:13 - loss: 1.054 - ETA: 3:12 - loss: 1.055 - ETA: 3:10 - loss: 1.055 - ETA: 3:09 - loss: 1.056 - ETA: 3:07 - loss: 1.056 - ETA: 3:05 - loss: 1.054 - ETA: 3:04 - loss: 1.054 - ETA: 3:02 - loss: 1.055 - ETA: 3:01 - loss: 1.054 - ETA: 2:59 - loss: 1.054 - ETA: 2:57 - loss: 1.054 - ETA: 2:56 - loss: 1.054 - ETA: 2:54 - loss: 1.058 - ETA: 2:53 - loss: 1.059 - ETA: 2:51 - loss: 1.062 - ETA: 2:50 - loss: 1.062 - ETA: 2:48 - loss: 1.061 - ETA: 2:46 - loss: 1.062 - ETA: 2:45 - loss: 1.063 - ETA: 2:43 - loss: 1.062 - ETA: 2:42 - loss: 1.061 - ETA: 2:40 - loss: 1.062 - ETA: 2:38 - loss: 1.061 - ETA: 2:37 - loss: 1.060 - ETA: 2:35 - loss: 1.062 - ETA: 2:34 - loss: 1.062 - ETA: 2:32 - loss: 1.063 - ETA: 2:30 - loss: 1.062 - ETA: 2:29 - loss: 1.062 - ETA: 2:27 - loss: 1.063 - ETA: 2:26 - loss: 1.061 - ETA: 2:24 - loss: 1.060 - ETA: 2:22 - loss: 1.060 - ETA: 2:21 - loss: 1.061 - ETA: 2:19 - loss: 1.061 - ETA: 2:18 - loss: 1.061 - ETA: 2:16 - loss: 1.061 - ETA: 2:15 - loss: 1.060 - ETA: 2:13 - loss: 1.060 - ETA: 2:11 - loss: 1.062 - ETA: 2:10 - loss: 1.063 - ETA: 2:08 - loss: 1.062 - ETA: 2:07 - loss: 1.062 - ETA: 2:05 - loss: 1.062 - ETA: 2:03 - loss: 1.062 - ETA: 2:02 - loss: 1.062 - ETA: 2:00 - loss: 1.062 - ETA: 1:59 - loss: 1.062 - ETA: 1:57 - loss: 1.064 - ETA: 1:55 - loss: 1.065 - ETA: 1:54 - loss: 1.064 - ETA: 1:52 - loss: 1.063 - ETA: 1:51 - loss: 1.063 - ETA: 1:49 - loss: 1.064 - ETA: 1:48 - loss: 1.064 - ETA: 1:46 - loss: 1.063 - ETA: 1:44 - loss: 1.063 - ETA: 1:43 - loss: 1.062 - ETA: 1:41 - loss: 1.063 - ETA: 1:40 - loss: 1.064 - ETA: 1:38 - loss: 1.062 - ETA: 1:36 - loss: 1.063 - ETA: 1:35 - loss: 1.062 - ETA: 1:33 - loss: 1.061 - ETA: 1:32 - loss: 1.062 - ETA: 1:30 - loss: 1.062 - ETA: 1:28 - loss: 1.062 - ETA: 1:27 - loss: 1.061 - ETA: 1:25 - loss: 1.060 - ETA: 1:24 - loss: 1.060 - ETA: 1:22 - loss: 1.060 - ETA: 1:21 - loss: 1.060 - ETA: 1:19 - loss: 1.060 - ETA: 1:17 - loss: 1.059 - ETA: 1:16 - loss: 1.060 - ETA: 1:14 - loss: 1.060 - ETA: 1:13 - loss: 1.061 - ETA: 1:11 - loss: 1.059 - ETA: 1:09 - loss: 1.058 - ETA: 1:08 - loss: 1.057 - ETA: 1:06 - loss: 1.058 - ETA: 1:05 - loss: 1.057 - ETA: 1:03 - loss: 1.056 - ETA: 1:01 - loss: 1.056 - ETA: 1:00 - loss: 1.055 - ETA: 58s - loss: 1.055 - ETA: 57s - loss: 1.05 - ETA: 55s - loss: 1.05 - ETA: 54s - loss: 1.05 - ETA: 52s - loss: 1.05 - ETA: 50s - loss: 1.05 - ETA: 49s - loss: 1.05 - ETA: 47s - loss: 1.05 - ETA: 46s - loss: 1.05 - ETA: 44s - loss: 1.05 - ETA: 42s - loss: 1.05 - ETA: 41s - loss: 1.05 - ETA: 39s - loss: 1.05 - ETA: 38s - loss: 1.05 - ETA: 36s - loss: 1.05 - ETA: 34s - loss: 1.05 - ETA: 33s - loss: 1.05 - ETA: 31s - loss: 1.05 - ETA: 30s - loss: 1.05 - ETA: 28s - loss: 1.05 - ETA: 27s - loss: 1.05 - ETA: 25s - loss: 1.05 - ETA: 23s - loss: 1.05 - ETA: 22s - loss: 1.05 - ETA: 20s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 17s - loss: 1.05 - ETA: 15s - loss: 1.06 - ETA: 14s - loss: 1.06 - ETA: 12s - loss: 1.05 - ETA: 11s - loss: 1.05 - ETA: 9s - loss: 1.0590 - ETA: 7s - loss: 1.058 - ETA: 6s - loss: 1.058 - ETA: 4s - loss: 1.058 - ETA: 3s - loss: 1.059 - ETA: 1s - loss: 1.059 - 299s 2s/step - loss: 1.0593 - val_loss: 1.3348\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.32666\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:57 - loss: 1.102 - ETA: 4:54 - loss: 1.039 - ETA: 4:52 - loss: 1.030 - ETA: 4:50 - loss: 1.003 - ETA: 4:49 - loss: 1.006 - ETA: 4:47 - loss: 1.007 - ETA: 4:45 - loss: 0.988 - ETA: 4:44 - loss: 1.021 - ETA: 4:42 - loss: 1.023 - ETA: 4:41 - loss: 1.016 - ETA: 4:39 - loss: 1.023 - ETA: 4:38 - loss: 1.020 - ETA: 4:36 - loss: 1.017 - ETA: 4:35 - loss: 1.032 - ETA: 4:33 - loss: 1.044 - ETA: 4:31 - loss: 1.047 - ETA: 4:30 - loss: 1.045 - ETA: 4:28 - loss: 1.051 - ETA: 4:27 - loss: 1.058 - ETA: 4:25 - loss: 1.059 - ETA: 4:23 - loss: 1.059 - ETA: 4:22 - loss: 1.054 - ETA: 4:20 - loss: 1.057 - ETA: 4:19 - loss: 1.063 - ETA: 4:17 - loss: 1.062 - ETA: 4:15 - loss: 1.057 - ETA: 4:14 - loss: 1.054 - ETA: 4:12 - loss: 1.049 - ETA: 4:11 - loss: 1.052 - ETA: 4:09 - loss: 1.054 - ETA: 4:07 - loss: 1.050 - ETA: 4:06 - loss: 1.045 - ETA: 4:04 - loss: 1.047 - ETA: 4:03 - loss: 1.041 - ETA: 4:01 - loss: 1.039 - ETA: 3:59 - loss: 1.043 - ETA: 3:58 - loss: 1.046 - ETA: 3:56 - loss: 1.044 - ETA: 3:55 - loss: 1.045 - ETA: 3:53 - loss: 1.046 - ETA: 3:52 - loss: 1.045 - ETA: 3:50 - loss: 1.044 - ETA: 3:48 - loss: 1.046 - ETA: 3:47 - loss: 1.047 - ETA: 3:45 - loss: 1.048 - ETA: 3:44 - loss: 1.047 - ETA: 3:42 - loss: 1.046 - ETA: 3:40 - loss: 1.046 - ETA: 3:39 - loss: 1.048 - ETA: 3:37 - loss: 1.048 - ETA: 3:36 - loss: 1.048 - ETA: 3:34 - loss: 1.047 - ETA: 3:32 - loss: 1.045 - ETA: 3:31 - loss: 1.045 - ETA: 3:29 - loss: 1.048 - ETA: 3:28 - loss: 1.051 - ETA: 3:26 - loss: 1.051 - ETA: 3:24 - loss: 1.049 - ETA: 3:23 - loss: 1.048 - ETA: 3:21 - loss: 1.050 - ETA: 3:20 - loss: 1.048 - ETA: 3:18 - loss: 1.051 - ETA: 3:16 - loss: 1.053 - ETA: 3:15 - loss: 1.053 - ETA: 3:13 - loss: 1.052 - ETA: 3:12 - loss: 1.052 - ETA: 3:10 - loss: 1.052 - ETA: 3:08 - loss: 1.053 - ETA: 3:07 - loss: 1.054 - ETA: 3:05 - loss: 1.053 - ETA: 3:04 - loss: 1.051 - ETA: 3:02 - loss: 1.053 - ETA: 3:01 - loss: 1.054 - ETA: 2:59 - loss: 1.052 - ETA: 2:57 - loss: 1.052 - ETA: 2:56 - loss: 1.053 - ETA: 2:54 - loss: 1.052 - ETA: 2:53 - loss: 1.052 - ETA: 2:51 - loss: 1.052 - ETA: 2:49 - loss: 1.051 - ETA: 2:48 - loss: 1.049 - ETA: 2:46 - loss: 1.050 - ETA: 2:45 - loss: 1.049 - ETA: 2:43 - loss: 1.048 - ETA: 2:41 - loss: 1.050 - ETA: 2:40 - loss: 1.051 - ETA: 2:38 - loss: 1.050 - ETA: 2:37 - loss: 1.051 - ETA: 2:35 - loss: 1.050 - ETA: 2:34 - loss: 1.050 - ETA: 2:32 - loss: 1.050 - ETA: 2:30 - loss: 1.051 - ETA: 2:29 - loss: 1.051 - ETA: 2:27 - loss: 1.050 - ETA: 2:26 - loss: 1.049 - ETA: 2:24 - loss: 1.050 - ETA: 2:22 - loss: 1.050 - ETA: 2:21 - loss: 1.049 - ETA: 2:19 - loss: 1.048 - ETA: 2:18 - loss: 1.048 - ETA: 2:16 - loss: 1.049 - ETA: 2:15 - loss: 1.048 - ETA: 2:13 - loss: 1.048 - ETA: 2:11 - loss: 1.047 - ETA: 2:10 - loss: 1.048 - ETA: 2:08 - loss: 1.047 - ETA: 2:07 - loss: 1.047 - ETA: 2:05 - loss: 1.047 - ETA: 2:03 - loss: 1.046 - ETA: 2:02 - loss: 1.048 - ETA: 2:00 - loss: 1.049 - ETA: 1:59 - loss: 1.047 - ETA: 1:57 - loss: 1.047 - ETA: 1:55 - loss: 1.047 - ETA: 1:54 - loss: 1.048 - ETA: 1:52 - loss: 1.048 - ETA: 1:51 - loss: 1.047 - ETA: 1:49 - loss: 1.046 - ETA: 1:48 - loss: 1.046 - ETA: 1:46 - loss: 1.045 - ETA: 1:44 - loss: 1.046 - ETA: 1:43 - loss: 1.047 - ETA: 1:41 - loss: 1.047 - ETA: 1:40 - loss: 1.048 - ETA: 1:38 - loss: 1.047 - ETA: 1:36 - loss: 1.048 - ETA: 1:35 - loss: 1.048 - ETA: 1:33 - loss: 1.047 - ETA: 1:32 - loss: 1.049 - ETA: 1:30 - loss: 1.050 - ETA: 1:28 - loss: 1.050 - ETA: 1:27 - loss: 1.051 - ETA: 1:25 - loss: 1.050 - ETA: 1:24 - loss: 1.051 - ETA: 1:22 - loss: 1.051 - ETA: 1:21 - loss: 1.052 - ETA: 1:19 - loss: 1.051 - ETA: 1:17 - loss: 1.052 - ETA: 1:16 - loss: 1.052 - ETA: 1:14 - loss: 1.051 - ETA: 1:13 - loss: 1.051 - ETA: 1:11 - loss: 1.051 - ETA: 1:09 - loss: 1.050 - ETA: 1:08 - loss: 1.049 - ETA: 1:06 - loss: 1.049 - ETA: 1:05 - loss: 1.049 - ETA: 1:03 - loss: 1.049 - ETA: 1:01 - loss: 1.050 - ETA: 1:00 - loss: 1.050 - ETA: 58s - loss: 1.050 - ETA: 57s - loss: 1.05 - ETA: 55s - loss: 1.04 - ETA: 54s - loss: 1.04 - ETA: 52s - loss: 1.05 - ETA: 50s - loss: 1.04 - ETA: 49s - loss: 1.04 - ETA: 47s - loss: 1.04 - ETA: 46s - loss: 1.04 - ETA: 44s - loss: 1.05 - ETA: 42s - loss: 1.04 - ETA: 41s - loss: 1.04 - ETA: 39s - loss: 1.04 - ETA: 38s - loss: 1.04 - ETA: 36s - loss: 1.04 - ETA: 34s - loss: 1.04 - ETA: 33s - loss: 1.04 - ETA: 31s - loss: 1.04 - ETA: 30s - loss: 1.04 - ETA: 28s - loss: 1.04 - ETA: 27s - loss: 1.05 - ETA: 25s - loss: 1.05 - ETA: 23s - loss: 1.05 - ETA: 22s - loss: 1.05 - ETA: 20s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 17s - loss: 1.05 - ETA: 15s - loss: 1.05 - ETA: 14s - loss: 1.05 - ETA: 12s - loss: 1.05 - ETA: 11s - loss: 1.05 - ETA: 9s - loss: 1.0587 - ETA: 7s - loss: 1.058 - ETA: 6s - loss: 1.058 - ETA: 4s - loss: 1.058 - ETA: 3s - loss: 1.058 - ETA: 1s - loss: 1.059 - 299s 2s/step - loss: 1.0589 - val_loss: 1.2677\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.32666 to 1.26773, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 16/20\n",
      "187/187 [==============================] - ETA: 4:54 - loss: 1.073 - ETA: 4:53 - loss: 1.152 - ETA: 4:51 - loss: 1.105 - ETA: 4:50 - loss: 1.114 - ETA: 4:49 - loss: 1.097 - ETA: 4:47 - loss: 1.088 - ETA: 4:46 - loss: 1.078 - ETA: 4:44 - loss: 1.087 - ETA: 4:42 - loss: 1.072 - ETA: 4:41 - loss: 1.076 - ETA: 4:39 - loss: 1.085 - ETA: 4:38 - loss: 1.075 - ETA: 4:36 - loss: 1.079 - ETA: 4:34 - loss: 1.079 - ETA: 4:33 - loss: 1.072 - ETA: 4:31 - loss: 1.061 - ETA: 4:30 - loss: 1.058 - ETA: 4:28 - loss: 1.053 - ETA: 4:27 - loss: 1.064 - ETA: 4:25 - loss: 1.062 - ETA: 4:23 - loss: 1.063 - ETA: 4:22 - loss: 1.061 - ETA: 4:20 - loss: 1.064 - ETA: 4:19 - loss: 1.070 - ETA: 4:17 - loss: 1.065 - ETA: 4:15 - loss: 1.064 - ETA: 4:14 - loss: 1.057 - ETA: 4:12 - loss: 1.058 - ETA: 4:11 - loss: 1.055 - ETA: 4:09 - loss: 1.054 - ETA: 4:08 - loss: 1.053 - ETA: 4:06 - loss: 1.060 - ETA: 4:04 - loss: 1.068 - ETA: 4:03 - loss: 1.067 - ETA: 4:01 - loss: 1.067 - ETA: 4:00 - loss: 1.065 - ETA: 3:58 - loss: 1.060 - ETA: 3:56 - loss: 1.059 - ETA: 3:55 - loss: 1.060 - ETA: 3:53 - loss: 1.059 - ETA: 3:52 - loss: 1.053 - ETA: 3:50 - loss: 1.057 - ETA: 3:48 - loss: 1.053 - ETA: 3:47 - loss: 1.049 - ETA: 3:45 - loss: 1.046 - ETA: 3:44 - loss: 1.046 - ETA: 3:42 - loss: 1.046 - ETA: 3:40 - loss: 1.042 - ETA: 3:39 - loss: 1.039 - ETA: 3:37 - loss: 1.038 - ETA: 3:36 - loss: 1.038 - ETA: 3:34 - loss: 1.040 - ETA: 3:32 - loss: 1.036 - ETA: 3:31 - loss: 1.036 - ETA: 3:29 - loss: 1.034 - ETA: 3:28 - loss: 1.033 - ETA: 3:26 - loss: 1.031 - ETA: 3:25 - loss: 1.034 - ETA: 3:23 - loss: 1.033 - ETA: 3:21 - loss: 1.035 - ETA: 3:20 - loss: 1.035 - ETA: 3:18 - loss: 1.037 - ETA: 3:17 - loss: 1.036 - ETA: 3:15 - loss: 1.036 - ETA: 3:13 - loss: 1.036 - ETA: 3:12 - loss: 1.034 - ETA: 3:10 - loss: 1.035 - ETA: 3:09 - loss: 1.037 - ETA: 3:07 - loss: 1.038 - ETA: 3:05 - loss: 1.038 - ETA: 3:04 - loss: 1.038 - ETA: 3:02 - loss: 1.039 - ETA: 3:01 - loss: 1.040 - ETA: 2:59 - loss: 1.041 - ETA: 2:57 - loss: 1.043 - ETA: 2:56 - loss: 1.041 - ETA: 2:54 - loss: 1.041 - ETA: 2:53 - loss: 1.041 - ETA: 2:51 - loss: 1.040 - ETA: 2:50 - loss: 1.041 - ETA: 2:48 - loss: 1.041 - ETA: 2:46 - loss: 1.043 - ETA: 2:45 - loss: 1.042 - ETA: 2:43 - loss: 1.043 - ETA: 2:42 - loss: 1.044 - ETA: 2:40 - loss: 1.044 - ETA: 2:38 - loss: 1.045 - ETA: 2:37 - loss: 1.044 - ETA: 2:35 - loss: 1.046 - ETA: 2:34 - loss: 1.046 - ETA: 2:32 - loss: 1.047 - ETA: 2:30 - loss: 1.048 - ETA: 2:29 - loss: 1.045 - ETA: 2:27 - loss: 1.045 - ETA: 2:26 - loss: 1.043 - ETA: 2:24 - loss: 1.043 - ETA: 2:23 - loss: 1.046 - ETA: 2:21 - loss: 1.044 - ETA: 2:19 - loss: 1.043 - ETA: 2:18 - loss: 1.044 - ETA: 2:16 - loss: 1.044 - ETA: 2:15 - loss: 1.046 - ETA: 2:13 - loss: 1.046 - ETA: 2:11 - loss: 1.046 - ETA: 2:10 - loss: 1.048 - ETA: 2:08 - loss: 1.048 - ETA: 2:07 - loss: 1.047 - ETA: 2:05 - loss: 1.048 - ETA: 2:03 - loss: 1.048 - ETA: 2:02 - loss: 1.049 - ETA: 2:00 - loss: 1.048 - ETA: 1:59 - loss: 1.048 - ETA: 1:57 - loss: 1.046 - ETA: 1:55 - loss: 1.047 - ETA: 1:54 - loss: 1.047 - ETA: 1:52 - loss: 1.048 - ETA: 1:51 - loss: 1.050 - ETA: 1:49 - loss: 1.050 - ETA: 1:48 - loss: 1.049 - ETA: 1:46 - loss: 1.050 - ETA: 1:44 - loss: 1.049 - ETA: 1:43 - loss: 1.049 - ETA: 1:41 - loss: 1.049 - ETA: 1:40 - loss: 1.050 - ETA: 1:38 - loss: 1.051 - ETA: 1:36 - loss: 1.050 - ETA: 1:35 - loss: 1.051 - ETA: 1:33 - loss: 1.052 - ETA: 1:32 - loss: 1.052 - ETA: 1:30 - loss: 1.053 - ETA: 1:28 - loss: 1.053 - ETA: 1:27 - loss: 1.054 - ETA: 1:25 - loss: 1.054 - ETA: 1:24 - loss: 1.055 - ETA: 1:22 - loss: 1.055 - ETA: 1:21 - loss: 1.054 - ETA: 1:19 - loss: 1.053 - ETA: 1:17 - loss: 1.054 - ETA: 1:16 - loss: 1.053 - ETA: 1:14 - loss: 1.053 - ETA: 1:13 - loss: 1.054 - ETA: 1:11 - loss: 1.055 - ETA: 1:09 - loss: 1.055 - ETA: 1:08 - loss: 1.056 - ETA: 1:06 - loss: 1.057 - ETA: 1:05 - loss: 1.056 - ETA: 1:03 - loss: 1.056 - ETA: 1:01 - loss: 1.056 - ETA: 1:00 - loss: 1.057 - ETA: 58s - loss: 1.057 - ETA: 57s - loss: 1.05 - ETA: 55s - loss: 1.05 - ETA: 54s - loss: 1.05 - ETA: 52s - loss: 1.05 - ETA: 50s - loss: 1.05 - ETA: 49s - loss: 1.05 - ETA: 47s - loss: 1.05 - ETA: 46s - loss: 1.05 - ETA: 44s - loss: 1.05 - ETA: 42s - loss: 1.05 - ETA: 41s - loss: 1.05 - ETA: 39s - loss: 1.05 - ETA: 38s - loss: 1.05 - ETA: 36s - loss: 1.05 - ETA: 34s - loss: 1.05 - ETA: 33s - loss: 1.05 - ETA: 31s - loss: 1.05 - ETA: 30s - loss: 1.05 - ETA: 28s - loss: 1.05 - ETA: 27s - loss: 1.05 - ETA: 25s - loss: 1.05 - ETA: 23s - loss: 1.05 - ETA: 22s - loss: 1.05 - ETA: 20s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 17s - loss: 1.05 - ETA: 15s - loss: 1.05 - ETA: 14s - loss: 1.05 - ETA: 12s - loss: 1.05 - ETA: 11s - loss: 1.05 - ETA: 9s - loss: 1.0524 - ETA: 7s - loss: 1.052 - ETA: 6s - loss: 1.051 - ETA: 4s - loss: 1.051 - ETA: 3s - loss: 1.051 - ETA: 1s - loss: 1.051 - 299s 2s/step - loss: 1.0524 - val_loss: 1.2484\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.26773 to 1.24836, saving model to ./weights/VGG16_LSTM_flickr8k_2l_32b_bn_dr_attn_bahdanau.hdf5\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:53 - loss: 0.919 - ETA: 4:53 - loss: 1.017 - ETA: 4:51 - loss: 1.022 - ETA: 4:49 - loss: 1.029 - ETA: 4:48 - loss: 1.033 - ETA: 4:46 - loss: 1.042 - ETA: 4:44 - loss: 1.074 - ETA: 4:43 - loss: 1.076 - ETA: 4:41 - loss: 1.074 - ETA: 4:40 - loss: 1.076 - ETA: 4:39 - loss: 1.061 - ETA: 4:37 - loss: 1.064 - ETA: 4:36 - loss: 1.065 - ETA: 4:34 - loss: 1.053 - ETA: 4:32 - loss: 1.052 - ETA: 4:31 - loss: 1.048 - ETA: 4:29 - loss: 1.063 - ETA: 4:28 - loss: 1.062 - ETA: 4:26 - loss: 1.074 - ETA: 4:24 - loss: 1.079 - ETA: 4:23 - loss: 1.076 - ETA: 4:21 - loss: 1.079 - ETA: 4:20 - loss: 1.073 - ETA: 4:18 - loss: 1.066 - ETA: 4:17 - loss: 1.057 - ETA: 4:15 - loss: 1.060 - ETA: 4:13 - loss: 1.058 - ETA: 4:12 - loss: 1.059 - ETA: 4:10 - loss: 1.058 - ETA: 4:09 - loss: 1.062 - ETA: 4:07 - loss: 1.060 - ETA: 4:06 - loss: 1.066 - ETA: 4:04 - loss: 1.065 - ETA: 4:02 - loss: 1.072 - ETA: 4:01 - loss: 1.076 - ETA: 3:59 - loss: 1.078 - ETA: 3:58 - loss: 1.076 - ETA: 3:56 - loss: 1.077 - ETA: 3:54 - loss: 1.075 - ETA: 3:53 - loss: 1.075 - ETA: 3:51 - loss: 1.076 - ETA: 3:50 - loss: 1.075 - ETA: 3:48 - loss: 1.074 - ETA: 3:47 - loss: 1.075 - ETA: 3:45 - loss: 1.074 - ETA: 3:43 - loss: 1.075 - ETA: 3:42 - loss: 1.073 - ETA: 3:40 - loss: 1.074 - ETA: 3:39 - loss: 1.072 - ETA: 3:37 - loss: 1.072 - ETA: 3:35 - loss: 1.070 - ETA: 3:34 - loss: 1.068 - ETA: 3:32 - loss: 1.066 - ETA: 3:31 - loss: 1.063 - ETA: 3:29 - loss: 1.064 - ETA: 3:28 - loss: 1.063 - ETA: 3:26 - loss: 1.064 - ETA: 3:24 - loss: 1.062 - ETA: 3:23 - loss: 1.062 - ETA: 3:21 - loss: 1.060 - ETA: 3:20 - loss: 1.060 - ETA: 3:18 - loss: 1.061 - ETA: 3:16 - loss: 1.061 - ETA: 3:15 - loss: 1.062 - ETA: 3:13 - loss: 1.061 - ETA: 3:12 - loss: 1.061 - ETA: 3:10 - loss: 1.060 - ETA: 3:09 - loss: 1.061 - ETA: 3:07 - loss: 1.062 - ETA: 3:05 - loss: 1.061 - ETA: 3:04 - loss: 1.061 - ETA: 3:02 - loss: 1.062 - ETA: 3:01 - loss: 1.062 - ETA: 2:59 - loss: 1.060 - ETA: 2:57 - loss: 1.061 - ETA: 2:56 - loss: 1.060 - ETA: 2:54 - loss: 1.060 - ETA: 2:53 - loss: 1.061 - ETA: 2:51 - loss: 1.059 - ETA: 2:49 - loss: 1.060 - ETA: 2:48 - loss: 1.059 - ETA: 2:46 - loss: 1.058 - ETA: 2:45 - loss: 1.060 - ETA: 2:43 - loss: 1.062 - ETA: 2:42 - loss: 1.061 - ETA: 2:40 - loss: 1.060 - ETA: 2:38 - loss: 1.060 - ETA: 2:37 - loss: 1.059 - ETA: 2:35 - loss: 1.060 - ETA: 2:34 - loss: 1.060 - ETA: 2:32 - loss: 1.063 - ETA: 2:30 - loss: 1.062 - ETA: 2:29 - loss: 1.061 - ETA: 2:27 - loss: 1.060 - ETA: 2:26 - loss: 1.060 - ETA: 2:24 - loss: 1.059 - ETA: 2:22 - loss: 1.058 - ETA: 2:21 - loss: 1.057 - ETA: 2:19 - loss: 1.055 - ETA: 2:18 - loss: 1.054 - ETA: 2:16 - loss: 1.054 - ETA: 2:15 - loss: 1.055 - ETA: 2:13 - loss: 1.057 - ETA: 2:11 - loss: 1.057 - ETA: 2:10 - loss: 1.057 - ETA: 2:08 - loss: 1.055 - ETA: 2:07 - loss: 1.056 - ETA: 2:05 - loss: 1.057 - ETA: 2:03 - loss: 1.058 - ETA: 2:02 - loss: 1.058 - ETA: 2:00 - loss: 1.059 - ETA: 1:59 - loss: 1.059 - ETA: 1:57 - loss: 1.060 - ETA: 1:55 - loss: 1.060 - ETA: 1:54 - loss: 1.059 - ETA: 1:52 - loss: 1.059 - ETA: 1:51 - loss: 1.060 - ETA: 1:49 - loss: 1.061 - ETA: 1:48 - loss: 1.060 - ETA: 1:46 - loss: 1.060 - ETA: 1:44 - loss: 1.061 - ETA: 1:43 - loss: 1.061 - ETA: 1:41 - loss: 1.060 - ETA: 1:40 - loss: 1.059 - ETA: 1:38 - loss: 1.059 - ETA: 1:36 - loss: 1.060 - ETA: 1:35 - loss: 1.059 - ETA: 1:33 - loss: 1.059 - ETA: 1:32 - loss: 1.059 - ETA: 1:30 - loss: 1.059 - ETA: 1:28 - loss: 1.059 - ETA: 1:27 - loss: 1.058 - ETA: 1:25 - loss: 1.058 - ETA: 1:24 - loss: 1.058 - ETA: 1:22 - loss: 1.058 - ETA: 1:21 - loss: 1.059 - ETA: 1:19 - loss: 1.060 - ETA: 1:17 - loss: 1.060 - ETA: 1:16 - loss: 1.059 - ETA: 1:14 - loss: 1.059 - ETA: 1:13 - loss: 1.059 - ETA: 1:11 - loss: 1.059 - ETA: 1:09 - loss: 1.059 - ETA: 1:08 - loss: 1.060 - ETA: 1:06 - loss: 1.060 - ETA: 1:05 - loss: 1.060 - ETA: 1:03 - loss: 1.060 - ETA: 1:01 - loss: 1.059 - ETA: 1:00 - loss: 1.059 - ETA: 58s - loss: 1.059 - ETA: 57s - loss: 1.05 - ETA: 55s - loss: 1.05 - ETA: 54s - loss: 1.05 - ETA: 52s - loss: 1.05 - ETA: 50s - loss: 1.05 - ETA: 49s - loss: 1.05 - ETA: 47s - loss: 1.05 - ETA: 46s - loss: 1.05 - ETA: 44s - loss: 1.05 - ETA: 42s - loss: 1.05 - ETA: 41s - loss: 1.05 - ETA: 39s - loss: 1.05 - ETA: 38s - loss: 1.05 - ETA: 36s - loss: 1.05 - ETA: 34s - loss: 1.05 - ETA: 33s - loss: 1.05 - ETA: 31s - loss: 1.05 - ETA: 30s - loss: 1.05 - ETA: 28s - loss: 1.05 - ETA: 27s - loss: 1.05 - ETA: 25s - loss: 1.05 - ETA: 23s - loss: 1.05 - ETA: 22s - loss: 1.05 - ETA: 20s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 17s - loss: 1.05 - ETA: 15s - loss: 1.05 - ETA: 14s - loss: 1.05 - ETA: 12s - loss: 1.05 - ETA: 11s - loss: 1.05 - ETA: 9s - loss: 1.0592 - ETA: 7s - loss: 1.058 - ETA: 6s - loss: 1.058 - ETA: 4s - loss: 1.057 - ETA: 3s - loss: 1.057 - ETA: 1s - loss: 1.057 - 299s 2s/step - loss: 1.0574 - val_loss: 1.3894\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.24836\n",
      "Epoch 18/20\n",
      "187/187 [==============================] - ETA: 4:55 - loss: 1.042 - ETA: 4:53 - loss: 0.978 - ETA: 4:52 - loss: 0.987 - ETA: 4:50 - loss: 1.002 - ETA: 4:48 - loss: 1.021 - ETA: 4:47 - loss: 1.024 - ETA: 4:45 - loss: 1.032 - ETA: 4:43 - loss: 1.037 - ETA: 4:42 - loss: 1.038 - ETA: 4:40 - loss: 1.032 - ETA: 4:39 - loss: 1.037 - ETA: 4:37 - loss: 1.048 - ETA: 4:35 - loss: 1.066 - ETA: 4:34 - loss: 1.065 - ETA: 4:32 - loss: 1.057 - ETA: 4:31 - loss: 1.049 - ETA: 4:29 - loss: 1.049 - ETA: 4:28 - loss: 1.054 - ETA: 4:26 - loss: 1.053 - ETA: 4:25 - loss: 1.049 - ETA: 4:23 - loss: 1.046 - ETA: 4:22 - loss: 1.039 - ETA: 4:20 - loss: 1.042 - ETA: 4:18 - loss: 1.042 - ETA: 4:17 - loss: 1.033 - ETA: 4:15 - loss: 1.027 - ETA: 4:14 - loss: 1.025 - ETA: 4:12 - loss: 1.025 - ETA: 4:10 - loss: 1.025 - ETA: 4:09 - loss: 1.023 - ETA: 4:07 - loss: 1.027 - ETA: 4:06 - loss: 1.038 - ETA: 4:04 - loss: 1.039 - ETA: 4:02 - loss: 1.038 - ETA: 4:01 - loss: 1.036 - ETA: 3:59 - loss: 1.040 - ETA: 3:58 - loss: 1.045 - ETA: 3:56 - loss: 1.047 - ETA: 3:54 - loss: 1.049 - ETA: 3:53 - loss: 1.045 - ETA: 3:51 - loss: 1.042 - ETA: 3:50 - loss: 1.042 - ETA: 3:48 - loss: 1.043 - ETA: 3:47 - loss: 1.043 - ETA: 3:45 - loss: 1.042 - ETA: 3:43 - loss: 1.040 - ETA: 3:42 - loss: 1.044 - ETA: 3:40 - loss: 1.045 - ETA: 3:39 - loss: 1.049 - ETA: 3:37 - loss: 1.048 - ETA: 3:35 - loss: 1.045 - ETA: 3:34 - loss: 1.045 - ETA: 3:32 - loss: 1.048 - ETA: 3:31 - loss: 1.048 - ETA: 3:29 - loss: 1.051 - ETA: 3:28 - loss: 1.051 - ETA: 3:26 - loss: 1.049 - ETA: 3:24 - loss: 1.049 - ETA: 3:23 - loss: 1.051 - ETA: 3:21 - loss: 1.051 - ETA: 3:20 - loss: 1.052 - ETA: 3:18 - loss: 1.052 - ETA: 3:16 - loss: 1.051 - ETA: 3:15 - loss: 1.050 - ETA: 3:13 - loss: 1.051 - ETA: 3:12 - loss: 1.052 - ETA: 3:10 - loss: 1.054 - ETA: 3:08 - loss: 1.055 - ETA: 3:07 - loss: 1.055 - ETA: 3:05 - loss: 1.055 - ETA: 3:04 - loss: 1.056 - ETA: 3:02 - loss: 1.055 - ETA: 3:01 - loss: 1.057 - ETA: 2:59 - loss: 1.056 - ETA: 2:57 - loss: 1.053 - ETA: 2:56 - loss: 1.051 - ETA: 2:54 - loss: 1.051 - ETA: 2:53 - loss: 1.050 - ETA: 2:51 - loss: 1.049 - ETA: 2:49 - loss: 1.051 - ETA: 2:48 - loss: 1.050 - ETA: 2:46 - loss: 1.051 - ETA: 2:45 - loss: 1.050 - ETA: 2:43 - loss: 1.049 - ETA: 2:41 - loss: 1.051 - ETA: 2:40 - loss: 1.050 - ETA: 2:38 - loss: 1.052 - ETA: 2:37 - loss: 1.053 - ETA: 2:35 - loss: 1.052 - ETA: 2:34 - loss: 1.053 - ETA: 2:32 - loss: 1.055 - ETA: 2:30 - loss: 1.054 - ETA: 2:29 - loss: 1.056 - ETA: 2:27 - loss: 1.055 - ETA: 2:26 - loss: 1.053 - ETA: 2:24 - loss: 1.054 - ETA: 2:22 - loss: 1.054 - ETA: 2:21 - loss: 1.054 - ETA: 2:19 - loss: 1.054 - ETA: 2:18 - loss: 1.055 - ETA: 2:16 - loss: 1.054 - ETA: 2:14 - loss: 1.056 - ETA: 2:13 - loss: 1.055 - ETA: 2:11 - loss: 1.054 - ETA: 2:10 - loss: 1.054 - ETA: 2:08 - loss: 1.054 - ETA: 2:07 - loss: 1.054 - ETA: 2:05 - loss: 1.054 - ETA: 2:03 - loss: 1.055 - ETA: 2:02 - loss: 1.055 - ETA: 2:00 - loss: 1.055 - ETA: 1:59 - loss: 1.055 - ETA: 1:57 - loss: 1.055 - ETA: 1:55 - loss: 1.057 - ETA: 1:54 - loss: 1.055 - ETA: 1:52 - loss: 1.056 - ETA: 1:51 - loss: 1.056 - ETA: 1:49 - loss: 1.055 - ETA: 1:47 - loss: 1.056 - ETA: 1:46 - loss: 1.055 - ETA: 1:44 - loss: 1.056 - ETA: 1:43 - loss: 1.057 - ETA: 1:41 - loss: 1.057 - ETA: 1:40 - loss: 1.057 - ETA: 1:38 - loss: 1.058 - ETA: 1:36 - loss: 1.058 - ETA: 1:35 - loss: 1.057 - ETA: 1:33 - loss: 1.057 - ETA: 1:32 - loss: 1.058 - ETA: 1:30 - loss: 1.058 - ETA: 1:28 - loss: 1.057 - ETA: 1:27 - loss: 1.055 - ETA: 1:25 - loss: 1.055 - ETA: 1:24 - loss: 1.054 - ETA: 1:22 - loss: 1.054 - ETA: 1:20 - loss: 1.055 - ETA: 1:19 - loss: 1.053 - ETA: 1:17 - loss: 1.054 - ETA: 1:16 - loss: 1.054 - ETA: 1:14 - loss: 1.054 - ETA: 1:13 - loss: 1.053 - ETA: 1:11 - loss: 1.052 - ETA: 1:09 - loss: 1.052 - ETA: 1:08 - loss: 1.053 - ETA: 1:06 - loss: 1.053 - ETA: 1:05 - loss: 1.051 - ETA: 1:03 - loss: 1.052 - ETA: 1:01 - loss: 1.051 - ETA: 1:00 - loss: 1.050 - ETA: 58s - loss: 1.050 - ETA: 57s - loss: 1.04 - ETA: 55s - loss: 1.04 - ETA: 54s - loss: 1.04 - ETA: 52s - loss: 1.04 - ETA: 50s - loss: 1.04 - ETA: 49s - loss: 1.04 - ETA: 47s - loss: 1.04 - ETA: 46s - loss: 1.04 - ETA: 44s - loss: 1.04 - ETA: 42s - loss: 1.04 - ETA: 41s - loss: 1.04 - ETA: 39s - loss: 1.04 - ETA: 38s - loss: 1.04 - ETA: 36s - loss: 1.04 - ETA: 34s - loss: 1.04 - ETA: 33s - loss: 1.04 - ETA: 31s - loss: 1.04 - ETA: 30s - loss: 1.04 - ETA: 28s - loss: 1.04 - ETA: 27s - loss: 1.04 - ETA: 25s - loss: 1.04 - ETA: 23s - loss: 1.04 - ETA: 22s - loss: 1.04 - ETA: 20s - loss: 1.04 - ETA: 19s - loss: 1.04 - ETA: 17s - loss: 1.04 - ETA: 15s - loss: 1.04 - ETA: 14s - loss: 1.04 - ETA: 12s - loss: 1.04 - ETA: 11s - loss: 1.04 - ETA: 9s - loss: 1.0471 - ETA: 7s - loss: 1.047 - ETA: 6s - loss: 1.046 - ETA: 4s - loss: 1.046 - ETA: 3s - loss: 1.045 - ETA: 1s - loss: 1.047 - 299s 2s/step - loss: 1.0484 - val_loss: 1.3270\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.24836\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 4:55 - loss: 1.126 - ETA: 4:53 - loss: 1.069 - ETA: 4:51 - loss: 1.113 - ETA: 4:49 - loss: 1.106 - ETA: 4:48 - loss: 1.097 - ETA: 4:46 - loss: 1.081 - ETA: 4:45 - loss: 1.078 - ETA: 4:43 - loss: 1.068 - ETA: 4:42 - loss: 1.044 - ETA: 4:40 - loss: 1.053 - ETA: 4:39 - loss: 1.064 - ETA: 4:37 - loss: 1.065 - ETA: 4:35 - loss: 1.060 - ETA: 4:34 - loss: 1.054 - ETA: 4:32 - loss: 1.046 - ETA: 4:31 - loss: 1.038 - ETA: 4:29 - loss: 1.044 - ETA: 4:28 - loss: 1.041 - ETA: 4:26 - loss: 1.045 - ETA: 4:25 - loss: 1.044 - ETA: 4:23 - loss: 1.051 - ETA: 4:21 - loss: 1.054 - ETA: 4:20 - loss: 1.051 - ETA: 4:18 - loss: 1.046 - ETA: 4:17 - loss: 1.046 - ETA: 4:15 - loss: 1.045 - ETA: 4:14 - loss: 1.042 - ETA: 4:12 - loss: 1.045 - ETA: 4:10 - loss: 1.043 - ETA: 4:09 - loss: 1.038 - ETA: 4:07 - loss: 1.037 - ETA: 4:06 - loss: 1.035 - ETA: 4:04 - loss: 1.033 - ETA: 4:02 - loss: 1.037 - ETA: 4:01 - loss: 1.041 - ETA: 3:59 - loss: 1.042 - ETA: 3:58 - loss: 1.042 - ETA: 3:56 - loss: 1.044 - ETA: 3:54 - loss: 1.042 - ETA: 3:53 - loss: 1.037 - ETA: 3:51 - loss: 1.039 - ETA: 3:50 - loss: 1.035 - ETA: 3:48 - loss: 1.034 - ETA: 3:46 - loss: 1.034 - ETA: 3:45 - loss: 1.032 - ETA: 3:43 - loss: 1.030 - ETA: 3:42 - loss: 1.032 - ETA: 3:40 - loss: 1.034 - ETA: 3:38 - loss: 1.033 - ETA: 3:37 - loss: 1.032 - ETA: 3:35 - loss: 1.030 - ETA: 3:34 - loss: 1.029 - ETA: 3:32 - loss: 1.028 - ETA: 3:31 - loss: 1.030 - ETA: 3:29 - loss: 1.027 - ETA: 3:27 - loss: 1.028 - ETA: 3:26 - loss: 1.026 - ETA: 3:24 - loss: 1.024 - ETA: 3:23 - loss: 1.025 - ETA: 3:21 - loss: 1.027 - ETA: 3:19 - loss: 1.028 - ETA: 3:18 - loss: 1.030 - ETA: 3:16 - loss: 1.030 - ETA: 3:15 - loss: 1.030 - ETA: 3:13 - loss: 1.030 - ETA: 3:12 - loss: 1.030 - ETA: 3:10 - loss: 1.028 - ETA: 3:08 - loss: 1.027 - ETA: 3:07 - loss: 1.028 - ETA: 3:05 - loss: 1.029 - ETA: 3:04 - loss: 1.030 - ETA: 3:02 - loss: 1.027 - ETA: 3:00 - loss: 1.027 - ETA: 2:59 - loss: 1.026 - ETA: 2:57 - loss: 1.025 - ETA: 2:56 - loss: 1.026 - ETA: 2:54 - loss: 1.029 - ETA: 2:53 - loss: 1.032 - ETA: 2:51 - loss: 1.031 - ETA: 2:49 - loss: 1.034 - ETA: 2:48 - loss: 1.037 - ETA: 2:46 - loss: 1.038 - ETA: 2:45 - loss: 1.038 - ETA: 2:43 - loss: 1.037 - ETA: 2:41 - loss: 1.038 - ETA: 2:40 - loss: 1.040 - ETA: 2:38 - loss: 1.039 - ETA: 2:37 - loss: 1.039 - ETA: 2:35 - loss: 1.039 - ETA: 2:34 - loss: 1.040 - ETA: 2:32 - loss: 1.040 - ETA: 2:30 - loss: 1.040 - ETA: 2:29 - loss: 1.040 - ETA: 2:27 - loss: 1.039 - ETA: 2:26 - loss: 1.039 - ETA: 2:24 - loss: 1.039 - ETA: 2:22 - loss: 1.037 - ETA: 2:21 - loss: 1.038 - ETA: 2:19 - loss: 1.037 - ETA: 2:18 - loss: 1.039 - ETA: 2:16 - loss: 1.038 - ETA: 2:14 - loss: 1.040 - ETA: 2:13 - loss: 1.038 - ETA: 2:11 - loss: 1.038 - ETA: 2:10 - loss: 1.038 - ETA: 2:08 - loss: 1.038 - ETA: 2:07 - loss: 1.038 - ETA: 2:05 - loss: 1.037 - ETA: 2:03 - loss: 1.038 - ETA: 2:02 - loss: 1.040 - ETA: 2:00 - loss: 1.041 - ETA: 1:59 - loss: 1.041 - ETA: 1:57 - loss: 1.042 - ETA: 1:55 - loss: 1.043 - ETA: 1:54 - loss: 1.043 - ETA: 1:52 - loss: 1.042 - ETA: 1:51 - loss: 1.042 - ETA: 1:49 - loss: 1.042 - ETA: 1:47 - loss: 1.042 - ETA: 1:46 - loss: 1.042 - ETA: 1:44 - loss: 1.041 - ETA: 1:43 - loss: 1.040 - ETA: 1:41 - loss: 1.040 - ETA: 1:40 - loss: 1.041 - ETA: 1:38 - loss: 1.039 - ETA: 1:36 - loss: 1.039 - ETA: 1:35 - loss: 1.040 - ETA: 1:33 - loss: 1.040 - ETA: 1:32 - loss: 1.041 - ETA: 1:30 - loss: 1.041 - ETA: 1:28 - loss: 1.040 - ETA: 1:27 - loss: 1.041 - ETA: 1:25 - loss: 1.041 - ETA: 1:24 - loss: 1.041 - ETA: 1:22 - loss: 1.042 - ETA: 1:21 - loss: 1.042 - ETA: 1:19 - loss: 1.041 - ETA: 1:17 - loss: 1.040 - ETA: 1:16 - loss: 1.039 - ETA: 1:14 - loss: 1.040 - ETA: 1:13 - loss: 1.040 - ETA: 1:11 - loss: 1.041 - ETA: 1:09 - loss: 1.042 - ETA: 1:08 - loss: 1.040 - ETA: 1:06 - loss: 1.041 - ETA: 1:05 - loss: 1.041 - ETA: 1:03 - loss: 1.041 - ETA: 1:01 - loss: 1.040 - ETA: 1:00 - loss: 1.040 - ETA: 58s - loss: 1.039 - ETA: 57s - loss: 1.04 - ETA: 55s - loss: 1.04 - ETA: 54s - loss: 1.03 - ETA: 52s - loss: 1.04 - ETA: 50s - loss: 1.04 - ETA: 49s - loss: 1.04 - ETA: 47s - loss: 1.04 - ETA: 46s - loss: 1.04 - ETA: 44s - loss: 1.04 - ETA: 42s - loss: 1.03 - ETA: 41s - loss: 1.04 - ETA: 39s - loss: 1.03 - ETA: 38s - loss: 1.03 - ETA: 36s - loss: 1.03 - ETA: 34s - loss: 1.03 - ETA: 33s - loss: 1.03 - ETA: 31s - loss: 1.03 - ETA: 30s - loss: 1.03 - ETA: 28s - loss: 1.03 - ETA: 27s - loss: 1.03 - ETA: 25s - loss: 1.03 - ETA: 23s - loss: 1.03 - ETA: 22s - loss: 1.03 - ETA: 20s - loss: 1.03 - ETA: 19s - loss: 1.03 - ETA: 17s - loss: 1.03 - ETA: 15s - loss: 1.03 - ETA: 14s - loss: 1.03 - ETA: 12s - loss: 1.03 - ETA: 11s - loss: 1.03 - ETA: 9s - loss: 1.0381 - ETA: 7s - loss: 1.038 - ETA: 6s - loss: 1.038 - ETA: 4s - loss: 1.038 - ETA: 3s - loss: 1.037 - ETA: 1s - loss: 1.037 - 299s 2s/step - loss: 1.0368 - val_loss: 1.3600\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.24836\n",
      "Epoch 20/20\n",
      "187/187 [==============================] - ETA: 4:55 - loss: 0.967 - ETA: 4:53 - loss: 0.998 - ETA: 4:51 - loss: 1.036 - ETA: 4:50 - loss: 0.989 - ETA: 4:48 - loss: 0.995 - ETA: 4:47 - loss: 1.023 - ETA: 4:45 - loss: 1.024 - ETA: 4:44 - loss: 1.043 - ETA: 4:42 - loss: 1.053 - ETA: 4:41 - loss: 1.070 - ETA: 4:39 - loss: 1.049 - ETA: 4:38 - loss: 1.055 - ETA: 4:36 - loss: 1.043 - ETA: 4:34 - loss: 1.033 - ETA: 4:33 - loss: 1.029 - ETA: 4:31 - loss: 1.022 - ETA: 4:30 - loss: 1.013 - ETA: 4:28 - loss: 1.019 - ETA: 4:27 - loss: 1.030 - ETA: 4:25 - loss: 1.031 - ETA: 4:23 - loss: 1.027 - ETA: 4:22 - loss: 1.035 - ETA: 4:20 - loss: 1.038 - ETA: 4:19 - loss: 1.041 - ETA: 4:17 - loss: 1.041 - ETA: 4:15 - loss: 1.038 - ETA: 4:14 - loss: 1.036 - ETA: 4:12 - loss: 1.036 - ETA: 4:11 - loss: 1.036 - ETA: 4:09 - loss: 1.035 - ETA: 4:07 - loss: 1.037 - ETA: 4:06 - loss: 1.042 - ETA: 4:04 - loss: 1.041 - ETA: 4:03 - loss: 1.038 - ETA: 4:01 - loss: 1.043 - ETA: 3:59 - loss: 1.040 - ETA: 3:58 - loss: 1.039 - ETA: 3:56 - loss: 1.035 - ETA: 3:55 - loss: 1.035 - ETA: 3:53 - loss: 1.035 - ETA: 3:51 - loss: 1.036 - ETA: 3:50 - loss: 1.034 - ETA: 3:48 - loss: 1.032 - ETA: 3:47 - loss: 1.030 - ETA: 3:45 - loss: 1.028 - ETA: 3:44 - loss: 1.027 - ETA: 3:42 - loss: 1.027 - ETA: 3:40 - loss: 1.028 - ETA: 3:39 - loss: 1.030 - ETA: 3:37 - loss: 1.030 - ETA: 3:36 - loss: 1.029 - ETA: 3:34 - loss: 1.031 - ETA: 3:32 - loss: 1.034 - ETA: 3:31 - loss: 1.032 - ETA: 3:29 - loss: 1.031 - ETA: 3:28 - loss: 1.035 - ETA: 3:26 - loss: 1.037 - ETA: 3:25 - loss: 1.036 - ETA: 3:23 - loss: 1.038 - ETA: 3:21 - loss: 1.038 - ETA: 3:20 - loss: 1.038 - ETA: 3:18 - loss: 1.040 - ETA: 3:17 - loss: 1.041 - ETA: 3:15 - loss: 1.041 - ETA: 3:13 - loss: 1.040 - ETA: 3:12 - loss: 1.040 - ETA: 3:10 - loss: 1.042 - ETA: 3:09 - loss: 1.042 - ETA: 3:07 - loss: 1.042 - ETA: 3:05 - loss: 1.044 - ETA: 3:04 - loss: 1.043 - ETA: 3:02 - loss: 1.045 - ETA: 3:01 - loss: 1.044 - ETA: 2:59 - loss: 1.042 - ETA: 2:57 - loss: 1.044 - ETA: 2:56 - loss: 1.046 - ETA: 2:54 - loss: 1.043 - ETA: 2:53 - loss: 1.043 - ETA: 2:51 - loss: 1.044 - ETA: 2:50 - loss: 1.045 - ETA: 2:48 - loss: 1.044 - ETA: 2:46 - loss: 1.045 - ETA: 2:45 - loss: 1.044 - ETA: 2:43 - loss: 1.042 - ETA: 2:42 - loss: 1.040 - ETA: 2:40 - loss: 1.040 - ETA: 2:38 - loss: 1.040 - ETA: 2:37 - loss: 1.040 - ETA: 2:35 - loss: 1.039 - ETA: 2:34 - loss: 1.039 - ETA: 2:32 - loss: 1.037 - ETA: 2:30 - loss: 1.036 - ETA: 2:29 - loss: 1.034 - ETA: 2:27 - loss: 1.033 - ETA: 2:26 - loss: 1.034 - ETA: 2:24 - loss: 1.035 - ETA: 2:23 - loss: 1.036 - ETA: 2:21 - loss: 1.035 - ETA: 2:19 - loss: 1.035 - ETA: 2:18 - loss: 1.034 - ETA: 2:16 - loss: 1.034 - ETA: 2:15 - loss: 1.034 - ETA: 2:13 - loss: 1.034 - ETA: 2:11 - loss: 1.035 - ETA: 2:10 - loss: 1.035 - ETA: 2:08 - loss: 1.034 - ETA: 2:07 - loss: 1.037 - ETA: 2:05 - loss: 1.037 - ETA: 2:03 - loss: 1.037 - ETA: 2:02 - loss: 1.036 - ETA: 2:00 - loss: 1.036 - ETA: 1:59 - loss: 1.039 - ETA: 1:57 - loss: 1.039 - ETA: 1:55 - loss: 1.040 - ETA: 1:54 - loss: 1.039 - ETA: 1:52 - loss: 1.039 - ETA: 1:51 - loss: 1.038 - ETA: 1:49 - loss: 1.037 - ETA: 1:48 - loss: 1.037 - ETA: 1:46 - loss: 1.038 - ETA: 1:44 - loss: 1.038 - ETA: 1:43 - loss: 1.036 - ETA: 1:41 - loss: 1.037 - ETA: 1:40 - loss: 1.037 - ETA: 1:38 - loss: 1.038 - ETA: 1:36 - loss: 1.038 - ETA: 1:35 - loss: 1.038 - ETA: 1:33 - loss: 1.037 - ETA: 1:32 - loss: 1.037 - ETA: 1:30 - loss: 1.037 - ETA: 1:28 - loss: 1.038 - ETA: 1:27 - loss: 1.038 - ETA: 1:25 - loss: 1.039 - ETA: 1:24 - loss: 1.039 - ETA: 1:22 - loss: 1.039 - ETA: 1:21 - loss: 1.039 - ETA: 1:19 - loss: 1.041 - ETA: 1:17 - loss: 1.039 - ETA: 1:16 - loss: 1.038 - ETA: 1:14 - loss: 1.038 - ETA: 1:13 - loss: 1.038 - ETA: 1:11 - loss: 1.038 - ETA: 1:09 - loss: 1.038 - ETA: 1:08 - loss: 1.039 - ETA: 1:06 - loss: 1.038 - ETA: 1:05 - loss: 1.038 - ETA: 1:03 - loss: 1.038 - ETA: 1:01 - loss: 1.038 - ETA: 1:00 - loss: 1.038 - ETA: 58s - loss: 1.038 - ETA: 57s - loss: 1.03 - ETA: 55s - loss: 1.03 - ETA: 54s - loss: 1.03 - ETA: 52s - loss: 1.03 - ETA: 50s - loss: 1.03 - ETA: 49s - loss: 1.03 - ETA: 47s - loss: 1.03 - ETA: 46s - loss: 1.03 - ETA: 44s - loss: 1.03 - ETA: 42s - loss: 1.03 - ETA: 41s - loss: 1.03 - ETA: 39s - loss: 1.03 - ETA: 38s - loss: 1.03 - ETA: 36s - loss: 1.03 - ETA: 34s - loss: 1.03 - ETA: 33s - loss: 1.03 - ETA: 31s - loss: 1.03 - ETA: 30s - loss: 1.03 - ETA: 28s - loss: 1.03 - ETA: 27s - loss: 1.03 - ETA: 25s - loss: 1.03 - ETA: 23s - loss: 1.03 - ETA: 22s - loss: 1.03 - ETA: 20s - loss: 1.03 - ETA: 19s - loss: 1.03 - ETA: 17s - loss: 1.03 - ETA: 15s - loss: 1.03 - ETA: 14s - loss: 1.03 - ETA: 12s - loss: 1.03 - ETA: 11s - loss: 1.03 - ETA: 9s - loss: 1.0337 - ETA: 7s - loss: 1.033 - ETA: 6s - loss: 1.033 - ETA: 4s - loss: 1.032 - ETA: 3s - loss: 1.032 - ETA: 1s - loss: 1.032 - 299s 2s/step - loss: 1.0330 - val_loss: 1.3392\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.24836\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "start = time.time()\n",
    "callbacks = decoder_model.fit_generator(generator=generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[checkpoints, reduce_lr],\n",
    "                            validation_data=val_generator,\n",
    "                            validation_steps=5)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training: 6076.838664531708 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Time for training: {} seconds\".format(time_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./callbacks'):\n",
    "    os.mkdir('./callbacks')   \n",
    "columns = callbacks.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_df = pd.DataFrame(callbacks.history)\n",
    "callback_df.to_csv(callbacks_path, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
