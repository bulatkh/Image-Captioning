{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "from keras.applications import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, Callback, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, GRU, Flatten, Dropout, BatchNormalization, RepeatVector, concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import coco_parse\n",
    "import flickr8k_parse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import text_processing\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building decoder, it is necessary to encode captions into one-hot vectors which further would be used in embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_path = 'D:/coco/annotations/'\n",
    "# images_path = 'D:/coco/images/'\n",
    "\n",
    "# # parse JSON file with captions to get paths to images with captions\n",
    "# val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=False)\n",
    "# val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "# train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=True)\n",
    "# train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)\n",
    "\n",
    "# ### Extract captions\n",
    "# train_captions = coco_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "# val_captions = coco_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'D:/Flickr8k/images/'\n",
    "annotations_path = 'D:/Flickr8k/annotations/'\n",
    "captions_file = 'D:/Flickr8k/annotations/Flickr8k.token.txt'\n",
    "train_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.trainImages.txt'\n",
    "dev_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.devImages.txt'\n",
    "test_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.testImages.txt'\n",
    "\n",
    "filenames_with_all_captions = flickr8k_parse.generate_filenames_with_all_captions(captions_file, images_path)\n",
    "\n",
    "train_filenames_with_all_captions = flickr8k_parse.generate_set(train_txt_path, filenames_with_all_captions, images_path)\n",
    "val_filenames_with_all_captions = flickr8k_parse.generate_set(dev_txt_path, filenames_with_all_captions, images_path)\n",
    "test_filenames_with_all_captions = flickr8k_parse.generate_set(test_txt_path, filenames_with_all_captions, images_path)\n",
    "\n",
    "train_captions = flickr8k_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "val_captions = flickr8k_parse.make_list_of_captions(val_filenames_with_all_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess captions\n",
    "text_processing.preprocess_captions(val_captions)\n",
    "text_processing.preprocess_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add markers of captions' starts and ends\n",
    "text_processing.add_start_and_end_to_captions(train_captions)\n",
    "text_processing.add_start_and_end_to_captions(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vocabulary from the training captions\n",
    "train_vocab = text_processing.Vocabulary()\n",
    "for caption_list in train_captions:\n",
    "    for caption in caption_list:\n",
    "        tmp_caption_list = caption.split()\n",
    "        for word in tmp_caption_list:\n",
    "            train_vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.save_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create transformed captions list - substitute words by their IDs from vocabulary\n",
    "def tokenise_captions(set_captions, vocabulary):\n",
    "    captions_tokens = [] \n",
    "    for captions in set_captions:\n",
    "        tmp_captions_for_img = []\n",
    "        for caption in captions:\n",
    "            caption_words = caption.split()\n",
    "            tmp = []\n",
    "            for word in caption_words:\n",
    "                if word in vocabulary.word_to_id:\n",
    "                    tmp.append(vocabulary.get_id_by_word(word))\n",
    "                else:\n",
    "                    tmp.append(0)\n",
    "            tmp_captions_for_img.append(tmp)\n",
    "        captions_tokens.append(tmp_captions_for_img)\n",
    "    return captions_tokens\n",
    "\n",
    "train_captions_tokens = tokenise_captions(train_captions, train_vocab)\n",
    "val_captions_tokens = tokenise_captions(val_captions, train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 2, 8, 4, 9, 10, 11, 12],\n",
       " [1, 3, 4, 13, 14, 4, 15, 11, 12],\n",
       " [1, 16, 17, 18, 19, 20, 21, 10, 22, 23, 12],\n",
       " [1, 16, 17, 24, 25, 9, 10, 11, 12],\n",
       " [1, 16, 17, 6, 15, 2, 26, 27, 28, 29, 30, 12]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  10,\n",
       "  50,\n",
       "  1325,\n",
       "  622,\n",
       "  94,\n",
       "  72,\n",
       "  2,\n",
       "  678,\n",
       "  5,\n",
       "  824,\n",
       "  2294,\n",
       "  67,\n",
       "  10,\n",
       "  23,\n",
       "  42,\n",
       "  265,\n",
       "  50,\n",
       "  12],\n",
       " [1, 16, 185, 24, 72, 2, 678, 9, 2, 3708, 12],\n",
       " [1, 16, 111, 24, 72, 2, 296, 678, 12],\n",
       " [1, 16, 280, 246, 9, 77, 1150, 40, 72, 2, 678, 12],\n",
       " [1, 16, 129, 246, 72, 2, 678, 852, 21, 2, 689, 12]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black dog is running after a white dog in the snow <eos>',\n",
       " '<sos> black dog chasing brown dog through snow <eos>',\n",
       " '<sos> two dogs chase each other across the snowy ground <eos>',\n",
       " '<sos> two dogs play together in the snow <eos>',\n",
       " '<sos> two dogs running through a low lying body of water <eos>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_one_hot_encode(batch, number_of_words):\n",
    "    \"\"\" \n",
    "    Applies one-hot encoding to the input batch\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = batch.shape[0]\n",
    "    sentence_size = batch.shape[1]\n",
    "    \n",
    "    one_hot_batch = np.zeros((batch_size, sentence_size, number_of_words))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(sentence_size):\n",
    "            one_hot_batch[i, j, batch[i, j]] = 1\n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(transfer_values, captions_tokens, number_of_words, gru=True, max_length_lstm=40, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate a batch of input-output data pairs:\n",
    "        input_data = {\n",
    "            transfer_values,\n",
    "            input_tokens\n",
    "        }\n",
    "        \n",
    "        output_data = {\n",
    "            output_tokens\n",
    "        }\n",
    "        \n",
    "     Parameters:\n",
    "        -----------\n",
    "        transfer_values: np.array\n",
    "            Encoded images features\n",
    "            \n",
    "        captions: list\n",
    "            list with all the captions\n",
    "        \n",
    "        \n",
    "        batch_size: int\n",
    "            The number of examples in a batch\n",
    "        -----------\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        number_of_images = len(captions_tokens)\n",
    "        \n",
    "        indices = np.random.randint(0, len(transfer_values), size=batch_size)\n",
    "\n",
    "        captions_batch = []\n",
    "        ### Randomly select one caption for each example index\n",
    "        for ind in indices:\n",
    "            num_captions = len(captions_tokens[ind])\n",
    "            selected_caption = captions_tokens[ind][np.random.randint(0, num_captions - 1)]\n",
    "            captions_batch.append(selected_caption)\n",
    "\n",
    "        if not gru:\n",
    "            captions_batch_padded = pad_sequences(captions_batch, \n",
    "                                              maxlen=max_length_lstm + 1, \n",
    "                                              padding='post', \n",
    "                                              value=0)\n",
    "        else:\n",
    "            ### Find the largest caption length and pad the remaining to be the same size\n",
    "            max_caption_size = max([len(cap) for cap in captions_batch])\n",
    "            captions_batch_padded = pad_sequences(captions_batch, \n",
    "                                              maxlen=max_caption_size, \n",
    "                                              padding='post', \n",
    "                                              value=0)\n",
    "        ### Input tokens are the initial ones starting from index 1\n",
    "        ### Output tokens are the initial ones shifted to the right\n",
    "        input_tokens = captions_batch_padded[:, :-1]\n",
    "        output_tokens = captions_batch_padded[:, 1:]\n",
    "\n",
    "        output_tokens = batch_one_hot_encode(output_tokens, number_of_words)\n",
    "\n",
    "        input_transfer_values = transfer_values[indices]\n",
    "\n",
    "        input_data = {\n",
    "            'encoder_input': input_transfer_values,\n",
    "            'decoder_input': input_tokens\n",
    "        }\n",
    "\n",
    "        output_data = {\n",
    "            'decoder_output': output_tokens\n",
    "        }\n",
    "\n",
    "        yield (input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_values = np.load('./cnn_features/vgg16_flickr8k_train.npy')\n",
    "val_transfer_values = np.load('./cnn_features/vgg16_flickr8k_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "steps_per_epoch = int(len(train_captions) / batch_size)\n",
    "initial_state_size = 512\n",
    "embedding_out_size = 512\n",
    "number_of_layers = 3\n",
    "batch_norm = False\n",
    "dropout = False\n",
    "gru = True\n",
    "max_len = 40\n",
    "path_checkpoint = utils.generate_weights_path(gru, 'flickr8k', number_of_layers, batch_size, batch_norm, dropout)\n",
    "model_path = utils.generate_model_path(gru, number_of_layers, batch_norm, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./weights/VGG_True_flickr8k_3l_64b.hdf5'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/VGG16_GRU_3l.json'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder input part\n",
    "encoder_input = Input(shape=(4096,), name='encoder_input')\n",
    "encoder_reduction = Dense(initial_state_size, activation='relu', name='encoder_reduction')\n",
    "if batch_norm:\n",
    "    bn1 = BatchNormalization()\n",
    "### For LSTM\n",
    "if not gru:\n",
    "    repeat = RepeatVector(max_len)\n",
    "### Decoder input and embedding\n",
    "if gru:\n",
    "    decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "else:\n",
    "    decoder_input = Input(shape=(40,), name='decoder_input')\n",
    "embedding = Embedding(input_dim=train_vocab.number_of_words, output_dim=embedding_out_size, name='embedding')\n",
    "if dropout:\n",
    "    drop1 = Dropout(0.5)\n",
    "### GRU1\n",
    "if gru:\n",
    "    gru1 = GRU(initial_state_size, name='GRU1', return_sequences=True)\n",
    "else:\n",
    "    lstm1 = LSTM(initial_state_size, name='LSTM1', return_sequences=True)\n",
    "if batch_norm:\n",
    "    bn2 = BatchNormalization()\n",
    "### GRU2    \n",
    "if number_of_layers >= 2:\n",
    "    if gru:\n",
    "        gru2 = GRU(initial_state_size, name='GRU2', return_sequences=True)\n",
    "    else:\n",
    "        lstm2 = LSTM(initial_state_size, name='LSTM2', return_sequences=True)\n",
    "    if batch_norm:\n",
    "        bn3 = BatchNormalization()\n",
    "### GRU3        \n",
    "if number_of_layers == 3:\n",
    "    if gru:\n",
    "        gru3 = GRU(initial_state_size, name='GRU3', return_sequences=True)\n",
    "    else:\n",
    "        lstm3 = LSTM(initial_state_size, name='LSTM3', return_sequences=True)\n",
    "    if batch_norm:\n",
    "        bn4 = BatchNormalization()\n",
    "\n",
    "decoder_dense = Dense(train_vocab.number_of_words, activation='softmax', name='decoder_output')\n",
    "\n",
    "def connect_transfer_values_gru(transfer_values):\n",
    "    \n",
    "    initial_state = encoder_reduction(transfer_values)\n",
    "    if batch_norm:\n",
    "        initial_state = bn1(initial_state)\n",
    "\n",
    "    X = decoder_input\n",
    "    X = embedding(X)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "    \n",
    "    X = gru1(X, initial_state=initial_state)\n",
    "    if batch_norm:\n",
    "        X = bn2(X)\n",
    "    if number_of_layers >= 2:\n",
    "        X = gru2(X, initial_state=initial_state)\n",
    "        if batch_norm:\n",
    "            X = bn3(X)\n",
    "    if number_of_layers == 3:\n",
    "        X = gru3(X, initial_state=initial_state)\n",
    "        if batch_norm:\n",
    "            X = bn4(X)\n",
    "\n",
    "    decoder_output = decoder_dense(X)\n",
    "    \n",
    "    return decoder_output\n",
    "\n",
    "def connect_transfer_values_lstm(transfer_values):\n",
    "    initial_state = encoder_reduction(transfer_values)\n",
    "    if batch_norm:\n",
    "        initial_state = bn1(initial_state)\n",
    "    initial_state = repeat(initial_state)\n",
    "    \n",
    "    X = decoder_input\n",
    "    X = embedding(X)\n",
    "    if dropout:\n",
    "        X = drop1(X)\n",
    "        \n",
    "    X = concatenate([initial_state, X])\n",
    "    \n",
    "    X = lstm1(X)\n",
    "    if batch_norm:\n",
    "        X = bn2(X)\n",
    "    if number_of_layers >= 2:\n",
    "        X = lstm2(X)\n",
    "        if batch_norm:\n",
    "            X = bn3(X)\n",
    "    if number_of_layers == 3:\n",
    "        X = lstm3(X)\n",
    "        if batch_norm:\n",
    "            X = bn4(X)\n",
    "    \n",
    "    decoder_output = decoder_dense(X)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if gru:\n",
    "    decoder_output = connect_transfer_values_gru(transfer_values=encoder_input)\n",
    "else:\n",
    "    decoder_output = connect_transfer_values_lstm(transfer_values=encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    3774976     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_reduction (Dense)       (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "GRU1 (GRU)                      (None, None, 512)    1574400     embedding[0][0]                  \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "GRU2 (GRU)                      (None, None, 512)    1574400     GRU1[0][0]                       \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "GRU3 (GRU)                      (None, None, 512)    1574400     GRU2[0][0]                       \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 7373)   3782349     GRU3[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 14,378,189\n",
      "Trainable params: 14,378,189\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer GRU1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_reduction_3/Relu:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer GRU2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_reduction_3/Relu:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer GRU3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_reduction_3/Relu:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "model_json = decoder_model.to_json()\n",
    "try:\n",
    "    os.mkdir('./models')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)\n",
    "val_generator = generate_batch(val_transfer_values, val_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=batch_size, gru=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> a young boy and girl hug and smile <eos> <sos> a person is surfing a big wave <eos> <sos> a biker flies through the air near sand dunes <eos> <sos> a man catches a huge wave on his surfboard <eos> <sos> several people are looking out from a building that is under construction <eos> <sos> a woman wearing a blue shirt and high heels stands on the sidewalk next to a man <eos> <sos> a woman smokes a cigarette and looks at a magazine in the middle of a snowy road <eos> <sos> a group of people wearing costumes jump as they walk down the street <eos> <sos> children smiling with facepaintings <eos> <sos> two boys one naked one in a skirt run on the sand <eos> <sos> a child in green winter clothes is holding his or her hand up while two other children look at him or her <eos> <sos> the man in the striped shirt and the man in the hat are posing for the camera <eos> <sos> a man sits watching a waterfall <eos> <sos> a boy wears plastic toy teeth and green plastic toy glasses which are much too small for him <eos> <sos> a shirtless man in striped shorts and sunglasses with a man in white shirt and sunglasses <eos> <sos> a man wearing a hi viz jacket is drilling into the road <eos> <sos> people trying to stay dry with umbrellas in the city <eos> <sos> a girl is sitting on a rock next to a waterfall <eos> <sos> three dogs run on wide grassy outdoor area <eos> <sos> a driver is at the wheel of a race car <eos> <sos> a child stands in front of a dancing wedding party <eos> <sos> girl swinging on a swing outside in a wooded area <eos> <sos> man walking through large puddle of water <eos> <sos> lone red team player breaks through two black team players to hit the ball <eos> <sos> the brown dog is walking along the dirt path with beautiful mountains behind him <eos> <sos> a skateboarder does a turn at the top of a half pipe covered in graffiti <eos> <sos> a man dances on a colorful wall <eos> <sos> a colourfully dressed and painted woman working a hula hoop in a crowd <eos> <sos> a brown and black dog fetching a tennis ball in a lake <eos> <sos> four young girls pose for a picture at school <eos> <sos> a bulldog with a red bandanna shakes off water <eos> <sos> a group of 5 people are walking toward the mountains <eos> <sos> a guy with a toboggan and a girl pose for the camera <eos> <sos> band plays in tight quarters <eos> <sos> a man in a red outfit balances on a bike <eos> <sos> a line of people are seen in silhouette on a snowy ridge when the sun is low in the sky <eos> <sos> a bunch of beer pull tabs at a bar with christmas lights on the ceiling <eos> <sos> a woman wearing a red scarf is standing on a hill by a winding road <eos> <sos> a horse and rider jump over a fence <eos> <sos> a brown dog running across the grass <eos> <sos> a big black dog runs through the water near the shore <eos> <sos> an elderly man is walking a brown dog using a red leash <eos> <sos> a black dog is running with a blue bone in his mouth <eos> <sos> a puppy plays with a camera <eos> <sos> a jockey in blue gallops a chestnut horse on a dirt track past a wooden fence <eos> <sos> a large white bird flying over water <eos> <sos> a dog staring at the end of an ice cream cone <eos> <sos> a dog is running on a wooden beam beside a blue fence <eos> <sos> a child shows other kids his still packaged toy <eos> <sos> a man is playing a guitar with a child playing a harmonica while three young girls watch <eos> <sos> a rock climber with it shirt off <eos> <sos> three people racing on ice skates <eos> <sos> a brown dog running through a pond <eos> <sos> an african american boy stands in front of a blue building in the handicapped space <eos> <sos> a man sticks his tongue out in an outdoor shower <eos> <sos> a dog with its mouth open is walking on a path on a garden which is next to a lake or a stream <eos> <sos> a man and a woman in front of the yellow flowers <eos> <sos> a person biking near trees <eos> <sos> a mountain biker in the forest <eos> <sos> a woman showing a long haired large black pedigree dog at a dog show <eos> <sos> an indian woman in a black shirt stands with her arms crossed looking at two other people talking to each other <eos> <sos> two horses are pulling a woman in a cart <eos> <sos> a girl and a boy hugging on a bridge <eos> <sos> two women holding checkered flags near an orange car <eos>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([x for x in [train_vocab.get_word_by_id(word) for x in next(generator)[0]['decoder_input'] for word in x if word != 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "During the training process, it is a good idea to save the weights periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('./decoders/')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "\n",
    "checkpoints = ModelCheckpoint(path_checkpoint, verbose=1, save_weights_only=True, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, verbose=1, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     decoder_model.load_weights(path_checkpoint)\n",
    "# except:\n",
    "#     print(\"Error while loading weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before using it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-50c6c5859be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                             validation_steps=5)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtime_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_function'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You must compile your model before using it.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before using it."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = decoder_model.fit_generator(generator=generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=20,\n",
    "                            callbacks=[checkpoints, reduce_lr],\n",
    "                            validation_data=val_generator,\n",
    "                            validation_steps=5)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training: 2987.9345643520355 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Time for training: {} seconds\".format(time_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'his' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5905b3b1b398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'his' is not defined"
     ]
    }
   ],
   "source": [
    "his"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
