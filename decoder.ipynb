{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.applications import VGG16\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, GRU, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import coco_parse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import text_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building decoder, it is necessary to encode captions into one-hot vectors which further would be used in embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = 'D:/coco/annotations/'\n",
    "images_path = 'D:/coco/images/'\n",
    "\n",
    "# parse JSON file with captions to get paths to images with captions\n",
    "val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=False)\n",
    "val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=True)\n",
    "train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)\n",
    "\n",
    "### Extract captions\n",
    "train_captions = coco_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "val_captions = coco_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bicycle replica with a clock as the front wheel.',\n",
       " 'The bike has a clock as a tire.',\n",
       " 'A black metal bicycle with a clock inside the front wheel.',\n",
       " 'A bicycle figurine in which the front wheel is replaced with a clock\\n',\n",
       " 'A clock with the appearance of the wheel of a bicycle ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A black Honda motorcycle parked in front of a garage.',\n",
       " 'A Honda motorcycle parked in a grass driveway',\n",
       " 'A black Honda motorcycle with a dark burgundy seat.',\n",
       " 'Ma motorcycle parked on the gravel in front of a garage',\n",
       " 'A motorcycle with its brake extended standing outside']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess captions\n",
    "text_processing.preprocess_captions(val_captions)\n",
    "text_processing.preprocess_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A black Honda motorcycle parked in front of a garage ',\n",
       " 'A Honda motorcycle parked in a grass driveway',\n",
       " 'A black Honda motorcycle with a dark burgundy seat ',\n",
       " 'Ma motorcycle parked on the gravel in front of a garage',\n",
       " 'A motorcycle with its brake extended standing outside']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bicycle replica with a clock as the front wheel ',\n",
       " 'The bike has a clock as a tire ',\n",
       " 'A black metal bicycle with a clock inside the front wheel ',\n",
       " 'A bicycle figurine in which the front wheel is replaced with a clock ',\n",
       " 'A clock with the appearance of the wheel of a bicycle ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add markers of captions' starts and ends\n",
    "text_processing.add_start_and_end_to_captions(train_captions)\n",
    "text_processing.add_start_and_end_to_captions(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a bicycle replica with a clock as the front wheel <eos>',\n",
       " '<sos> the bike has a clock as a tire <eos>',\n",
       " '<sos> a black metal bicycle with a clock inside the front wheel <eos>',\n",
       " '<sos> a bicycle figurine in which the front wheel is replaced with a clock <eos>',\n",
       " '<sos> a clock with the appearance of the wheel of a bicycle <eos>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black honda motorcycle parked in front of a garage <eos>',\n",
       " '<sos> a honda motorcycle parked in a grass driveway <eos>',\n",
       " '<sos> a black honda motorcycle with a dark burgundy seat <eos>',\n",
       " '<sos> ma motorcycle parked on the gravel in front of a garage <eos>',\n",
       " '<sos> a motorcycle with its brake extended standing outside <eos>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vocabulary from the training captions\n",
    "train_vocab = text_processing.Vocabulary()\n",
    "for caption_list in train_captions:\n",
    "    for caption in caption_list:\n",
    "        tmp_caption_list = caption.split()\n",
    "        for word in tmp_caption_list:\n",
    "            train_vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.save_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create transformed captions list - substitute words by their IDs from vocabulary\n",
    "train_captions_tokens = [] \n",
    "for captions in train_captions:\n",
    "    tmp_captions_for_img = []\n",
    "    for caption in captions:\n",
    "        caption_words = caption.split()\n",
    "        tmp = []\n",
    "        for word in caption_words:\n",
    "            tmp.append(train_vocab.get_id_by_word(word))\n",
    "        tmp_captions_for_img.append(tmp)\n",
    "    train_captions_tokens.append(tmp_captions_for_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 11],\n",
       " [1, 8, 12, 13, 2, 6, 7, 2, 14, 11],\n",
       " [1, 2, 15, 16, 3, 5, 2, 6, 17, 8, 9, 10, 11],\n",
       " [1, 2, 3, 18, 19, 20, 8, 9, 10, 21, 22, 5, 2, 6, 11],\n",
       " [1, 2, 6, 5, 8, 23, 24, 8, 10, 24, 2, 3, 11]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a bicycle replica with a clock as the front wheel <eos>',\n",
       " '<sos> the bike has a clock as a tire <eos>',\n",
       " '<sos> a black metal bicycle with a clock inside the front wheel <eos>',\n",
       " '<sos> a bicycle figurine in which the front wheel is replaced with a clock <eos>',\n",
       " '<sos> a clock with the appearance of the wheel of a bicycle <eos>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_one_hot_encode(batch, number_of_words):\n",
    "    \"\"\" \n",
    "    Applies one-hot encoding to the input batch\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = batch.shape[0]\n",
    "    sentence_size = batch.shape[1]\n",
    "    \n",
    "    one_hot_batch = np.zeros((batch_size, sentence_size, number_of_words))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(sentence_size):\n",
    "            one_hot_batch[i, j, batch[i, j]] = 1\n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(transfer_values, captions_tokens, number_of_words, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate a batch of input-output data pairs:\n",
    "        input_data = {\n",
    "            transfer_values,\n",
    "            input_tokens\n",
    "        }\n",
    "        \n",
    "        output_data = {\n",
    "            output_tokens\n",
    "        }\n",
    "        \n",
    "     Parameters:\n",
    "        -----------\n",
    "        transfer_values: np.array\n",
    "            Encoded images features\n",
    "            \n",
    "        captions: list\n",
    "            list with all the captions\n",
    "        \n",
    "        \n",
    "        batch_size: int\n",
    "            The number of examples in a batch\n",
    "        -----------\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        number_of_images = len(captions_tokens)\n",
    "        \n",
    "        indices = range(i * batch_size, (i + 1) * batch_size)\n",
    "        i += 1\n",
    "\n",
    "        captions_batch = []\n",
    "        ### Randomly select one caption for each example index\n",
    "        for ind in indices:\n",
    "            num_captions = len(captions_tokens[ind])\n",
    "            selected_caption = captions_tokens[ind][np.random.randint(0, num_captions)]\n",
    "            captions_batch.append(selected_caption)\n",
    "\n",
    "\n",
    "\n",
    "        ### Find the largest caption length and pad the remaining to be the same size\n",
    "        max_caption_size = max([len(cap) for cap in captions_batch])\n",
    "\n",
    "        captions_batch_padded = pad_sequences(captions_batch, \n",
    "                                              maxlen=max_caption_size, \n",
    "                                              padding='post', \n",
    "                                              value=0)\n",
    "\n",
    "        ### Input tokens are the initial ones starting from index 1\n",
    "        ### Output tokens are the initial ones shifted to the right\n",
    "        input_tokens = captions_batch_padded[:, :-1]\n",
    "        output_tokens = captions_batch_padded[:, 1:]\n",
    "\n",
    "        output_tokens = batch_one_hot_encode(output_tokens, number_of_words)\n",
    "\n",
    "        input_transfer_values = transfer_values[indices]\n",
    "\n",
    "        input_data = {\n",
    "            'encoder_input': input_transfer_values,\n",
    "            'decoder_input': input_tokens\n",
    "        }\n",
    "\n",
    "        output_data = {\n",
    "            'decoder_output': output_tokens\n",
    "        }\n",
    "\n",
    "        yield (input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> a bicycle figurine in which the front wheel is replaced with a clock <eos> <sos> a blue boat themed bathroom with a life preserver on the wall <eos> <sos> city street with parked cars and a bench <eos> <sos> there is a gol plane taking off in a partly cloudy sky <eos> <sos> a toilet in a circular matte glass stall <eos> <sos> a picture of a modern looking kitchen area <eos> <sos> a messy bathroom countertop perched atop black cabinetry <eos> <sos> an open box contains an unknown purple object <eos> <sos> an old green car parked on the side of the street <eos> <sos> this is a kitchen with dishes and a silver sink <eos> <sos> a group of motorcycle riders driving past buildings <eos> <sos> a cat drinking water from a toilet in a bathroom <eos> <sos> a man in a wheelchair and another sitting on a bench that is overlooking the water <sos> a man sits next to his horse in an alley in costume <eos> <sos> a series of motorbikes parked in a row on a street <eos> <sos> a small kitten is sitting in a bowl <eos>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([x for x in [train_vocab.get_word_by_id(word) for x in next(generator)[0]['decoder_input'] for word in x if word != 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_size = 512\n",
    "embedding_out_size = 512\n",
    "\n",
    "encoder_input = Input(shape=(4096,), name='encoder_input')\n",
    "encoder_reduction = Dense(initial_state_size, activation='tanh', name='encoder_reduction')\n",
    "\n",
    "decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "embedding = Embedding(input_dim=train_vocab.number_of_words, output_dim=embedding_out_size, name='embedding')\n",
    "\n",
    "gru1 = GRU(initial_state_size, name='GRU1', return_sequences=True)\n",
    "gru2 = GRU(initial_state_size, name='GRU2', return_sequences=True)\n",
    "gru3 = GRU(initial_state_size, name='GRU3', return_sequences=True)\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "decoder_dense = Dense(train_vocab.number_of_words, activation='softmax', name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_transfer_values(transfer_values):\n",
    "    \n",
    "    initial_state = encoder_reduction(transfer_values)\n",
    "\n",
    "    X = decoder_input\n",
    "    \n",
    "    X = embedding(X)\n",
    "    \n",
    "    X = gru1(X, initial_state=initial_state)\n",
    "    X = gru2(X, initial_state=initial_state)\n",
    "    X = gru3(X, initial_state=initial_state)\n",
    "\n",
    "    decoder_output = decoder_dense(X)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_values = np.load('./cnn_features/vgg16_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_transfer_values(transfer_values=encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    13748224    decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_reduction (Dense)       (None, 512)          2097664     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "GRU1 (GRU)                      (None, None, 512)    1574400     embedding[0][0]                  \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "GRU2 (GRU)                      (None, None, 512)    1574400     GRU1[0][0]                       \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "GRU3 (GRU)                      (None, None, 512)    1574400     GRU2[0][0]                       \n",
      "                                                                 encoder_reduction[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 26852)  13775076    GRU3[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 34,344,164\n",
      "Trainable params: 34,344,164\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate_batch(transfer_values, train_captions_tokens, number_of_words=train_vocab.number_of_words, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" \".join([x for x in [train_vocab.get_word_by_id(word) for x in next(generator)[0]['decoder_input'] for word in x if word != 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "During the training process, it is a good idea to save the weights periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('./decoders/')\n",
    "except:\n",
    "    print('The folder already exists')\n",
    "path_checkpoint = './decoders/VGG16_GRU.hdf5'\n",
    "checkpoints = ModelCheckpoint(path_checkpoint, verbose=1, save_weights_only=True)\n",
    "loss_history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    decoder_model.load_weights(path_checkpoint)\n",
    "except:\n",
    "    print(\"Error while loading weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/5\n",
      "256/256 [==============================] - 213s 832ms/step - loss: 3.7293\n",
      "\n",
      "Epoch 00001: saving model to ./decoders/VGG16_GRU.hdf5\n",
      "Epoch 2/5\n",
      "256/256 [==============================] - 209s 817ms/step - loss: 3.0601\n",
      "\n",
      "Epoch 00002: saving model to ./decoders/VGG16_GRU.hdf5\n",
      "Epoch 3/5\n",
      "256/256 [==============================] - 212s 830ms/step - loss: 2.7620\n",
      "\n",
      "Epoch 00003: saving model to ./decoders/VGG16_GRU.hdf5\n",
      "Epoch 4/5\n",
      "256/256 [==============================] - 213s 833ms/step - loss: 2.7153\n",
      "\n",
      "Epoch 00004: saving model to ./decoders/VGG16_GRU.hdf5\n",
      "Epoch 5/5\n",
      "256/256 [==============================] - 217s 849ms/step - loss: 2.7186\n",
      "\n",
      "Epoch 00005: saving model to ./decoders/VGG16_GRU.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = decoder_model.fit_generator(generator=generator,\n",
    "                            steps_per_epoch=256,\n",
    "                            epochs=5,\n",
    "                            callbacks=[checkpoints, loss_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
