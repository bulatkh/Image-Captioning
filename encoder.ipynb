{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from configs.default import _C as config\n",
    "from configs.default import update_config\n",
    "\n",
    "from datasets import coco_parse\n",
    "from datasets import flickr8k_parse\n",
    "\n",
    "from keras import Model \n",
    "from keras.applications import VGG16\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from models import image_preprocessing, transfer_models\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image encoding\n",
    "\n",
    "Currently, an image encoder is built using VGG16 architecture pre-trained on imagenet database.\n",
    "\n",
    "The features were obtained from the \"fc2\" layer - last fully-connected layer before the predictions layer.\n",
    "\n",
    "Generated features in numpy arrays for both training and validation datasets were saved to .npy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./configs/baseline.yaml\"\n",
    "update_config(config, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.DATASET == 'Coco':\n",
    "    if config.ATTENTION:\n",
    "        features_file_train = \"vgg16_coco_train_attn.npy\"\n",
    "        features_file_val = \"vgg16_coco_val_attn.npy\"\n",
    "        features_file_test = \"vgg16_coco_test_attn.npy\"\n",
    "    else:\n",
    "        features_file_train = \"vgg16_coco_train.npy\"\n",
    "        features_file_val = \"vgg16_coco_val.npy\"\n",
    "        features_file_test = \"vgg16_coco_test.npy\"\n",
    "    \n",
    "    \n",
    "    val_filenames_with_captions = coco_parse.get_image_filename_with_caption(config.ANNOTATIONS_PATH, \n",
    "                                                                             config.IMG_PATH, \n",
    "                                                                             train=False)\n",
    "\n",
    "    val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "    train_filenames_with_captions = coco_parse.get_image_filename_with_caption(config.ANNOTATIONS_PATH, \n",
    "                                                                               config.IMG_PATH,\n",
    "                                                                               train=True)\n",
    "    train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.DATASET == 'Flickr8k':\n",
    "    captions_file = os.path.join(config.ANNOTATIONS_PATH, \"Flickr8k.token.txt\")\n",
    "    train_txt_path = os.path.join(config.ANNOTATIONS_PATH, \"Flickr_8k.trainImages.txt\")\n",
    "    dev_txt_path = os.path.join(config.ANNOTATIONS_PATH, \"Flickr_8k.devImages.txt\")\n",
    "    test_txt_path = os.path.join(config.ANNOTATIONS_PATH, \"Flickr_8k.testImages.txt\")\n",
    "    \n",
    "    if config.ATTENTION:\n",
    "        features_file_train = \"vgg16_flickr8k_train_attn.npy\"\n",
    "        features_file_val = \"vgg16_flickr8k_val_attn.npy\"\n",
    "        features_file_test = \"vgg16_flickr8k_test_attn.npy\"\n",
    "    else:\n",
    "        features_file_train = \"vgg16_flickr8k_train.npy\"\n",
    "        features_file_val = \"vgg16_flickr8k_val.npy\"\n",
    "        features_file_test = \"vgg16_flickr8k_test.npy\"\n",
    "\n",
    "    filenames_with_all_captions = flickr8k_parse.generate_filenames_with_all_captions(captions_file, \n",
    "                                                                                      config.IMG_PATH)\n",
    "    train_filenames_with_all_captions = flickr8k_parse.generate_set(train_txt_path, \n",
    "                                                                    filenames_with_all_captions,\n",
    "                                                                    config.IMG_PATH)\n",
    "    val_filenames_with_all_captions = flickr8k_parse.generate_set(dev_txt_path, \n",
    "                                                                  filenames_with_all_captions, \n",
    "                                                                  config.IMG_PATH)\n",
    "    test_filenames_with_all_captions = flickr8k_parse.generate_set(test_txt_path, \n",
    "                                                                   filenames_with_all_captions, \n",
    "                                                                   config.IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation dataset: 1000\n",
      "Number of images in training dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in validation dataset: {}'.format(len(val_filenames_with_all_captions)))\n",
    "print('Number of images in training dataset: {}'.format(len(train_filenames_with_all_captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0% of images processed\n",
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for validation images\n",
    "start = time.time()\n",
    "val_transfer_values = transfer_models.use_pretrained_model_for_images(val_filenames_with_all_captions,\n",
    "                                                                      config.ATTENTION, \n",
    "                                                                      batch_size=config.ENCODER.BATCH_SIZE)\n",
    "time_val = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset encoding took 2.1 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Validation dataset encoding took {:.1f} minutes'.format(time_val / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_flickr8k_val_attn.npy\n"
     ]
    }
   ],
   "source": [
    "### save features for validation images\n",
    "transfer_models.save_features(val_transfer_values, './cnn_features/', features_file_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_flickr8k_val_attn.npy')\n",
    "(a == val_transfer_values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x0000028EC089EB38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of images processed\n",
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for training images\n",
    "start = time.time()\n",
    "train_transfer_values = transfer_models.use_pretrained_model_for_images(train_filenames_with_all_captions, \n",
    "                                                                        config.ATTENTION, \n",
    "                                                                        batch_size=config.ENCODER.BATCH_SIZE)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset encoding took 8.1 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset encoding took {:.1f} minutes'.format(time_train / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_flickr8k_train_attn.npy\n"
     ]
    }
   ],
   "source": [
    "transfer_models.save_features(train_transfer_values, './cnn_features/', features_file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_flickr8k_train_attn.npy')\n",
    "(a == train_transfer_values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of images processed\n",
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for training images\n",
    "start = time.time()\n",
    "test_transfer_values = transfer_models.use_pretrained_model_for_images(test_filenames_with_all_captions, \n",
    "                                                                       config.ATTENTION, \n",
    "                                                                       batch_size=config.ENCODER.BATCH_SIZE)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_flickr8k_test_attn.npy\n"
     ]
    }
   ],
   "source": [
    "transfer_models.save_features(test_transfer_values, './cnn_features/', features_file_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
