{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model \n",
    "from keras.applications import VGG16\n",
    "from PIL import Image\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import coco_parse\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17365205738817286873\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1470421401\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10081240021118732857\n",
      "physical_device_desc: \"device: 0, name: GeForce 840M, pci bus id: 0000:07:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image encoding\n",
    "\n",
    "Currently, an image encoder is built using VGG16 architecture pre-trained on imagenet database.\n",
    "\n",
    "The features were obtained from the \"fc2\" layer - last fully-connected layer before the predictions layer.\n",
    "\n",
    "Generated features in numpy arrays for both training and validation datasets were saved to .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(image_path, new_size):\n",
    "    \"\"\"\n",
    "    Reads the image and applies preprocessing including:\n",
    "    - resizing to the new_size\n",
    "    - rescaling pixel values at [0, 1]\n",
    "    - transforming grayscale images to RGB format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        full path to the image\n",
    "    new_size: tuple\n",
    "        size of the output image\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = np.array(image.resize(new_size, Image.LANCZOS))\n",
    "    image = np.divide(image, 255)\n",
    "    if len(image.shape) != 3:\n",
    "        image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = 'D:/coco/annotations/'\n",
    "images_path = 'D:/coco/images/'\n",
    "\n",
    "# parse JSON file with captions to get paths to images with captions\n",
    "\n",
    "val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=False)\n",
    "\n",
    "val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=True)\n",
    "train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation dataset: 5000\n",
      "Number of images in training dataset: 118285\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in validation dataset: {}'.format(len(val_filenames_with_all_captions)))\n",
    "print('Number of images in training dataset: {}'.format(len(train_filenames_with_all_captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create instance of the VGG16 model pretrained on imagenet\n",
    "VGG_model = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "### alter initial VGG model by eliminating of the last layer which produces preditions. \n",
    "### The output of the second fully-connected layer will be used as the last layer of the encoder part\n",
    "transfer_layer = VGG_model.get_layer('fc2')\n",
    "VGG_transfer_model = Model(inputs=VGG_model.input, outputs=transfer_layer.output)\n",
    "\n",
    "### get the size of the images used for VGG16 to resize COCO images\n",
    "input_layer = VGG_model.get_layer('input_2')\n",
    "VGG_image_size = input_layer.input_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pretrained_model_for_images(filenames_with_all_captions, transfer_model, img_size ,batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the pretrained model without prediction layer to encode the images into the set of the features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filenames with all captions : list\n",
    "        List of dictionaries containing images with the corresponding captions\n",
    "    \n",
    "    transfer_model: keras.Model\n",
    "        Model which is used to process images\n",
    "        \n",
    "    img_size: tuple\n",
    "        Size of images required by the model\n",
    "        \n",
    "    batch_size: int\n",
    "        Size of the batch for CNN\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get the number of images in the dataset\n",
    "    num_images = len(filenames_with_all_captions)\n",
    "    ### calculate the number of iterations \n",
    "    iter_num = int(num_images / batch_size)\n",
    "    ### variable to print the progress each 5% of the dataset \n",
    "    five_perc = int(iter_num * 0.05)\n",
    "    iter_count = 0\n",
    "    cur_progress = 0\n",
    "    \n",
    "    ### get the paths to all images without captions\n",
    "    image_paths = list(filenames_with_all_captions.keys())\n",
    "    ### list for the final result\n",
    "    transfer_values = []\n",
    "    \n",
    "    ### start and end index for each batch\n",
    "    first_i = 0\n",
    "    last_i = batch_size\n",
    "    \n",
    "    ### loop through the images\n",
    "    while first_i < num_images:\n",
    "        iter_count += 1\n",
    "        \n",
    "        ### progress print\n",
    "        if iter_count == five_perc:\n",
    "            cur_progress += 5\n",
    "            iter_count = 0\n",
    "            print(str(cur_progress) + \"% of images processed\")\n",
    "        \n",
    "        ### to make sure that last batch is not beyond the number of the images\n",
    "        if last_i > num_images:\n",
    "            last_i = num_images\n",
    "        \n",
    "        ### initialize the list for the batch\n",
    "        image_batch = []\n",
    "        \n",
    "        ### loop to form batches\n",
    "        for image in image_paths[first_i:last_i]:\n",
    "            ### preprocess image\n",
    "            image = image_preprocessing(image, img_size)\n",
    "            ### append image to batch list\n",
    "            image_batch.append(image)\n",
    "        \n",
    "        ### run the model to encode the features\n",
    "        preds = transfer_model.predict(np.array(image_batch))\n",
    "        \n",
    "        ### append predictions from the batch to the final list\n",
    "        for pred in preds:\n",
    "            transfer_values.append(pred)\n",
    "        \n",
    "        ### update first and last indices in the batch\n",
    "        first_i += batch_size\n",
    "        last_i += batch_size\n",
    "        \n",
    "    return np.array(transfer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(np_arr, folder, model, train=True):\n",
    "    \"\"\"\n",
    "    Saves encoded features into the .npy file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    np_arr : numpy.ndarray\n",
    "        The array which should be saved\n",
    "        \n",
    "    folder: str\n",
    "        Path to the destination folder \n",
    "    \n",
    "    model: str\n",
    "        Name of the used CNN model\n",
    "    \n",
    "    train: boolean\n",
    "        Flag for training dataset \n",
    "        set True if training and False if validaton\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    ### form the final filename\n",
    "    if train:\n",
    "        filename = model + '_train.npy'\n",
    "    else:\n",
    "        filename = model + '_val.npy'\n",
    "    \n",
    "    ### form the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "    \n",
    "    ### create the folder if it does not exist\n",
    "    try: \n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        pass\n",
    "    ### save file \n",
    "    np.save(full_path, np_arr)\n",
    "    \n",
    "    print(\"Array was saved to {}\".format(full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for validation images\n",
    "start = time.time()\n",
    "val_transfer_values = use_pretrained_model_for_images(val_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_val = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset encoding took 7.9 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Validation dataset encoding took {:.1f} minutes'.format(time_val / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_val.npy\n"
     ]
    }
   ],
   "source": [
    "### save features for validation images\n",
    "save_features(val_transfer_values, './cnn_features/', 'vgg16', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_val.npy')\n",
    "(a == val_transfer_values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for training images\n",
    "start = time.time()\n",
    "train_transfer_values = use_pretrained_model_for_images(train_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset encoding took 189.6 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset encoding took {:.1f} minutes'.format(time_train / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_train.npy\n"
     ]
    }
   ],
   "source": [
    "save_features(train_transfer_values, './cnn_features/', 'vgg16', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_train.npy')\n",
    "(a == train_transfer_values).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
