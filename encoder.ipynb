{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datasets import coco_parse\n",
    "from datasets import flickr8k_parse\n",
    "from keras import Model \n",
    "from keras.applications import VGG16\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image encoding\n",
    "\n",
    "Currently, an image encoder is built using VGG16 architecture pre-trained on imagenet database.\n",
    "\n",
    "The features were obtained from the \"fc2\" layer - last fully-connected layer before the predictions layer.\n",
    "\n",
    "Generated features in numpy arrays for both training and validation datasets were saved to .npy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_path = 'D:/coco/annotations/'\n",
    "# images_path = 'D:/coco/images/'\n",
    "\n",
    "# # parse JSON file with captions to get paths to images with captions\n",
    "\n",
    "# val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=False)\n",
    "\n",
    "# val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "# train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "#                                                                      train=True)\n",
    "# train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'D:/Flickr8k/images/'\n",
    "annotations_path = 'D:/Flickr8k/annotations/'\n",
    "captions_file = 'D:/Flickr8k/annotations/Flickr8k.token.txt'\n",
    "train_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.trainImages.txt'\n",
    "dev_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.devImages.txt'\n",
    "test_txt_path = 'D:/Flickr8k/annotations/Flickr_8k.testImages.txt'\n",
    "\n",
    "filenames_with_all_captions = flickr8k_parse.generate_filenames_with_all_captions(captions_file, images_path)\n",
    "\n",
    "train_filenames_with_all_captions = flickr8k_parse.generate_set(train_txt_path, filenames_with_all_captions, images_path)\n",
    "val_filenames_with_all_captions = flickr8k_parse.generate_set(dev_txt_path, filenames_with_all_captions, images_path)\n",
    "test_filenames_with_all_captions = flickr8k_parse.generate_set(test_txt_path, filenames_with_all_captions, images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation dataset: 1000\n",
      "Number of images in training dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in validation dataset: {}'.format(len(val_filenames_with_all_captions)))\n",
    "print('Number of images in training dataset: {}'.format(len(train_filenames_with_all_captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "### Create instance of the VGG16 model pretrained on imagenet\n",
    "VGG_model = VGG16(include_top=True, weights='imagenet')\n",
    "### alter initial VGG model by eliminating of the last layer which produces preditions. \n",
    "### The output of the second fully-connected layer will be used as the last layer of the encoder part\n",
    "if attn:\n",
    "    transfer_layer = VGG_model.get_layer('block5_conv3')\n",
    "else:\n",
    "    transfer_layer = VGG_model.get_layer('fc2')\n",
    "VGG_transfer_model = Model(inputs=VGG_model.input, outputs=transfer_layer.output)\n",
    "### get the size of the images used for VGG16 to resize COCO images\n",
    "input_layer = VGG_model.get_layer('input_1')\n",
    "VGG_image_size = input_layer.input_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG_transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pretrained_model_for_images(filenames_with_all_captions, transfer_model, img_size ,batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the pretrained model without prediction layer to encode the images into the set of the features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filenames with all captions : list\n",
    "        List of dictionaries containing images with the corresponding captions\n",
    "    \n",
    "    transfer_model: keras.Model\n",
    "        Model which is used to process images\n",
    "        \n",
    "    img_size: tuple\n",
    "        Size of images required by the model\n",
    "        \n",
    "    batch_size: int\n",
    "        Size of the batch for CNN\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get the number of images in the dataset\n",
    "    num_images = len(filenames_with_all_captions)\n",
    "    ### calculate the number of iterations \n",
    "    iter_num = int(num_images / batch_size)\n",
    "    ### variable to print the progress each 5% of the dataset \n",
    "    five_perc = int(iter_num * 0.05)\n",
    "    iter_count = 0\n",
    "    cur_progress = 0\n",
    "    \n",
    "    ### get the paths to all images without captions\n",
    "    image_paths = list(filenames_with_all_captions.keys())\n",
    "    ### list for the final result\n",
    "    transfer_values = []\n",
    "    \n",
    "    ### start and end index for each batch\n",
    "    first_i = 0\n",
    "    last_i = batch_size\n",
    "    \n",
    "    ### loop through the images\n",
    "    while first_i < num_images:\n",
    "        iter_count += 1\n",
    "        \n",
    "        ### progress print\n",
    "        if iter_count == five_perc:\n",
    "            cur_progress += 5\n",
    "            iter_count = 0\n",
    "            print(str(cur_progress) + \"% of images processed\")\n",
    "        \n",
    "        ### to make sure that last batch is not beyond the number of the images\n",
    "        if last_i > num_images:\n",
    "            last_i = num_images\n",
    "        \n",
    "        ### initialize the list for the batch\n",
    "        image_batch = []\n",
    "        \n",
    "        ### loop to form batches\n",
    "        for image in image_paths[first_i:last_i]:\n",
    "            ### preprocess image\n",
    "            image = utils.image_preprocessing(image, img_size)\n",
    "            ### append image to batch list\n",
    "            image_batch.append(image)\n",
    "        \n",
    "        ### run the model to encode the features\n",
    "        preds = transfer_model.predict(np.array(image_batch))\n",
    "        \n",
    "        ### append predictions from the batch to the final list\n",
    "        for pred in preds:\n",
    "            transfer_values.append(pred)\n",
    "        \n",
    "        ### update first and last indices in the batch\n",
    "        first_i += batch_size\n",
    "        last_i += batch_size\n",
    "        \n",
    "    return np.array(transfer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(np_arr, folder, filename):\n",
    "    \"\"\"\n",
    "    Saves encoded features into the .npy file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    np_arr : numpy.ndarray\n",
    "        The array which should be saved\n",
    "        \n",
    "    folder: str\n",
    "        Path to the destination folder \n",
    "    \n",
    "    model: str\n",
    "        Name of the used CNN model\n",
    "    \n",
    "    train: boolean\n",
    "        Flag for training dataset \n",
    "        set True if training and False if validaton\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    ### form the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "    \n",
    "    ### create the folder if it does not exist\n",
    "    try: \n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        pass\n",
    "    ### save file \n",
    "    np.save(full_path, np_arr)\n",
    "    \n",
    "    print(\"Array was saved to {}\".format(full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n",
      "105% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for validation images\n",
    "start = time.time()\n",
    "val_transfer_values = use_pretrained_model_for_images(val_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_val = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset encoding took 1.9 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Validation dataset encoding took {:.1f} minutes'.format(time_val / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_flickr8k_val_attn.npy\n"
     ]
    }
   ],
   "source": [
    "### save features for validation images\n",
    "save_features(val_transfer_values, './cnn_features/', 'vgg16_flickr8k_val_attn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_flickr8k_val_attn.npy')\n",
    "(a == val_transfer_values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for training images\n",
    "start = time.time()\n",
    "train_transfer_values = use_pretrained_model_for_images(train_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset encoding took 9.2 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset encoding took {:.1f} minutes'.format(time_train / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_flickr8k_train_attn.npy\n"
     ]
    }
   ],
   "source": [
    "save_features(train_transfer_values, './cnn_features/', 'vgg16_flickr8k_train_attn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_flickr8k_train_attn.npy')\n",
    "(a == train_transfer_values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n",
      "105% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for training images\n",
    "start = time.time()\n",
    "test_transfer_values = use_pretrained_model_for_images(test_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_flickr8k_test_attn.npy\n"
     ]
    }
   ],
   "source": [
    "save_features(test_transfer_values, './cnn_features/', 'vgg16_flickr8k_test_attn.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
