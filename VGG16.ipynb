{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import coco_parse\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.image import grayscale_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = 'D:/coco/annotations/'\n",
    "images_path = 'D:/coco/images/'\n",
    "\n",
    "# parse JSON file with captions to get paths to images woth captions\n",
    "filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=False)\n",
    "filenames_with_all_captions = coco_parse.get_image_with_all_captions(filenames_with_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(image_path, new_size):\n",
    "    \"\"\"\n",
    "    Function reads the image and applies preprocessing including:\n",
    "    - resizing to the new_size\n",
    "    - rescaling pixel values at [0, 1]\n",
    "    - transforming grayscale images to RGB format\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = np.array(image.resize(new_size, Image.LANCZOS))\n",
    "    image = np.divide(image, 255)\n",
    "    if len(image.shape) != 3:\n",
    "        image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create instance of the VGG16 model pretrained on imagenet\n",
    "VGG_model = VGG16(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### alter initial VGG model by eliminating of the last layer which produces preditions. \n",
    "### The output of the second fully-connected layer will be used as the last layer of the encoder part\n",
    "transfer_layer = VGG_model.get_layer('fc2')\n",
    "VGG_transfer_model = Model(inputs=VGG_model.input, outputs=transfer_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the size of the images used for VGG16 to resize COCO images\n",
    "input_layer = VGG_model.get_layer('input_1')\n",
    "VGG_image_size = input_layer.input_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pretrained_model_for_images(filenames_with_all_captions, transfer_model, img_size ,batch_size=64):\n",
    "    \"\"\"\n",
    "    Function uses the pretrained model without prediction layer to encode the images into the set\n",
    "    of the features.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get the number of images in the dataset\n",
    "    num_images = len(filenames_with_all_captions)\n",
    "    ### calculate the number of iterations \n",
    "    iter_num = int(num_images / batch_size)\n",
    "    ### variable to print the progress each 5% of the dataset \n",
    "    five_perc = int(iter_num * 0.05)\n",
    "    iter_count = 0\n",
    "    cur_progress = 0\n",
    "    \n",
    "    ### get the paths to all images without captions\n",
    "    image_paths = list(filenames_with_all_captions.keys())\n",
    "    ### list for the final result\n",
    "    transfer_values = []\n",
    "    \n",
    "    ### start and end index for each batch\n",
    "    first_i = 0\n",
    "    last_i = batch_size\n",
    "    \n",
    "    ### loop through the images\n",
    "    while first_i < num_images:\n",
    "        iter_count += 1\n",
    "        \n",
    "        ### progress print\n",
    "        if iter_count == five_perc:\n",
    "            cur_progress += 5\n",
    "            iter_count = 0\n",
    "            print(str(cur_progress) + \"% of images processed\")\n",
    "        \n",
    "        ### to make sure that last batch is not beyond the number of the images\n",
    "        if last_i > num_images:\n",
    "            last_i = num_images\n",
    "        \n",
    "        ### initialize the list for the batch\n",
    "        image_batch = []\n",
    "        \n",
    "        ### loop to form batches\n",
    "        for image in image_paths[first_i:last_i]:\n",
    "            ### preprocess image\n",
    "            image = image_preprocessing(image, img_size)\n",
    "            ### append image to batch list\n",
    "            image_batch.append(image)\n",
    "        \n",
    "        ### run the model to encode the features\n",
    "        preds = transfer_model.predict(np.array(image_batch))\n",
    "        \n",
    "        ### append predictions from the batch to the final list\n",
    "        for pred in preds:\n",
    "            transfer_values.append(pred)\n",
    "        \n",
    "        ### update first and last indices in the batch\n",
    "        first_i += batch_size\n",
    "        last_i += batch_size\n",
    "        \n",
    "    return np.array(transfer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(np_arr, folder, model, train=True):\n",
    "    \"\"\"\n",
    "    Function for saving encoded features into the .npy file.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### form the final filename\n",
    "    if train:\n",
    "        filename = model + '_train.npy'\n",
    "    else:\n",
    "        filename = model + '_val.npy'\n",
    "    \n",
    "    ### form the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "    \n",
    "    ### create the folder if it does not exist\n",
    "    try: \n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        pass\n",
    "    ### save file \n",
    "    np.save(full_path, np_arr)\n",
    "    \n",
    "    print(\"Array was saved to {}\".format(full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for validation images\n",
    "val_transfer_values = use_pretrained_model_for_images(filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_val.npy\n"
     ]
    }
   ],
   "source": [
    "### save features for validation images\n",
    "save_features(val_transfer_values, './cnn_features/', 'vgg16', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
