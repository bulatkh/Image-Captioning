{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import coco_parse\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.image import grayscale_to_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image encoding\n",
    "\n",
    "Currently, an image encoder is built using VGG16 architecture pre-trained on imagenet database.\n",
    "\n",
    "The features were obtained from the \"fc2\" layer - last fully-connected layer before the predictions layer.\n",
    "\n",
    "Generated features in numpy arrays for both training and validation datasets were saved to .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(image_path, new_size):\n",
    "    \"\"\"\n",
    "    Reads the image and applies preprocessing including:\n",
    "    - resizing to the new_size\n",
    "    - rescaling pixel values at [0, 1]\n",
    "    - transforming grayscale images to RGB format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        full path to the image\n",
    "    new_size: tuple\n",
    "        size of the output image\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = np.array(image.resize(new_size, Image.LANCZOS))\n",
    "    image = np.divide(image, 255)\n",
    "    if len(image.shape) != 3:\n",
    "        image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = 'D:/coco/annotations/'\n",
    "images_path = 'D:/coco/images/'\n",
    "\n",
    "# parse JSON file with captions to get paths to images woth captions\n",
    "val_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=False)\n",
    "val_filenames_with_all_captions = coco_parse.get_image_with_all_captions(val_filenames_with_captions)\n",
    "\n",
    "train_filenames_with_captions = coco_parse.get_image_filename_with_caption(captions_path, images_path, \n",
    "                                                                     train=True)\n",
    "train_filenames_with_all_captions = coco_parse.get_image_with_all_captions(train_filenames_with_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation dataset: 5000\n",
      "Number of images in training dataset: 118285\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in validation dataset: {}'.format(len(val_filenames_with_all_captions)))\n",
    "print('Number of images in training dataset: {}'.format(len(train_filenames_with_all_captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create instance of the VGG16 model pretrained on imagenet\n",
    "VGG_model = VGG16(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### alter initial VGG model by eliminating of the last layer which produces preditions. \n",
    "### The output of the second fully-connected layer will be used as the last layer of the encoder part\n",
    "transfer_layer = VGG_model.get_layer('fc2')\n",
    "VGG_transfer_model = Model(inputs=VGG_model.input, outputs=transfer_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the size of the images used for VGG16 to resize COCO images\n",
    "input_layer = VGG_model.get_layer('input_1')\n",
    "VGG_image_size = input_layer.input_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pretrained_model_for_images(filenames_with_all_captions, transfer_model, img_size ,batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the pretrained model without prediction layer to encode the images into the set of the features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filenames with all captions : list\n",
    "        List of dictionaries containing images with the corresponding captions\n",
    "    \n",
    "    transfer_model: keras.Model\n",
    "        Model which is used to process images\n",
    "        \n",
    "    img_size: tuple\n",
    "        Size of images required by the model\n",
    "        \n",
    "    batch_size: int\n",
    "        Size of the batch for CNN\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get the number of images in the dataset\n",
    "    num_images = len(filenames_with_all_captions)\n",
    "    ### calculate the number of iterations \n",
    "    iter_num = int(num_images / batch_size)\n",
    "    ### variable to print the progress each 5% of the dataset \n",
    "    five_perc = int(iter_num * 0.05)\n",
    "    iter_count = 0\n",
    "    cur_progress = 0\n",
    "    \n",
    "    ### get the paths to all images without captions\n",
    "    image_paths = list(filenames_with_all_captions.keys())\n",
    "    ### list for the final result\n",
    "    transfer_values = []\n",
    "    \n",
    "    ### start and end index for each batch\n",
    "    first_i = 0\n",
    "    last_i = batch_size\n",
    "    \n",
    "    ### loop through the images\n",
    "    while first_i < num_images:\n",
    "        iter_count += 1\n",
    "        \n",
    "        ### progress print\n",
    "        if iter_count == five_perc:\n",
    "            cur_progress += 5\n",
    "            iter_count = 0\n",
    "            print(str(cur_progress) + \"% of images processed\")\n",
    "        \n",
    "        ### to make sure that last batch is not beyond the number of the images\n",
    "        if last_i > num_images:\n",
    "            last_i = num_images\n",
    "        \n",
    "        ### initialize the list for the batch\n",
    "        image_batch = []\n",
    "        \n",
    "        ### loop to form batches\n",
    "        for image in image_paths[first_i:last_i]:\n",
    "            ### preprocess image\n",
    "            image = image_preprocessing(image, img_size)\n",
    "            ### append image to batch list\n",
    "            image_batch.append(image)\n",
    "        \n",
    "        ### run the model to encode the features\n",
    "        preds = transfer_model.predict(np.array(image_batch))\n",
    "        \n",
    "        ### append predictions from the batch to the final list\n",
    "        for pred in preds:\n",
    "            transfer_values.append(pred)\n",
    "        \n",
    "        ### update first and last indices in the batch\n",
    "        first_i += batch_size\n",
    "        last_i += batch_size\n",
    "        \n",
    "    return np.array(transfer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(np_arr, folder, model, train=True):\n",
    "    \"\"\"\n",
    "    Saves encoded features into the .npy file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    np_arr : numpy.ndarray\n",
    "        The array which should be saved\n",
    "        \n",
    "    folder: str\n",
    "        Path to the destination folder \n",
    "    \n",
    "    model: str\n",
    "        Name of the used CNN model\n",
    "    \n",
    "    train: boolean\n",
    "        Flag for training dataset \n",
    "        set True if training and False if validaton\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    ### form the final filename\n",
    "    if train:\n",
    "        filename = model + '_train.npy'\n",
    "    else:\n",
    "        filename = model + '_val.npy'\n",
    "    \n",
    "    ### form the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "    \n",
    "    ### create the folder if it does not exist\n",
    "    try: \n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        pass\n",
    "    ### save file \n",
    "    np.save(full_path, np_arr)\n",
    "    \n",
    "    print(\"Array was saved to {}\".format(full_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for validation images\n",
    "start = time.time()\n",
    "val_transfer_values = use_pretrained_model_for_images(val_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_val = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset encoding took 7.9 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Validation dataset encoding took {:.1f} minutes'.format(time_val / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_val.npy\n"
     ]
    }
   ],
   "source": [
    "### save features for validation images\n",
    "save_features(val_transfer_values, './cnn_features/', 'vgg16', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_val.npy')\n",
    "(a == val_transfer_values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of images processed\n",
      "10% of images processed\n",
      "15% of images processed\n",
      "20% of images processed\n",
      "25% of images processed\n",
      "30% of images processed\n",
      "35% of images processed\n",
      "40% of images processed\n",
      "45% of images processed\n",
      "50% of images processed\n",
      "55% of images processed\n",
      "60% of images processed\n",
      "65% of images processed\n",
      "70% of images processed\n",
      "75% of images processed\n",
      "80% of images processed\n",
      "85% of images processed\n",
      "90% of images processed\n",
      "95% of images processed\n",
      "100% of images processed\n"
     ]
    }
   ],
   "source": [
    "### encode features for training images\n",
    "start = time.time()\n",
    "train_transfer_values = use_pretrained_model_for_images(train_filenames_with_all_captions, VGG_transfer_model, VGG_image_size, batch_size=16)\n",
    "time_train = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset encoding took 189.6 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset encoding took {:.1f} minutes'.format(time_train / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array was saved to ./cnn_features/vgg16_train.npy\n"
     ]
    }
   ],
   "source": [
    "save_features(train_transfer_values, './cnn_features/', 'vgg16', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('./cnn_features/vgg16_train.npy')\n",
    "(a == train_transfer_values).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captions encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building decoder, it is necessary to encode captions into one-hot vectors which further would be used in embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(all_captions):\n",
    "    \"\"\"\n",
    "    Replaces all the signs by whitespaces\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_captions: list\n",
    "        List of lists with all the captions\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    for captions_list in all_captions:\n",
    "        for i, caption in enumerate(captions_list):\n",
    "            captions_list[i] = re.sub('\\W+', ' ', caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_and_end_to_captions(all_captions, start_str = '<SOS>', end_str = '<EOS>'):\n",
    "    \"\"\"\n",
    "    Adds start and end of caption markers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_captions: list\n",
    "        List of lists with all the captions\n",
    "        \n",
    "    start_str: str\n",
    "        Start of caption marker\n",
    "        \n",
    "    end_str: str\n",
    "        End of caption marker\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    for captions in all_captions:\n",
    "        for i in range(len(captions)):\n",
    "            captions[i] =  '{} {} {}'.format(start_str, captions[i], end_str)\n",
    "            captions[i] = captions[i].replace('  ', ' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract captions\n",
    "train_captions = coco_parse.make_list_of_captions(train_filenames_with_all_captions)\n",
    "val_captions = coco_parse.make_list_of_captions(val_filenames_with_all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bicycle replica with a clock as the front wheel.',\n",
       " 'The bike has a clock as a tire.',\n",
       " 'A black metal bicycle with a clock inside the front wheel.',\n",
       " 'A bicycle figurine in which the front wheel is replaced with a clock\\n',\n",
       " 'A clock with the appearance of the wheel of a bicycle ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A black Honda motorcycle parked in front of a garage.',\n",
       " 'A Honda motorcycle parked in a grass driveway',\n",
       " 'A black Honda motorcycle with a dark burgundy seat.',\n",
       " 'Ma motorcycle parked on the gravel in front of a garage',\n",
       " 'A motorcycle with its brake extended standing outside']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess captions\n",
    "preprocess_captions(val_captions)\n",
    "preprocess_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A black Honda motorcycle parked in front of a garage ',\n",
       " 'A Honda motorcycle parked in a grass driveway',\n",
       " 'A black Honda motorcycle with a dark burgundy seat ',\n",
       " 'Ma motorcycle parked on the gravel in front of a garage',\n",
       " 'A motorcycle with its brake extended standing outside']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bicycle replica with a clock as the front wheel ',\n",
       " 'The bike has a clock as a tire ',\n",
       " 'A black metal bicycle with a clock inside the front wheel ',\n",
       " 'A bicycle figurine in which the front wheel is replaced with a clock ',\n",
       " 'A clock with the appearance of the wheel of a bicycle ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add markers of captions' starts and ends\n",
    "add_start_and_end_to_captions(train_captions)\n",
    "add_start_and_end_to_captions(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a bicycle replica with a clock as the front wheel <eos>',\n",
       " '<sos> the bike has a clock as a tire <eos>',\n",
       " '<sos> a black metal bicycle with a clock inside the front wheel <eos>',\n",
       " '<sos> a bicycle figurine in which the front wheel is replaced with a clock <eos>',\n",
       " '<sos> a clock with the appearance of the wheel of a bicycle <eos>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a black honda motorcycle parked in front of a garage <eos>',\n",
       " '<sos> a honda motorcycle parked in a grass driveway <eos>',\n",
       " '<sos> a black honda motorcycle with a dark burgundy seat <eos>',\n",
       " '<sos> ma motorcycle parked on the gravel in front of a garage <eos>',\n",
       " '<sos> a motorcycle with its brake extended standing outside <eos>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    The class is used to form a vocabulary (bag-of-words)\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    number_of_words\n",
    "        current number of words in the class instance\n",
    "        \n",
    "    word_to_id\n",
    "        dictionary mapping words (tokens) to their ids\n",
    "        \n",
    "    id_to_word\n",
    "        dictionary mapping ids to the corresponding words\n",
    "    -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.number_of_words = 0\n",
    "        self.word_to_id = dict()\n",
    "        self.id_to_word = dict()\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Adds a word in the vocabulary\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        word : str\n",
    "            The word to add\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        if word not in self.word_to_id:\n",
    "            self.word_to_id.update({word: self.number_of_words})\n",
    "            self.id_to_word.update({self.number_of_words: word})\n",
    "            self.number_of_words += 1\n",
    "    \n",
    "    def get_id_by_word(self, word):\n",
    "        \"\"\"\n",
    "        Returns id for an input word\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        word : str\n",
    "            The word for which id is needed\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def get_word_by_id(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a word for an input id\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        id : int\n",
    "            The id for which word is needed\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    def save_vocabulary(self, filename_word_to_id='word_to_id.pickle', filename_id_to_word='id_to_word.pickle'):\n",
    "        \"\"\"\n",
    "        Saves vocabulary dictionaries to pickle files\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename_word_to_id : str\n",
    "            The filename for word_to_id dictionary\n",
    "        \n",
    "        filename_id_to_word : str\n",
    "            The filename for id_to_word dictionary\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        try: \n",
    "            os.mkdir('./vocabulary')\n",
    "        except:\n",
    "            pass\n",
    "        path_word_to_id = os.path.join('./vocabulary/', filename_word_to_id)\n",
    "        path_id_to_word = os.path.join('./vocabulary/', filename_id_to_word)\n",
    "        \n",
    "        with open(path_word_to_id, 'wb') as writer:\n",
    "            pickle.dump(self.word_to_id, writer)\n",
    "            \n",
    "        with open(path_id_to_word, 'wb') as writer:\n",
    "            pickle.dump(self.id_to_word, writer)\n",
    "            \n",
    "    def load_vocabulary(self, path_word_to_id='./vocabulary/word_to_id.pickle', path_id_to_word='./vocabulary/id_to_word.pickle'):\n",
    "        \"\"\"\n",
    "        Loads vocabulary dictionaries from pickle files\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path_word_to_id : str\n",
    "            The path to file with word_to_id dictionary\n",
    "        \n",
    "        filename_id_to_word : str\n",
    "            The path to file with id_to_word dictionary\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        with open(path_word_to_id, 'rb') as reader:\n",
    "            self.word_to_id = pickle.load(reader)\n",
    "            \n",
    "        with open(path_id_to_word, 'rb') as reader:\n",
    "            self.id_to_word = pickle.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vocabulary from the training captions\n",
    "train_vocab = Vocabulary()\n",
    "for i, caption_list in enumerate(train_captions):\n",
    "    for caption in caption_list:\n",
    "        tmp_caption_list = caption.split()\n",
    "        for word in tmp_caption_list:\n",
    "            train_vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab.save_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create transformed captions list - substitute words by their IDs from vocabulary\n",
    "train_captions_tokens = [] \n",
    "for captions in train_captions:\n",
    "    tmp_captions_for_img = []\n",
    "    for caption in captions:\n",
    "        caption_words = caption.split()\n",
    "        tmp = []\n",
    "        for word in caption_words:\n",
    "            tmp.append(train_vocab.get_id_by_word(word))\n",
    "        tmp_captions_for_img.append(tmp)\n",
    "    train_captions_tokens.append(tmp_captions_for_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 1, 5, 6, 7, 8, 9, 10],\n",
       " [0, 7, 11, 12, 1, 5, 6, 1, 13, 10],\n",
       " [0, 1, 14, 15, 2, 4, 1, 5, 16, 7, 8, 9, 10],\n",
       " [0, 1, 2, 17, 18, 19, 7, 8, 9, 20, 21, 4, 1, 5, 10],\n",
       " [0, 1, 5, 4, 7, 22, 23, 7, 9, 23, 1, 2, 10]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> a bicycle replica with a clock as the front wheel <eos>',\n",
       " '<sos> the bike has a clock as a tire <eos>',\n",
       " '<sos> a black metal bicycle with a clock inside the front wheel <eos>',\n",
       " '<sos> a bicycle figurine in which the front wheel is replaced with a clock <eos>',\n",
       " '<sos> a clock with the appearance of the wheel of a bicycle <eos>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(transfer_values, captions_tokens, eos_token = 10, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate a batch of input-output data pairs:\n",
    "        input_data = {\n",
    "            transfer_values,\n",
    "            input_tokens\n",
    "        }\n",
    "        \n",
    "        output_data = {\n",
    "            output_tokens\n",
    "        }\n",
    "        \n",
    "     Parameters:\n",
    "        -----------\n",
    "        transfer_values: np.array\n",
    "            Encoded images features\n",
    "            \n",
    "        captions: list\n",
    "            list with all the captions\n",
    "        \n",
    "        \n",
    "        batch_size: int\n",
    "            The number of examples in a batch\n",
    "        -----------\n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_images = len(captions)\n",
    "    \n",
    "    indices = np.random.randint(0, number_of_images, size=batch_size)\n",
    "    \n",
    "    captions_batch = []\n",
    "    ### Randomly select one caption for each example index\n",
    "    for ind in indices:\n",
    "        num_captions = len(captions_tokens[ind])\n",
    "        selected_caption = captions_tokens[ind][np.random.randint(0, num_captions)]\n",
    "        captions_batch.append(selected_caption)\n",
    "    \n",
    "    ### Find the largest caption length and pad the remaining to be the same size\n",
    "    max_caption_size = max([len(cap) for cap in captions_batch])\n",
    "    \n",
    "    captions_batch_padded = pad_sequences(captions_batch, \n",
    "                                          maxlen=max_caption_size, \n",
    "                                          padding='post', \n",
    "                                          value=eos_token)\n",
    "    \n",
    "    ### Input tokens are the initial ones starting from index 1\n",
    "    ### Output tokens are the initial ones shifted to the right\n",
    "    input_tokens = captions_batch_padded[:, :-1]\n",
    "    output_tokens = captions_batch_padded[:, 1:]\n",
    "    input_transfer_values = transfer_values[indices]\n",
    "    \n",
    "    input_data = {\n",
    "        'transfer_values': input_transfer_values,\n",
    "        'input_tokens': input_tokens\n",
    "    }\n",
    "    \n",
    "    output_data = {\n",
    "        'output_tokens': output_tokens\n",
    "    }\n",
    "    \n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_values = np.load('./cnn_features/vgg16_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, out = generate_batch(transfer_values, train_captions_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4096)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp['transfer_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 18)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp['input_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 18)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['output_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1 69 23 56 51 18  1 70 58 57 10 10 10 10 10 10 10]\n",
      "[ 1 69 23 56 51 18  1 70 58 57 10 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "print(inp['input_tokens'][0])\n",
    "print(out['output_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> a couple of cars parked in a busy street sidewalk <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "a couple of cars parked in a busy street sidewalk <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([train_vocab.get_word_by_id(x) for x in inp['input_tokens'][0]]))\n",
    "print(\" \".join([train_vocab.get_word_by_id(x) for x in out['output_tokens'][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
